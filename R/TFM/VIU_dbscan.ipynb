{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06dfa56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de librearías necesarias\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import socket\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import pickle  #Para guardar archivos\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "from pympler import asizeof #Para liberar memoria\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from kneed import KneeLocator \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa5f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path general de archivos\n",
    "if socket.gethostname()=='LAPTOP-PUSGG08B': #Ip de la laptop\n",
    "    ruta = \"E:/Cristian Guatemal/Master/Big Data y Ciencia de Datos/VIU_TFM/Data/TFM/\"\n",
    "    r_ruta = \"E:/Cristian Guatemal/Master/Big Data y Ciencia de Datos/VIU_TFM/RData/TFM/\"\n",
    "elif socket.gethostname()=='PCUIOMTDAIE6382': #Ip del working\n",
    "    ruta =   \"D:/Master/Big_Data_Ciencia_Datos/VIU_TFM/Data/TFM/\"\n",
    "    r_ruta = \"D:/Master/Big_Data_Ciencia_Datos/VIU_TFM/RData/TFM/\"\n",
    "# Ruta del archivo de pensionistas de vejez\n",
    "ruta_vj = ruta + 'POB_VEJ_CD656_NEW.dsv'\n",
    "# Ruta del archivo de historia laboral de pensionistas\n",
    "ruta_afi = ruta + 'APORTES_CD656_new.dsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83cb945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo------------------------------------------------------------------------------------------------------------\n",
    "directorio = r_ruta\n",
    "nombre_archivo = 'viu_clean_afi_sel_g_all_2.pkl'\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "\n",
    "with open( ruta_archivo, 'rb') as archivo:\n",
    "    data_l = pickle.load( archivo ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca1ba133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución es:  0.0  horas con  1.0  minutos y 3.390178918838501  segundos\n"
     ]
    }
   ],
   "source": [
    "inicio = time.time()\n",
    "data_l = data_l.sort_values( by=[\"CEDULA_COD\",\"ANIO\", \"MES\"], ascending=[ True, True, True] )\n",
    "data_l.reset_index(inplace=True)\n",
    "data_l.rename(columns={'index': 'nuevo_indice'}, inplace=True)\n",
    "data_l.drop(columns=['nuevo_indice'], inplace=True)\n",
    "data_l['INDICE'] = data_l.index\n",
    "data_l['LS_MS'] = np.nan\n",
    "\n",
    "#Casos de no análisis\n",
    "data_no_grupo = data_l[ (data_l['GRUPO_SEL']==0) ].copy()\n",
    "data_no_grupo['ATI_DB'] = np.nan\n",
    "data_no_grupo['EPS'] = np.nan\n",
    "data_no_grupo['MINPTS'] = np.nan\n",
    "\n",
    "#Casos de análisis\n",
    "data = data_l[ (data_l['GRUPO_SEL']==1) ].copy()\n",
    "\n",
    "fin = time.time()  \n",
    "print('Tiempo de ejecución es: ',  (fin-inicio)//3600, ' horas con ' ,  (fin-inicio)%3600//60 , ' minutos y', (fin-inicio)%60, ' segundos' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da92486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTiempo de ejecución es:  0.0  horas con  6.0  minutos y 34.17673945426941  segundos\n"
     ]
    }
   ],
   "source": [
    "#Se calculo los bigotes superiores para toda la historia laboral\n",
    "inicio = time.time() \n",
    "dataa = data_l.copy()\n",
    "dic_aux = dataa.groupby('CEDULA_COD').agg({'SALARIO_SECTOR': list, 'INDICE': list}).to_dict(orient='index')\n",
    "\n",
    "for cedula in dic_aux:\n",
    "    dic_aux[cedula]['SALARIO_SECTOR'] = [ [float(val) for val in sal.replace(':', ';').split(';')] \n",
    "                                            if isinstance(sal, str) \n",
    "                                            else [float(sal)]\n",
    "                                            for sal in dic_aux[cedula]['SALARIO_SECTOR']\n",
    "                                        ]\n",
    "    # Aplanar la lista de listas de SALARIO_SECTOR\n",
    "    salarios = None\n",
    "    salarios = [salario for sublist in dic_aux[cedula]['SALARIO_SECTOR'] for salario in sublist]\n",
    "    dic_aux[cedula]['SAL_PROM1'] =  np.nanmean(salarios)\n",
    "    dic_aux[cedula]['Q1'] =  np.percentile(salarios, 25)\n",
    "    dic_aux[cedula]['Q3'] =  np.percentile(salarios, 75)\n",
    "    dic_aux[cedula]['IQR'] = dic_aux[cedula]['Q3'] -  dic_aux[cedula]['Q1']\n",
    "    dic_aux[cedula]['LI'] =  dic_aux[cedula]['Q1'] -  1.5  * dic_aux[cedula]['IQR']\n",
    "    dic_aux[cedula]['LS'] =  dic_aux[cedula]['Q3'] +  1.5  * dic_aux[cedula]['IQR']\n",
    "\n",
    "data1 = { 'CEDULA_COD': [], 'LS': [], 'SAL_PROM1': [] }\n",
    "\n",
    "for cedula, val in dic_aux.items():\n",
    "    data1['CEDULA_COD'].append(cedula)\n",
    "    data1['LS'].append( val['LS'] )\n",
    "    data1['SAL_PROM1'].append( val['SAL_PROM1'] )\n",
    "\n",
    "LS = pd.DataFrame( data1 )\n",
    "LS = LS.groupby('CEDULA_COD')['LS'].first()\n",
    "data_l.loc[:, 'LS1'] = data_l['CEDULA_COD'].map(LS)\n",
    "\n",
    "SPROM = pd.DataFrame( data1 )\n",
    "SPROM = SPROM.groupby('CEDULA_COD')['SAL_PROM1'].first()\n",
    "data_l.loc[:, 'SAL_PROM1'] = data_l['CEDULA_COD'].map(SPROM)\n",
    "\n",
    "del dataa, data1, dic_aux, LS, salarios, SPROM\n",
    "gc.collect()\n",
    "\n",
    "##Se calculo los bigotes superiores para toda la historia laboral, a partir del año 2000 en adelante.\n",
    "dataa = data_l[data_l['ANIO']>=2000].copy()\n",
    "dic_aux = dataa.groupby('CEDULA_COD').agg({'SALARIO_SECTOR': list, 'INDICE': list}).to_dict(orient='index')\n",
    "\n",
    "for cedula in dic_aux:\n",
    "    dic_aux[cedula]['SALARIO_SECTOR'] = [ [float(val) for val in sal.replace(':', ';').split(';')] \n",
    "                                            if isinstance(sal, str) \n",
    "                                            else [float(sal)]\n",
    "                                            for sal in dic_aux[cedula]['SALARIO_SECTOR']\n",
    "                                        ]\n",
    "    # Aplanar la lista de listas de SALARIO_SECTOR\n",
    "    salarios = None\n",
    "    salarios = [salario for sublist in dic_aux[cedula]['SALARIO_SECTOR'] for salario in sublist]\n",
    "    dic_aux[cedula]['SAL_PROM2'] =  np.nanmean(salarios)\n",
    "    dic_aux[cedula]['Q1'] =  np.percentile(salarios, 25)\n",
    "    dic_aux[cedula]['Q3'] =  np.percentile(salarios, 75)\n",
    "    dic_aux[cedula]['IQR'] = dic_aux[cedula]['Q3'] -  dic_aux[cedula]['Q1']\n",
    "    dic_aux[cedula]['LI'] =  dic_aux[cedula]['Q1'] -  1.5  * dic_aux[cedula]['IQR']\n",
    "    dic_aux[cedula]['LS'] =  dic_aux[cedula]['Q3'] +  1.5  * dic_aux[cedula]['IQR']\n",
    "\n",
    "data1 = { 'CEDULA_COD': [], 'LS': [], 'SAL_PROM2': []  }\n",
    "\n",
    "for cedula, val in dic_aux.items():\n",
    "    data1['CEDULA_COD'].append(cedula)\n",
    "    data1['LS'].append( val['LS'] )\n",
    "    data1['SAL_PROM2'].append( val['SAL_PROM2'] )\n",
    "\n",
    "LS = pd.DataFrame( data1 )\n",
    "LS = LS.groupby('CEDULA_COD')['LS'].first()\n",
    "data_l.loc[:, 'LS2'] = data_l['CEDULA_COD'].map(LS)\n",
    "\n",
    "SPROM = pd.DataFrame( data1 )\n",
    "SPROM = SPROM.groupby('CEDULA_COD')['SAL_PROM2'].first()\n",
    "data_l.loc[:, 'SAL_PROM2'] = data_l['CEDULA_COD'].map(SPROM)\n",
    "\n",
    "del dataa, data1, dic_aux, LS, salarios, SPROM\n",
    "gc.collect()\n",
    "\n",
    "fin = time.time()  \n",
    "print('\\tTiempo de ejecución es: ',  (fin-inicio)//3600, ' horas con ' ,  (fin-inicio)%3600//60 , ' minutos y', (fin-inicio)%60, ' segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7825b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algortimos DBSCAN\n",
    "cedula_1 = data.groupby('CEDULA_COD')['NUM_SEC_MES'].apply( lambda x: (x != 1).any() )\n",
    "cedula_dist = cedula_1[ cedula_1 ].index\n",
    "ul = data[ ~data['CEDULA_COD'].isin( cedula_dist )]\n",
    "ml = data[  data['CEDULA_COD'].isin( cedula_dist )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3abcc1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se calculo los bigotes superiores para la historia laboral de los mejores años\n",
    "inicio = time.time() \n",
    "dataa = ul.copy()\n",
    "dic_aux = dataa.groupby('CEDULA_COD').agg({'SALARIO_SECTOR': list, 'INDICE': list}).to_dict(orient='index')\n",
    "\n",
    "for cedula in dic_aux:\n",
    "    dic_aux[cedula]['SALARIO_SECTOR'] = [ [float(val) for val in sal.replace(':', ';').split(';')] \n",
    "                                            if isinstance(sal, str) \n",
    "                                            else [float(sal)]\n",
    "                                            for sal in dic_aux[cedula]['SALARIO_SECTOR']\n",
    "                                        ]\n",
    "    # Aplanar la lista de listas de SALARIO_SECTOR\n",
    "    salarios = None\n",
    "    salarios = [salario for sublist in dic_aux[cedula]['SALARIO_SECTOR'] for salario in sublist]\n",
    "    dic_aux[cedula]['Q1'] =  np.percentile(salarios, 25)\n",
    "    dic_aux[cedula]['Q3'] =  np.percentile(salarios, 75)\n",
    "    dic_aux[cedula]['IQR'] = dic_aux[cedula]['Q3'] -  dic_aux[cedula]['Q1']\n",
    "    dic_aux[cedula]['LI'] =  dic_aux[cedula]['Q1'] -  1.5  * dic_aux[cedula]['IQR']\n",
    "    dic_aux[cedula]['LS'] =  dic_aux[cedula]['Q3'] +  1.5  * dic_aux[cedula]['IQR']\n",
    "\n",
    "data1 = { 'CEDULA_COD': [], 'LS': []}\n",
    "\n",
    "for cedula, val in dic_aux.items():\n",
    "    data1['CEDULA_COD'].append(cedula)\n",
    "    data1['LS'].append( val['LS'] )\n",
    "\n",
    "LS = pd.DataFrame( data1 )\n",
    "LS = LS.groupby('CEDULA_COD')['LS'].first()\n",
    "data_l.loc[ ~data_l['CEDULA_COD'].isin( cedula_dist ), 'LS_MS'] = data_l['CEDULA_COD'].map(LS)\n",
    "\n",
    "del dataa, data1, dic_aux, LS\n",
    "gc.collect()\n",
    "\n",
    "dataa = ml.copy()\n",
    "dic_aux = dataa.groupby('CEDULA_COD').agg({'SALARIO_SECTOR': list, 'INDICE': list}).to_dict(orient='index')\n",
    "\n",
    "for cedula in dic_aux:\n",
    "    dic_aux[cedula]['SALARIO_SECTOR'] = [ [float(val) for val in sal.replace(':', ';').split(';')] \n",
    "                                            if isinstance(sal, str) \n",
    "                                            else [float(sal)]\n",
    "                                            for sal in dic_aux[cedula]['SALARIO_SECTOR']\n",
    "                                        ]\n",
    "    # Aplanar la lista de listas de SALARIO_SECTOR\n",
    "    salarios = None\n",
    "    salarios = [salario for sublist in dic_aux[cedula]['SALARIO_SECTOR'] for salario in sublist]\n",
    "    dic_aux[cedula]['Q1'] =  np.percentile(salarios, 25)\n",
    "    dic_aux[cedula]['Q3'] =  np.percentile(salarios, 75)\n",
    "    dic_aux[cedula]['IQR'] = dic_aux[cedula]['Q3'] -  dic_aux[cedula]['Q1']\n",
    "    dic_aux[cedula]['LI'] =  dic_aux[cedula]['Q1'] -  1.5  * dic_aux[cedula]['IQR']\n",
    "    dic_aux[cedula]['LS'] =  dic_aux[cedula]['Q3'] +  1.5  * dic_aux[cedula]['IQR']\n",
    "\n",
    "data1 = { 'CEDULA_COD': [], 'LS': []}\n",
    "\n",
    "for cedula, val in dic_aux.items():\n",
    "    data1['CEDULA_COD'].append(cedula)\n",
    "    data1['LS'].append( val['LS'] )\n",
    "\n",
    "LS = pd.DataFrame( data1 )\n",
    "LS = LS.groupby('CEDULA_COD')['LS'].first()\n",
    "data_l.loc[  data_l['CEDULA_COD'].isin( cedula_dist ), 'LS_MS'] = data_l['CEDULA_COD'].map(LS)\n",
    "\n",
    "del dataa, data1, dic_aux, LS\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0431bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_dic = ul.groupby('CEDULA_COD').agg({'SALARIO': list, 'INDICE': list}).to_dict(orient='index')\n",
    "ml_dic = ml.groupby('CEDULA_COD').agg({'SALARIO': list, 'INDICE': list}).to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_dic[230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f08e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valor_epsilon( data, val ):\n",
    "    \n",
    "    for ced in data:\n",
    "        aux = None\n",
    "        aux = np.array( data[ced][val] ).reshape(-1, 1)\n",
    "        eps = []\n",
    "        minpt = []\n",
    "        data[ced]['eps'] = []\n",
    "        data[ced]['min_pt'] = []\n",
    "\n",
    "        for i in range(1, 12):\n",
    "            vecinos = NearestNeighbors( n_neighbors = min( i, len(aux) ) + 1  )\n",
    "            vecinos_ajustado = vecinos.fit( aux )\n",
    "            distancias = vecinos_ajustado.kneighbors(aux )[0]\n",
    "            distancias1 = distancias[:, 1:]\n",
    "            distancias_ordenadas = np.sort(distancias1.ravel())\n",
    "\n",
    "            if len( np.unique( distancias_ordenadas ) )==1:\n",
    "                eps.append( 0.0 )\n",
    "                minpt.append( i + 1 )\n",
    "\n",
    "            if len( np.unique( distancias_ordenadas ) )!=1:\n",
    "                codo = KneeLocator(np.arange(len(distancias_ordenadas)), distancias_ordenadas, curve=\"convex\", direction=\"increasing\")\n",
    "\n",
    "                eps.append( distancias_ordenadas[codo.elbow])\n",
    "                minpt.append( i + 1 )\n",
    "\n",
    "        for i, eps in enumerate( eps ):\n",
    "            if eps not in data[ced]['eps']:\n",
    "                data[ced]['eps'].append( eps )\n",
    "                data[ced]['min_pt'].append( minpt[i] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59079493",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "valor_epsilon(ul_dic, 'SALARIO')\n",
    "valor_epsilon(ml_dic, 'SALARIO')\n",
    "fin = time.time()  \n",
    "print('\\tTiempo de ejecución es: ',  (fin-inicio)//3600, ' horas con ' ,  (fin-inicio)%3600//60 , ' minutos y', (fin-inicio)%60, ' segundos' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d6801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se guardan los resultados \n",
    "nombre_archivo = 'viu_db_ul.pkl'\n",
    "# Ruta completa del archivo\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "# Objetos a guardar\n",
    "objeto1 = ul_dic\n",
    "\n",
    "# Guardar los objetos en el archivo\n",
    "with open(ruta_archivo, 'wb') as archivo:\n",
    "    pickle.dump(objeto1, archivo) \n",
    "    \n",
    "nombre_archivo = 'viu_db_ml.pkl'\n",
    "# Ruta completa del archivo\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "# Objetos a guardar\n",
    "objeto2 = ml_dic\n",
    "\n",
    "# Guardar los objetos en el archivo\n",
    "with open(ruta_archivo, 'wb') as archivo:\n",
    "    pickle.dump(objeto2, archivo)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d5612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo------------------------------------------------------------------------------------------------------------\n",
    "directorio = r_ruta\n",
    "nombre_archivo = 'viu_db_ul.pkl'\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "\n",
    "with open( ruta_archivo, 'rb') as archivo:\n",
    "    ul_dic = pickle.load( archivo ) \n",
    "    \n",
    "directorio = r_ruta\n",
    "nombre_archivo = 'viu_db_ml.pkl'\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "\n",
    "with open( ruta_archivo, 'rb') as archivo:\n",
    "    ml_dic = pickle.load( archivo ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e763573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rangos_valores(number):\n",
    "    actions = {\n",
    "        (0, 1): 2,\n",
    "        (1, 2): 1,\n",
    "        (2, 3): 2,\n",
    "        (3, 4): 3,\n",
    "        (4, 5): 4,\n",
    "        (5, float('inf')): 5\n",
    "    }\n",
    "    \n",
    "    for (start, end), (val1) in actions.items():\n",
    "        if start < number <= end:\n",
    "            return val1 \n",
    "        \n",
    "def esti_eps_out( data_dic, val ):\n",
    "    \n",
    "    for ced in data_dic:\n",
    "        aux =  np.array( data_dic[ced][val] ).reshape(-1, 1)\n",
    "        ep = data_dic[ced]['eps']\n",
    "        mt = data_dic[ced]['min_pt']\n",
    "\n",
    "        n_clusters = []\n",
    "        n_outliers = []\n",
    "        v_e = []\n",
    "        g_e = []\n",
    "        n_e = []\n",
    "\n",
    "        for j in range(0, len(ep) ):\n",
    "\n",
    "            if  ep[ j ] <= 0.1:\n",
    "                n_clusters.append( -1 )\n",
    "                n_outliers.append( -1 )\n",
    "                v_e.append( ep[ j ] )\n",
    "                g_e.append( j )\n",
    "                n_e.append( mt[j] )\n",
    "            \n",
    "            else:\n",
    "                var1 = rangos_valores(ep[ j ])\n",
    "                ini =  max(0.1, ep[ j ] - var1 )\n",
    "                for eps in np.linspace(ini, ep[ j ], 6):\n",
    "                        labels=None\n",
    "                        db = DBSCAN( eps, min_samples = mt[j] ).fit( aux )\n",
    "                        core_samples_mask = np.zeros_like( db.labels_, dtype=bool)\n",
    "                        core_samples_mask[db.core_sample_indices_] = True\n",
    "                        labels = db.labels_\n",
    "                        n_clusters.append( len(set(labels)) - (1 if -1 in labels else 0) )\n",
    "                        n_outliers.append( list(labels).count(-1) )\n",
    "                        v_e.append( eps ) \n",
    "                        g_e.append( j )\n",
    "                        n_e.append( mt[j] )\n",
    " \n",
    "        ag = np.empty((len(v_e) , 5))   \n",
    "        ag[ :, 0 ] = g_e\n",
    "        ag[ :, 1 ] = v_e\n",
    "        ag[ :, 2 ] = n_e\n",
    "        ag[ :, 3 ] = n_clusters\n",
    "        ag[ :, 4 ] = n_outliers\n",
    "        data_dic[ced]['esti_eps_out'] = ag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2876111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************\n",
      "Determinación del número de clúster  y número atípicos\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22180\\692710450.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Determinación del número de clúster  y número atípicos'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SALARIO'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mesti_eps_out\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mul_dic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mesti_eps_out\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mml_dic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22180\\124868962.py\u001b[0m in \u001b[0;36mesti_eps_out\u001b[1;34m(data_dic, val)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0meps\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mini\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                         \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                         \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0maux\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                         \u001b[0mcore_samples_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                         \u001b[0mcore_samples_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore_sample_indices_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0mneighbors_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;31m# This has worst case O(n^2) memory complexity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mneighborhoods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneighbors_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mradius_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mradius_neighbors\u001b[1;34m(self, X, radius, return_distance, sort_results)\u001b[0m\n\u001b[0;32m   1112\u001b[0m                 \u001b[0mparallel_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"prefer\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"threads\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             chunked_results = Parallel(n_jobs, **parallel_kwargs)(\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 delayed_query(\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mradius\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36m_tree_query_radius_parallel_helper\u001b[1;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[0;32m    911\u001b[0m     \u001b[0mcloudpickle\u001b[0m \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \"\"\"\n\u001b[1;32m--> 913\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery_radius\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inicio = time.time()\n",
    "print('*' * 102)\n",
    "print('Determinación del número de clúster  y número atípicos')\n",
    "val='SALARIO'\n",
    "esti_eps_out( ul_dic, val )\n",
    "esti_eps_out( ml_dic, val )\n",
    "fin = time.time()  \n",
    "print('\\tTiempo de ejecución es: ',  (fin-inicio)//3600, ' horas con ' ,  (fin-inicio)%3600//60 , ' minutos y', (fin-inicio)%60, ' segundos' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35074d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionar el valor de epsilon y los atipicos\n",
    "def cal_eps_out( data_dic ):\n",
    "    for ced in data_dic:\n",
    "        aux = data_dic[ced]['esti_eps_out']\n",
    "        aux1 =  np.array( data_dic[ced]['SALARIO'] ).reshape(-1, 1)\n",
    "        col1 = 4\n",
    "        col2 = 3\n",
    "\n",
    "        # Encontrar el valor máximo en la columna 4 (índice 3)\n",
    "        val1 = np.max(aux[:,  col1])\n",
    "\n",
    "        # Filtrar las filas que tienen el valor máximo en la columna 4\n",
    "        fil= aux[aux[:, col1] == val1]\n",
    "\n",
    "        # Encontrar la fila con el menor valor en la columna 3 (índice 2) entre las filas filtradas\n",
    "        indice = np.argmin( fil[:, col2 ])\n",
    "\n",
    "        # Seleccionar la fila correspondiente\n",
    "        fila_max_min =  fil[ indice]\n",
    "        data_dic[ced]['eps_c'] = fila_max_min[1]\n",
    "        data_dic[ced]['mpt_c'] = fila_max_min[2]\n",
    "        \n",
    "        if fila_max_min[1] ==0:\n",
    "            data_dic[ced]['ATI_DB'] = [-1] * ( len(aux1) )\n",
    "        else:\n",
    "            db = DBSCAN( eps = fila_max_min[1], min_samples = int(fila_max_min[2]) ).fit( aux1 )\n",
    "            data_dic[ced]['ATI_DB'] = (db.labels_).tolist()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "print('*' * 102)\n",
    "print('Determinación del número de clúster  y número atípicos')\n",
    "val='SALARIO'\n",
    "cal_eps_out( ul_dic )\n",
    "cal_eps_out( ml_dic )\n",
    "fin = time.time()  \n",
    "print('\\tTiempo de ejecución es: ',  (fin-inicio)//3600, ' horas con ' ,  (fin-inicio)%3600//60 , ' minutos y', (fin-inicio)%60, ' segundos' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extensión a dataframe\n",
    "def extend_data( data_val_ati ):\n",
    "    inicio = time.time()\n",
    "    print('Extensión a dataframe de los diccionarios')\n",
    "    data1 = { 'CEDULA_COD': [], 'SALARIO': [], 'INDICE':[], 'ATI_DB':[], 'EPS':[], 'MINPTS':[]}\n",
    "\n",
    "    # Llenar las listas con los datos del diccionario\n",
    "    for cedula, values in data_val_ati.items():\n",
    "        salario = values['SALARIO']\n",
    "        indice = values['INDICE']\n",
    "        atipico= values['ATI_DB']\n",
    "        ep = values['eps_c']\n",
    "        mt = values['mpt_c']\n",
    "        num_rows = len(salario)\n",
    "\n",
    "        # Extender las listas en el diccionario de datos\n",
    "        data1['CEDULA_COD'].extend( [cedula] * num_rows )\n",
    "        data1['SALARIO'].extend( salario )\n",
    "        data1['ATI_DB' ].extend( atipico )\n",
    "        data1['INDICE'].extend( indice )\n",
    "        data1['EPS'].extend( [ep] * num_rows )\n",
    "        data1['MINPTS'].extend( [mt] * num_rows  )\n",
    "\n",
    "    data1 = pd.DataFrame( data1 )\n",
    "    \n",
    "    fin = time.time()  \n",
    "    print('\\tTiempo de ejecución es: ',  (fin-inicio)//3600, ' horas con ' ,  (fin-inicio)%3600//60 , ' minutos y', (fin-inicio)%60, ' segundos' )\n",
    "    return(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_dic( data, data_no_grupo, data_dic1, data_dic2):\n",
    "    inicio = time.time()\n",
    "    print('*' * 102)\n",
    "    print('Concatenación con el dataframe original') \n",
    "    data1 = extend_data( data_dic1 ) \n",
    "    data2 = extend_data( data_dic2 ) \n",
    "\n",
    "    data_db = pd.concat( [ data_no_grupo[['CEDULA_COD', 'SALARIO', 'ATI_DB', 'EPS', 'MINPTS','INDICE']],\n",
    "                                   data1[['CEDULA_COD', 'SALARIO', 'ATI_DB', 'EPS', 'MINPTS','INDICE']],\n",
    "                                   data2[['CEDULA_COD', 'SALARIO', 'ATI_DB', 'EPS', 'MINPTS','INDICE']]], axis=0)\n",
    "\n",
    "    data_db = data_db.sort_values( by = [\"INDICE\"], ascending = [ True ] )\n",
    "    data_db.reset_index( inplace = True )\n",
    "    data_db.rename( columns = {'index': 'nuevo_indice'}, inplace = True )\n",
    "    data_db.drop( columns = ['nuevo_indice'], inplace=True )\n",
    "\n",
    "    col = ['ATI_DB','EPS', 'MINPTS']\n",
    "    data[ col ] = np.nan\n",
    "\n",
    "    for nom_col in col:\n",
    "        aux = None\n",
    "        aux = data_db[ nom_col ].to_numpy()\n",
    "        data.iloc[:, data.columns.get_loc( nom_col )] = aux\n",
    "    \n",
    "    del data1, data2, data_db\n",
    "    gc.collect()\n",
    "    #return(data_db)\n",
    "       \n",
    "    fin = time.time()  \n",
    "    tm = (fin-inicio)\n",
    "    print('\\tTiempo de ejecución es: ', tm//3600, ' horas con ' , tm%3600//60 , ' minutos y', tm%60, ' segundos' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dic( data_l, data_no_grupo, ul_dic, ml_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data_l[data_l['SALARIO'] != a['SALARIO']]\n",
    "\n",
    "# Mostrar las filas donde 'SALARIO' no coincide\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se guardan resultados\n",
    "nombre_archivo = 'viu_db_ul_ati.pkl'\n",
    "# Ruta completa del archivo\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "# Objetos a guardar\n",
    "objeto1 = ul_dic\n",
    "\n",
    "# Guardar los objetos en el archivo\n",
    "with open(ruta_archivo, 'wb') as archivo:\n",
    "    pickle.dump(objeto1, archivo) \n",
    "    \n",
    "nombre_archivo = 'viu_db_ml_ati.pkl'\n",
    "# Ruta completa del archivo\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "# Objetos a guardar\n",
    "objeto2 = ml_dic\n",
    "\n",
    "# Guardar los objetos en el archivo\n",
    "with open(ruta_archivo, 'wb') as archivo:\n",
    "    pickle.dump(objeto2, archivo) \n",
    "    \n",
    "nombre_archivo = 'viu_db_data_l_cl.pkl'\n",
    "# Ruta completa del archivo\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "# Objetos a guardar\n",
    "objeto3 = data_l\n",
    "\n",
    "# Guardar los objetos en el archivo\n",
    "with open(ruta_archivo, 'wb') as archivo:\n",
    "    pickle.dump(objeto3, archivo) \n",
    "    \n",
    "del objeto1, objeto2, objeto3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9aeb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desde esta parte comienza la corrección de datos\n",
    "# Cargar archivo------------------------------------------------------------------------------------------------------------\n",
    "directorio = r_ruta\n",
    "nombre_archivo = 'viu_db_data_l_cl'\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "\n",
    "with open( ruta_archivo, 'rb') as archivo:\n",
    "    data_l = pickle.load( archivo ) \n",
    "    \n",
    "# Cargar archivo------------------------------------------------------------------------------------------------------------\n",
    "directorio = r_ruta\n",
    "nombre_archivo = 'viu_db_ul_ati.pkl'\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "\n",
    "with open( ruta_archivo, 'rb') as archivo:\n",
    "    ul_dic = pickle.load( archivo ) \n",
    "    \n",
    "# Cargar archivo------------------------------------------------------------------------------------------------------------\n",
    "directorio = r_ruta\n",
    "nombre_archivo = 'viu_db_ml_ati.pkl'\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "\n",
    "with open( ruta_archivo, 'rb') as archivo:\n",
    "    ml_dic = pickle.load( archivo ) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data_l['CEDULA_COD'].drop_duplicates().sample(n=20000, random_state=1)\n",
    "muestra = data_l[data_l['CEDULA_COD'].isin(grouped)]\n",
    "muestra.to_csv('muestra_db.txt', sep='\\t', index=False)# Agrupar por CEDULA_COD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed95e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTiempo de ejecución es:  0.0  horas con  1.0  minutos y 11.555240631103516  segundos\n"
     ]
    }
   ],
   "source": [
    "#Comprobación de la clasificación del dbscan\n",
    "inicio = time.time()\n",
    "#Se trabaja primero con las cédulas que tienen un único sector\n",
    "data = data_l[data_l['GRUPO_SEL']==1]\n",
    "nodata = data_l[data_l['GRUPO_SEL']==0]\n",
    "del data_l\n",
    "gc.collect()\n",
    "\n",
    "fin = time.time()  \n",
    "tm=fin-inicio\n",
    "print('\\tTiempo de ejecución es: ',  tm//3600, ' horas con ' ,  tm%3600//60 , ' minutos y', tm%60, ' segundos' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d426358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTiempo de ejecución es:  0.0  horas con  0.0  minutos y 45.895063161849976  segundos\n"
     ]
    }
   ],
   "source": [
    "inicio = time.time()\n",
    "cedula_1 = data.groupby('CEDULA_COD')['NUM_SEC_MES'].apply( lambda x: (x != 1).any() )\n",
    "cedula_dist = cedula_1[ cedula_1 ].index\n",
    "ul = data[ ~data['CEDULA_COD'].isin( cedula_dist )]\n",
    "ml = data[  data['CEDULA_COD'].isin( cedula_dist )]\n",
    "fin = time.time()  \n",
    "print('\\tTiempo de ejecución es: ',  (fin-inicio)//3600, ' horas con ' ,  (fin-inicio)%3600//60 , ' minutos y', (fin-inicio)%60, ' segundos' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9680ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cédulas con un único sector que tienen al menos un atípico 371806\n",
      "Cédulas con múltiples sectores que tienen al menos un atípico 69267\n"
     ]
    }
   ],
   "source": [
    "#Se analiza los casos con únicos sectores\n",
    "print('Cédulas con un único sector que tienen al menos un atípico', ul[ (ul['ATI_DB']==-1)]['CEDULA_COD'].nunique()) \n",
    "print('Cédulas con múltiples sectores que tienen al menos un atípico', ml[ (ml['ATI_DB']==-1)]['CEDULA_COD'].nunique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "293b7e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************** Cédulas con sectores únicos ****************************************\n",
      "\tCedulas que tienen un único sector y que fue clasificado como no atipico, que se modificaran a  atipico: 29111\n",
      "\tCedulas que tienen un único sector y que fue clasificado como atipico, que se modificaran a  no atipico: 128704\n"
     ]
    }
   ],
   "source": [
    "print('*' * 40, 'Cédulas con sectores únicos', '*' * 40)\n",
    "print('\\tCedulas que tienen un único sector y que fue clasificado como no atipico, que se modificaran a  atipico:',\n",
    "ul[ (ul['ATI_DB'] != -1) & (ul['SALARIO'] > ul['LS2'])]['CEDULA_COD'].nunique()) \n",
    "print('\\tCedulas que tienen un único sector y que fue clasificado como atipico, que se modificaran a  no atipico:',\n",
    "ul[ (ul['ATI_DB'] == -1) & (ul['SALARIO'] <= ul['SBU'])]['CEDULA_COD'].nunique())       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ul[ul['CEDULA_COD']==13603]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ul[ (ul['ATI_DB'] == 1) & (ul['SALARIO'] < ul[['LS2', 'LS_MS']].min(axis=1) ) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graf_dbscan(data_i, ced, val, grupo_sel = 0):\n",
    "    #Adecuación de la base de datos\n",
    "    idx = data_i.columns.get_loc('LS_MS')\n",
    "    columnas_requeridas = ['CEDULA_COD', 'ANIO', 'MES', val, 'GRUPO_SEL', 'INDICE', 'BASE_CAL'] + list(data_i.columns[idx:])\n",
    "    data = data_i[data_i['CEDULA_COD'] == ced][columnas_requeridas]\n",
    "    \n",
    "    # Establecer si se quieren todos o solos los 5 mejores años\n",
    "    if(grupo_sel == 1):\n",
    "        data = data[data['GRUPO_SEL']==1]\n",
    "    \n",
    "    #Fecha para eje x\n",
    "    anios = data['ANIO'].tolist()\n",
    "    meses = data['MES'].tolist()\n",
    "    fechas = [datetime(year=anio, month=mes, day=1) for anio, mes in zip(anios, meses)]\n",
    "    data['FECHA']= pd.to_datetime(fechas)\n",
    "    data = data.sort_values(by='FECHA')\n",
    "    \n",
    "    #Paleta de colores\n",
    "    colors = plt.get_cmap('tab20').colors\n",
    "\n",
    "    # Gráfico\n",
    "    plt.figure(figsize=(12, 8))  \n",
    "    marker_dict = {1: 'o', 2: 'x'} \n",
    "    \n",
    "    data_aux = data[data['GRUPO_SEL']==1]\n",
    "    if grupo_sel == 0: \n",
    "        Grupo_b= data[(data['GRUPO_SEL'] == 0)]\n",
    "        plt.scatter(Grupo_b['FECHA'], Grupo_b[val], marker= '^', color='yellow', label='Valores no considerados')\n",
    "        \n",
    "        for i in np.sort(data_aux['ATI_DB'].unique()):\n",
    "            if i == -1:\n",
    "                Grupo_a = data_aux[data_aux['ATI_DB'] == -1]\n",
    "                plt.scatter(Grupo_a['FECHA'], Grupo_a[val], marker= 'x', color='red', label='Valores Atípicos')\n",
    "\n",
    "            else:\n",
    "                color = colors[int(i % len(colors))]\n",
    "                Grupo= data_aux[data_aux['ATI_DB'] == i]\n",
    "                plt.scatter(Grupo['FECHA'], Grupo[val], marker= marker_dict[1], color= color, label=f'Clúster {int(i + 1)}')\n",
    "                \n",
    "    else:\n",
    "        for i in np.sort(data['ATI_DB'].unique()):\n",
    "            if i == -1:\n",
    "                Grupo_a = data[data['ATI_DB'] == -1]\n",
    "                plt.scatter(Grupo_a['FECHA'], Grupo_a[val], marker= 'x', color='red', label='Valores Atípicos')\n",
    "\n",
    "            else:\n",
    "                color = colors[int(i % len(colors))]\n",
    "                Grupo= data[data['ATI_DB'] == i]\n",
    "                plt.scatter(Grupo['FECHA'], Grupo[val], marker= marker_dict[1], color= color, label=f'Clúster {int(i + 1)}')\n",
    "    \n",
    "    plt.axhline(y= data['SAL_PROM2'].iloc[0], color= '#1297ff', linestyle='-', label='Promedio Salarios')\n",
    "    plt.axhline(y=data['LS2'].iloc[0], alpha=1 , color= '#25ce00', linestyle='--', label='LS2')\n",
    "    plt.axhline(y=data_aux['LS_MS'].iloc[0], alpha=1, color= '#875b20', linestyle='--', label='LS_MS')\n",
    "    plt.axhline(y=data['BASE_CAL'].iloc[0], alpha=1, color= '#E800FF', linestyle=':', label='Base de cálculo')\n",
    "    \n",
    "    # Añadir el número de clusters, EPS, MINPTS\n",
    "    plt.scatter( data_aux['FECHA'].iloc[0], data_aux[val].iloc[0], facecolors='none', label=\"Número de clústers: {}\".format(int(max(data_aux['ATI_DB'].unique()) + 1)))\n",
    "    plt.scatter( data_aux['FECHA'].iloc[0], data_aux[val].iloc[0], facecolors='none', label=\"Épsilon: {}\".format(round(data_aux['EPS'].iloc[0], 2)))\n",
    "    plt.scatter( data_aux['FECHA'].iloc[0], data_aux[val].iloc[0], facecolors='none', label=\"Min Pts: {}\".format(int(data_aux['MINPTS'].iloc[0])))\n",
    "    \n",
    "    # Añadir títulos y etiquetas\n",
    "    plt.title(f\"Aportaciones de la cédula: {ced}\")\n",
    "    plt.xlabel('FECHA')\n",
    "    plt.ylabel(f\"{val}\")\n",
    "    \n",
    "    # Crear la leyenda\n",
    "    leyenda = plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "#graf_dbscan(data_l , 52783, 'SALARIO',  grupo_sel = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "graf_dbscan(ul , 13603, 'SALARIO',  grupo_sel = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8496b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corrección de los atípicos\n",
    "ul.loc[ (ul['ATI_DB'] != -1) & (ul['SALARIO'] > ul['LS2']) , 'ATI_DB'] = -1\n",
    "ul.loc[ (ul['ATI_DB'] == -1) & (ul['SALARIO'] < ul[['LS2', 'LS_MS']].min(axis=1) )  , 'ATI_DB'] = ul['ATI_DB'].max(skipna=True) + 1\n",
    "ul.loc[ (ul['ATI_DB'] == -1) & (ul['SALARIO'] <= ul['SBU']) , 'ATI_DB'] = ul['ATI_DB'].max(skipna=True) + 1\n",
    "\n",
    "ul.loc[ (ul['ATI_DB'] != -1), 'ATI_DB'] = 0\n",
    "ul.loc[ (ul['ATI_DB'] == -1), 'ATI_DB'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00601ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "graf_dbscan(ul , 13603, 'SALARIO',  grupo_sel = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e589935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCedulas con al menos un atípico con un único sector: 157992\n"
     ]
    }
   ],
   "source": [
    "#Cedulas que tiene datos atípicos luego de la corrección---------------------------------------------------------------------\n",
    "print('\\tCedulas con al menos un atípico con un único sector:', ul[ (ul['ATI_DB']==1) ]['CEDULA_COD'].nunique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e464f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ul[ul['ATI_DB']==1]['CEDULA_COD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ul[ul['CEDULA_COD']==13603]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4fc209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_15636\\3619269662.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ml.loc[ indi, 'ATI_DB_AS'] = filtro['ATI_DB_AS']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sim = ml.groupby('CEDULA_COD').agg({'SALARIO_SECTOR': list, 'LS2':list,'LS_MS':list,'INDICE': list}).to_dict(orient='index')\n",
    "\n",
    "for cedula in dic_sim:\n",
    "    dic_sim[cedula]['SALARIO_SECTOR'] = [[float(val) for val in sal.split(';')] if isinstance(sal, str) else [sal] \n",
    "                                          for sal in dic_sim[cedula]['SALARIO_SECTOR']]\n",
    "\n",
    "    min_ls = [min(ls2, ls_as) for ls2, ls_as in zip(dic_sim[cedula]['LS2'], dic_sim[cedula]['LS_MS'])]\n",
    "\n",
    "    dic_sim[cedula]['ATI_DB_AS'] = [ 1 if any(sal > ls for sal in salarios) else 0 \n",
    "                                    for salarios, ls in zip( dic_sim[cedula]['SALARIO_SECTOR'], min_ls )]\n",
    "\n",
    "data1 = { 'CEDULA_COD': [], 'SALARIO_SECTOR': [], \n",
    "          'LS_MS' : [], 'ATI_DB_AS':[],  'INDICE':[]}\n",
    "\n",
    "# Llenar las listas con los datos del diccionario\n",
    "for cedula, values in dic_sim.items():\n",
    "    salario = values['SALARIO_SECTOR']\n",
    "    ls = values[ 'LS_MS' ]\n",
    "    atipico = values[ 'ATI_DB_AS' ]\n",
    "    indice = values['INDICE']\n",
    "    num_rows = len(salario)\n",
    "\n",
    "    # Extender las listas en el diccionario de datos\n",
    "    data1['CEDULA_COD'].extend([cedula] * num_rows)\n",
    "    data1['SALARIO_SECTOR'].extend(salario)\n",
    "    data1['LS_MS' ].extend(ls)\n",
    "    data1['ATI_DB_AS' ].extend(atipico)\n",
    "    data1['INDICE'].extend(indice)\n",
    "\n",
    "data1 = pd.DataFrame( data1 )\n",
    "data1.set_index('INDICE', inplace=True )\n",
    "cedul = list(dic_sim.keys())  #total de 111903\n",
    "filtro = data1[ data1['CEDULA_COD'].isin(cedul )] # 111903 cedulas\n",
    "indi = filtro.index\n",
    "ml.loc[ indi, 'ATI_DB_AS'] = filtro['ATI_DB_AS']\n",
    "\n",
    "del data1,  dic_sim, cedul, filtro, indi, salario, ls, atipico, indice, num_rows\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79baec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2b3c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corrección de los atípicos\n",
    "ml.loc[ (ml['ATI_DB'] != -1) & (ml['SALARIO'] > ml['LS2']) , 'ATI_DB'] = -1\n",
    "ml.loc[ (ml['ATI_DB'] == -1) & (ml['SALARIO'] <= ml['SBU']) , 'ATI_DB'] = ml['ATI_DB'].max(skipna=True) + 1\n",
    "\n",
    "#Para cuando un valor de la suma de salarios es mayor al LS, como si fueran un solo aporte individual\n",
    "ml.loc[(ml['ATI_DB'] == -1) & (ml['ATI_DB_AS'] == 0), 'ATI_DB'] = ml['ATI_DB_AS']\n",
    "ml.loc[(ml['ATI_DB'] != -1) & (ml['ATI_DB_AS'] == 1), 'ATI_DB'] = -ml['ATI_DB_AS']\n",
    "\n",
    "ml.loc[ (ml['ATI_DB'] != -1), 'ATI_DB'] = 0\n",
    "ml.loc[ (ml['ATI_DB'] == -1), 'ATI_DB'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92ebde59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCedulas con al menos un atípico con múltiples sectores: 33943\n"
     ]
    }
   ],
   "source": [
    "#Cedulas que tiene datos atípicos luego de la corrección---------------------------------------------------------------------\n",
    "print('\\tCedulas con al menos un atípico con múltiples sectores:', ml[ (ml['ATI_DB']==1) ]['CEDULA_COD'].nunique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbe23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e46ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ul.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e461d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce481494",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_db = pd.concat( [ nodata, ul, ml ], axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ac8119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del nodata, ul, ml\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_db.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e3b58a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_db = data_db.sort_values( by = [\"INDICE\"], ascending = [ True ] )\n",
    "data_db.reset_index( inplace = True )\n",
    "data_db.rename( columns = {'index': 'nuevo_indice'}, inplace = True )\n",
    "data_db.drop( columns = ['nuevo_indice'], inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58d3bf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombre_archivo = 'viu_db_data_ati.pkl'\n",
    "# Ruta completa del archivo\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "# Objetos a guardar\n",
    "objeto1 = data_db\n",
    "\n",
    "# Guardar los objetos en el archivo\n",
    "with open(ruta_archivo, 'wb') as archivo:\n",
    "    pickle.dump(objeto1, archivo) \n",
    "    \n",
    "del objeto1\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1270153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo------------------------------------------------------------------------------------------------------------\n",
    "directorio = r_ruta\n",
    "nombre_archivo = 'viu_db_data_ati.pkl'\n",
    "ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "\n",
    "with open( ruta_archivo, 'rb') as archivo:\n",
    "    data_l = pickle.load( archivo ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a572b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data_l['CEDULA_COD'].drop_duplicates().sample(n=30000, random_state=1)\n",
    "muestra = data_l[data_l['CEDULA_COD'].isin(grouped)]\n",
    "muestra.to_csv('muestra_db.txt', sep='\\t', index=False)# Agrupar por CEDULA_COD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val=[]\n",
    "data_l[(data_l['CEDULA_COD']==13603) & (data_l['GRUPO_SEL']==1) & (data_l['SALARIO'] < data_l[['LS2', 'LS_MS']].min(axis=1) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb47fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "    data_l['ATI_DB'].max(skipna=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
