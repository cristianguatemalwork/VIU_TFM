\chapter{Estado del arte y marco teórico}\label{cap:estado_arte}

\section{Sistema de Seguridad Social en Ecuador}\label{sistema_seg_soc}

La base normativa de la Seguridad Social en Ecuador está principalmente regulada por la \legalcite{ConsEcu:2008}, específicamente en el artículo 34, donde se establece que:

\begin{displayquote}
Art. 34 .- El derecho a la seguridad social es un derecho irrenunciable de todas las personas, y será deber y responsabilidad primordial del Estado. La seguridad social se regirá por los principios de solidaridad, obligatoriedad, universalidad, equidad, eficiencia, subsidiaridad, suficiencia, transparencia y participación, para la atención de las necesidades individuales y colectivas. El Estado garantizará y hará efectivo el ejercicio pleno del derecho a la seguridad social, que incluye a las personas que realizan trabajo no remunerado en los hogares, actividades para el auto sustento en el campo, toda forma de trabajo autónomo y a quienes se encuentran en situación de desempleo (p. 19).
\end{displayquote}

El IESS es la entidad responsable de brindar servicios y prestaciones de Seguridad Social, garantizando la protección de sus asegurados a nivel nacional. Esta institución está encargada de la protección tanto de la población urbana como rural, con o sin relación de dependencia laboral, ante contingencias como enfermedad, maternidad, riesgos laborales, discapacidad, cesantía, desempleo, invalidez, vejez y muerte. Los seguros cubiertos por el IESS incluyen: el Seguro del Sistema de Pensiones, el Seguro General de Salud Individual y Familiar, el Seguro General de Riesgos del Trabajo, el Seguro Social Campesino, así como los Fondos de Terceros y el Seguro de Desempleo \citep{iessboletin2022}. 

El financiamiento de estos seguros proviene principalmente de las contribuciones individuales de los afiliados, el aporte patronal obligatorio de los empleadores públicos y privados, la contribución financiera obligatoria del Estado, reservas técnicas, rentas, y otro tipo de ingresos. Es necesario destacar que el funcionamiento de la Seguridad Social en el Ecuador se basa en el principio de solidaridad intergeneracional \citep{iessboletin2022}.

Uno de los principales seguros es el Seguro del Sistema de Pensiones, también conocido como Seguro de Invalidez, Vejez y Muerte (Seguro IVM). Este seguro cubre prestaciones como la jubilación ordinaria por vejez, la jubilación por discapacidad, la jubilación por invalidez (incluyendo las prestaciones temporales por incapacidad), así como las pensiones de montepío y el auxilio de funerales \citep{iess2024web}.

Para acceder a estas prestaciones, los individuos deben cumplir determinados requisitos\footnote{Relacionados principalmente con la edad y el número de imposiciones. Una imposición se define como 30 días de trabajo} establecidos en la \legalcite{LeySS}. Una vez cumplidos estos requisitos, el monto de las prestaciones es determinado según lo dispuesto en el artículo 13  de la \legalcite{ResIESS_CD100}. Este artículo establece que la pensión mensual por invalidez o vejez, así como el subsidio transitorio por incapacidad, se calcularán multiplicando la base de cálculo, obtenida de acuerdo al procedimiento descrito en el artículo 2 de la misma resolución, por el coeficiente anual de años cumplidos de imposiciones. La base de cálculo se determina como el salario promedio de los cinco mejores años de la vida laboral del individuo.

\section{Desafíos de Integridad en la Seguridad Social}\label{estafas_ss}

\subsection{Definición legal de estafa en Ecuador}

Según el artículo 186 del \estcite{coip_2014}, se define a la estafa como: 

\begin{displayquote}

La persona que, para obtener un beneficio patrimonial para sí misma o para una tercera persona, mediante la simulación de hechos falsos o la deformación u ocultamiento de hechos verdaderos, induzca a error a otra, con el fin de que realice un acto que perjudique su patrimonio o el de una tercera, será sancionada con pena privativa de libertad de cinco a siete años (p. 31).
\end{displayquote}

\subsection{Sostenibilidad del sistema de Seguridad Social en Ecuador}

Según la \textcite{oit_sostenibilidad_2021}, ‘se define la sostenibilidad como la continuación de los beneficios de una intervención de desarrollo después de que esta se haya completado’ (p. 2). Así, se tiene que la sostenibilidad puede ser analizada desde diferentes contextos. Sin embargo, la sostenibilidad debe ser siempre examinada en función del proyecto en estudio, y su evaluación debe estar centrada en la capacidad de la misma para asegurar que los cambios logrados se mantengan, a pesar de que ya no sea posible mantenerlos o expandirlos \citep{oit_sostenibilidad_2021}. 

Debido a las diferentes crisis que han afectado tanto a Ecuador como a varios otros países a lo largo de los años, tales como la pandemia, crisis económicas, energéticas, migratorias, así como el envejecimiento de la población, se han generado varios retos para lograr que los fondos que son administrados por las instituciones de seguridad social sean sostenibles. Lo que ha llevado a los distintos gobiernos a enfrentar desafíos para asegurar que estos sistemas sean viables a largo plazo \citep{oit_pisos_2024}.

Es por esto que distintas organizaciones como la OIT y la Organización de Naciones Unidas (ONU) se han pronunciado con el fin de dar asistencia y fortalecer los sistemas de seguridad social. Sin embargo, la implementación de las diversas propuestas se ve obstaculizada por las dificultades presentes para la modificación de la legislación actual. Es así como, a pesar de las recomendaciones dadas por estos organismos para evitar el déficit, basándose tanto en teoría tanto como en ejemplos prácticos, se adopción resulta inviable ya que pueden desembocar tanto en problemas políticos como sociales, por lo que ninguna autoridad acepta tomar el peso de estas decisiones. De esta forma, no se toma ninguna acción frente a los problemas que existen actualmente, lo que lleva a que cada vez la situación de los fondos empeore y que el déficit sea más cercano \citep{Guatemal2023VIU}.


\subsection{Impacto y medidas necesarias frente a las estafas en la Seguridad Social}

La presencia de estafas en la Seguridad Social, ya sea al momento de obtener indebidamente prestaciones o al ejecutar acciones fraudulentas, supone un fenómeno social con graves consecuencias para toda la sociedad, afectando especialmente a ciertos grupos. Estas prácticas no solo conllevan a un deterioro social, sino que también provocan un importante desequilibrio en los recursos y gastos del sistema de Seguridad Social, cuyas consecuencias pueden afectar a una amplia parte de la población. Por ello, el estudio, la detección y el control mediante la implementación de medidas correctivas contra este tipo de fraudes son fundamentales para garantizar la sostenibilidad y el desarrollo de dicho sistema.

Como se mencionó previamente, el sistema de Seguridad Social se basa en el principio de solidaridad intergeneracional. Por lo que, la ocurrencia de estafas representa una disminución de los recursos disponibles, impactando negativamente a toda la sociedad. De ahí surge la necesidad primordial de detectar estas situaciones y combatirlas a través de planes estratégicos, con la participación activa de las diferentes áreas del IESS.

\subsection{Estudios previos acerca de Fraudes en los Sistemas de Seguridad Social}

El campo de investigación referente a los fraudes en la Seguridad Social en Ecuador, realizado por parte de los afiliados, está poco desarrollado. En contraste, en otros países, este tema ha sido ampliamente estudiado. Existen diversas investigaciones que analizan los distintos tipos de fraudes, las posibles formas de detección y las medidas a implementar para combatirlos.

\textcite{aibar2012fraude} realiza el estudio de las actividades ilegales presentadas en la Seguridad Social, relacionadas a la omisión del pago de cuotas, la manipulación de las prestaciones indebidas y la creación de empresas ficticias en España. Enfatiza las repercusiones de las mismas, como la pérdida de ingresos en la caja única de la Seguridad Social, las inscripciones fraudulentas en el sistema, complicaciones para los trabajadores en el acceso a prestaciones, competencia desleal, entre otras. Es por ello que, se han adoptado ciertas medidas para combatir el fraude como el fortalecimiento del intercambio y control de la información, análisis del riesgo e identificación de patrones de fraude, embargos preventivos para asegurar pago de deudas, control de bonificaciones y reducciones indebidas; y, creaciones de planes integrales contra el fraude. Finalmente, se reconoce la necesidad de innovar en medidas y herramienta tecnológicas para enfrentar el fraude.

De manera análoga, \textcite{Rodriguez1999} aborda la problemática de uno de los tipos de fraude relacionado con los incrementos injustificados de las bases de cotización a la seguridad social. Este fenómeno toma lugar cuando los trabajadores incrementan artificialmente sus bases de cotización en los últimos años laborales para recibir una pensión de jubilación más alta. Según este estudio, se atribuye que los casos de fraudes más comunes son cometidos por personas que tienen influencia sobre la empresa, como directivos o dueños, ya que tienen mayor facilidad para aumentar su salario.
A pesar de que la detección del fraude es compleja, debido a las diferentes estrategias para eludir normativas antifraude, se han tomado medidas como la limitación de incrementos salariales en los dos años previos a la jubilación que exceden cierto valor, permitir incrementos solo en ascensos o nuevas responsabilidades, y aplicación de códigos civiles y normativas específicas.
La detección de fraudes implementada se da mediante informes de análisis de casos donde el trabajador ha aumentado su base de cotización sin justificación. Se presta especial atención a trabajadores con posiciones de poder, se toma en cuenta aumentos salariales desproporcionados, y se analizan patrones y presunciones. 

\section{Proceso KDD}\label{sec:proceso_kdd}

Según \textcite{fayyad1996kdd}, el proceso Knowledge Discovery in Databases, abreviado como KDD, es un proceso iterativo enfocado en la obtención de la información más importante presente en grandes volúmenes de datos, en el cual se necesita la intervención humana para realizar ajustes durante las iteraciones. Por tanto, su tarea es extraer conocimiento más comprensible y útil de datos complejos y grandes. La presentación de la información obtenida depende de los objetivos de cada proceso, la misma puede darse en forma de modelos predictivos, descripciones, entre otros. 

Las etapas de este procedimiento son, en primer lugar, la selección, preparación y transformación de los datos, donde se elige un conjunto relevante de lo información proporcionada, se limpia y prepara el mismo mediante procesos como la eliminación de ruido o tratamiento de datos faltantes y posteriormente, los datos son convertidos al formato de trabajo adecuado. Luego, se lleva a cabo la aplicación de algoritmos de detección de patrones sobre los mismos, es decir, se aplican técnicas de minería de datos. Finalmente, se evalúan e interpretan los resultados.

La importancia de este algoritmo radica en que el mismo permite la automatización del análisis, es capaz de trabajar con grandes cantidades de datos, mejora la toma de decisiones mediante la información extraída, y es aplicable en todos los campos requeridos


\section{Machine Learning}\label{sec:machine_learning}

El Machine Learning o Aprendizaje Automático comprende un conjunto de algoritmos y modelos computacionales evolutivos, los cuales se han diseñado con el fin de reproducir la inteligencia humana aprendiendo y mejorando su rendimiento basándose en los datos del entorno, sin la intervención humana o tareas explícitamente programadas. Esto se logra analizando diferentes patrones y relaciones de los datos, a partir de los cuales se generan resultados \citep{ElNaqa2015}.

Este tipo de modelos se emplean, principalmente, en problemas que requieren ajustes complejos o un gran número de reglas, ya que estas técnicas simplifican el código y mejoran el rendimiento. También son eficaces para problemas complejos, donde las técnicas tradicionales no obtienen buenas soluciones, así como en entornos dinámicos, en los cuales su capacidad para reentrenarse fácilmente con nuevos datos permite mantener el modelo actualizado. Además, son útiles para la obtención de información proveniente de problemas complejos y grandes volúmenes de datos \citep{geron2023hands}. 

\subsection{Algoritmos no supervisados}\label{sec:no_supervisados}

Los algoritmos no supervisados son un enfoque del Aprendizaje Automático, cuyo objetivo es detectar patrones en puntos que no tienen estructura o etiquetas. Por lo que, el modelo es provisto únicamente de los datos de entrada, pero no de los resultados asociados a la salida. Por la naturaleza del mismo, no se necesita de supervisión humana, ya que el mismo modelo identifica y analiza los datos. \citep{alloghani2020systematic}.

Varios problemas del entorno cuentan con datos que no están etiquetados, y etiquetarlos resulta en un proceso que consume una gran cantidad de tiempo y requiere intervención humana. Por lo tanto, la importancia de estos algoritmos radica en que los mismos permiten automatizar el proceso de codificación, y de esta forma, ayudan a la investigación de datos no procesados o desconocidos. También, por su naturaleza, estos modelos son útiles para trabajar con una gran cantidad de datos y detectar patrones. \citep{naeem2023unsupervised}.


\subsubsection{Clustering jerárquico (de división)}\label{sec:cluster_jerarquico}

El algoritmo de Clustering Jerárquico se basa en la división de los elementos de la base de datos, tomando en cuenta el orden en el que los mismos se dividen. Para la representación de este proceso se hace uso de árboles o dendogramas. El algoritmo inicia con un único clúster que contiene todos los elementos de la base de datos. Luego, los separa iterativamente, removiendo de los clústeres aquellos elementos que no tienen similitudes con los demás. Finalmente, se continúa con el proceso hasta alcanzar el criterio de parada o que todos los elementos formen grupos de un solo elemento,  \citep{shetty2021hierarchical}. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.45]{graficos/jerarquico.png}
\caption{\headlinecolor{\underline{Clustering jerárquico}}}
\label{fig:cluster_jerarquico}
\vspace{2mm}
\small \textit{Fuente:} \textit{Clúster jerárquico} \citep{shetty2021hierarchical}.
\end{figure}


\subsubsection{K Means}\label{sec:k_means}
 
Según \textcite{kanagala2016comparative}, el algoritmo K-Means es un procedimiento de agrupamiento no supervisado, que agrupa o clasifica datos en clústeres tomando en cuenta su similitud. Su objetivo es dividir los datos en k conjuntos definidos, donde k es un parámetro de entrada. La agrupación se dará de forma que la distancia de los puntos a los centroides de cada grupo sea minimizada, garantizando la similaridad de los datos pertenecientes a cada clúster

El funcionamiento de este algoritmo sigue los siguientes pasos: en primer lugar, se seleccionan arbitrariamente k elementos de la base de datos como las medias o centroides iniciales de los clústeres. Luego, se calcula la distancia de cada elemento hacia los distintos centroides y se asignan al clúster más cercano. Posteriormente, se vuelve a calcular el centro de cada clúster como la media de los elementos que pertenecen al mismo. Finalmente, se repiten ambos pasos anteriores hasta que no haya cambio en los elementos que forman parte de cada conjunto \citep{kanagala2016comparative}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.40]{graficos/kmeans.png}
\caption{\headlinecolor{\underline{Algoritmo K-Means}}}
\label{fig:kmeans}
\vspace{2mm}
\small \textit{Fuente:} Resultados obtenidos con la aplicación del algoritmo K-means \citep{towardsdatascience_kmeans}.
\end{figure}

La eficacia de este modelo está condicionada a la elección inicial de los centroides, por lo que, en ciertos casos los resultados no son los esperados. Por este motivo, una mejora sustancial a este algoritmo es la utilización de k-means++. La diferencia radica principalmente en que la elección de los centroides utiliza una distribución de probabilidad proporcional a la distancia. Esto quiere decir que, cuanta más distancia exista entre un (los) centroide(s) con el resto de los elementos,  más probabilidad tiene ese elemento de ser elegido.

\subsubsection{DBSCAN}\label{sec:dbscan}

Según \textcite{kanagala2016comparative}, DBSCAN es un algoritmo de agrupamiento basado en la densidad, ya identifica las regiones de alta concentración de puntos, las cuales están separadas entre sí por áreas de baja concentración. En este algoritmo, los puntos se clasifican en tres tipos: punto core (central), aquellos que tienen más del mínimo de puntos (MinPts) especificado por el usuario dentro de un radio (Eps) de igual forma especificado por el usuario; punto border (frontera), los puntos que dentro de un radio Eps tienen menos puntos que el número MinPts, pero un punto core se encuentra en su vecindad; y puntos de ruido, que son aquellos que no son core ni border.

El algoritmo, en primer lugar, identifica de qué tipo es cada punto y se eliminan los puntos de ruido, o, dicho de otra forma, se identifican los puntos atípicos y se eliminan. Luego, se establece un borde entre los puntos cercanos unos a otros y se convierte cada grupo de puntos centrales que están conectados en grupos separados. Finalmente, los puntos frontera son asignados a los grupos de los puntos centrales que tengan asociados \citep{kanagala2016comparative}

\begin{figure}[H]
\centering
\includegraphics[scale=0.30]{graficos/dbscan.png}
\caption{\headlinecolor{\underline{Ejemplo ilustrativo del algoritmo DBSCAN}}}
\label{fig:dbscan}
\vspace{2mm}
\small \textit{Fuente:} \textit{Un ejemplo ilustrativo del algoritmo DBSCAN, con MinPts=3. El radio de los círculos representa la distancia Eps entre cada punto. Los puntos rojos son puntos centrales porque tienen al menos MinPts vecinos entre sí. Los puntos amarillos son puntos frontera porque tienen menos de MinPts puntos en sus vecindarios, y dentro de los mismos tienen un punto central. El punto azul se considera ruido porque no está dentro de la distancia Eps de ningún otro punto} \citep{aribido2021self}.
\end{figure}


\subsection{Algoritmos supervisados}\label{sec:supervisados}

Es una técnica de Aprendizaje Automático usada cuando el conjunto de datos se encuentra etiquetado, es decir, existe una preclasificación. Los datos son procesados y el algoritmo corrige sus errores iterativamente hasta que alcanza el nivel de precisión requerido \citep{unnoba2020tratamiento}.

De esta forma, el modelo va a ser entrenado con un conjunto de datos con resultados conocidos, donde la respuesta correcta es su etiqueta asociada, de los cuales aprende realizando ajustes a sus parámetros. Posteriormente, realizará predicciones de las etiquetas de nuevas observaciones. Estos algoritmos se dividen en dos tipos: de regresión y de clasificación \citep{moreno2023manual}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{graficos/supervisados.png}
\caption{\headlinecolor{\underline{Algoritmos supervisados}}}
\label{fig:supervisados}
\vspace{2mm}
\small \textit{Fuente:} Descripción genérica del flujo de entrenamiento de un algoritmo supervisado \citep{moreno2023manual}.
\end{figure}

En concordancia con el objetivo del presente trabajo de titulación, en lo que sigue se describirán los algoritmos para clasificación a utilizar.

\subsubsection{Algoritmos supervisados de clasificación}\label{sec:supervisado_clasificacion}
 
Este tipo de algoritmos son aquellos que parten con datos etiquetados, donde la variable dependiente es conocida y su tipo es categórica. Una vez entrenado el modelo, se aplica a datos nuevos y permite realizar una predicción de a qué categoría pertenece la información nueva \citep{moreno2023manual}.
% 
% \subsubsection{Support Vector Machine}\label{sec:svm}
% 
% Es un método de Aprendizaje Supervisado usado para tareas de clasificación, regresión y detección de datos atípicos. Este algoritmo busca el hiperplano óptimo que separe los datos en dos clases, esto se lo realiza maximizando la distancia entre los puntos más cercanos de cada clase. Los puntos que se encuentran en los límites se conocen como vectores de soporte, y el centro del margen representa el hiperplano que realiza la separación óptima \citep{meyer2009svm}.

\subsubsection{KNN}\label{sec:knn}
 
El algoritmo K-Nearest Neighbors (KNN) se basa en la clasificación de un punto de la base de datos considerando la categoría que es más frecuente entre sus k vecinos más cercanos, donde k es un parámetro que es establecido por el usuario. De esta manera, se etiqueta a cada punto con la clase que predomina en la mayoría de sus vecinos. Se puede dar el mismo valor a todos los vecinos del punto. Sin embargo, también hay la posibilidad de dar el peso a cada vecino dependiendo de su distancia al punto \citep{9314060}.

\subsubsection{Árboles de decisión}\label{sec:arboles}

Este algoritmo clasifica los elementos de estudio al plantear una serie de preguntas sobre las características que están asociadas a los elementos. En cada nodo se encuentra una pregunta, y a cada nodo hijo se le asigna una respuesta de la misma, formando una jerarquía al seguir este proceso. Así, los árboles se construyen aumentando nodos pregunta de manera incremental, utilizando los elementos de entramiento para determinar la elección de preguntas, donde una buena pregunta dividirá a un conjunto de elementos de clases heterogéneas en subconjuntos que tengan etiquetas más homogéneas, formando grupos con poca variación en ellos. Existen varias medidas que evalúan el grado de impureza en los conjuntos de elementos, entre las principales encontramos la entropía y el índice de Gini. Este algoritmo tiene algunas ventajas, entre las cuales destacan su fácil interpretabilidad y su flexibilidad para el manejo de características numéricas y categóricas \citep{kingsford2008decisiontrees}. 

\subsubsection{Bagging}\label{sec:bagging}

El Bagging es una técnica usada para mejorar la predicción de varios modelos. La misma extrae muestras bootstrap de los datos y, sobre cada una de ellas, se aplica un método de predicción. Finalmente, se combinan los resultados obtenidos de las diferentes muestras. En el caso de clasificación, para obtener el resultado final, se vota de manera simple y así se encuentra la predicción general \citep{SUTTON2005303}.


\subsubsection{Random Forest}\label{sec:random_forest}

Este algoritmo usa el método de Bagging, de donde, se obtienen múltiples conjuntos de entrenamiento que son diferentes entre sí. Cada uno de estos conjuntos genera un árbol de decisión en el cual se usan características seleccionadas aleatoriamente para su construcción. Debido a la gran cantidad de árboles que este algoritmo genera, se concluye que el mismo es bueno para resistir el ruido y tiene una buena capacidad de clasificación. Dependiendo de la información dada, cada árbol generará un resultado y, finalmente, con la integración de los resultados de todos los árboles se obtendrá el resultado final \citep{moreno2023manual}.

Este algoritmo, en primer lugar, se encarga de seleccionar aleatoriamente un conjunto de datos con reemplazo del conjunto de entrenamiento, formando así conjuntos de entrenamiento diversos. Luego, se escoge un grupo de variables de forma aleatoria y las escogidas se toman en cuenta para la división de nodos al construir los árboles. Finalmente, se construyen los árboles y el resultado final se obtendrá por métodos como votación por mayoría \citep{moreno2023manual}.

\subsubsection{Boosting}\label{sec:boosting}

El Boosting es una técnica usada para mejorar la precisión de los modelos. Es similar al Bagging pero con la diferencia de que en esta técnica las muestras utilizadas en cada iteración no se extraen de la población de la misma forma. Si no, los casos que han sido clasificados incorrectamente en pasos anteriores reciben un peso mayor en el siguiente paso. Así, este es un proceso interativo que incorpora pesos, y no toma un promedio simple para las predicciones \citep{SUTTON2005303}.

\subsubsection{Gradient Boosting}\label{sec:gboosting}

En este procedimiento se ajustan iterativamente nuevos modelos que van haciendo más precisa la estimación de la variable respuesta. Para ello, este algoritmo construye nuevos algoritmos que estén lo más correlacionados con el gradiente negativo de la función de pérdida, de donde, se tiene un proceso de optimización que se basa en minimizar la función de pérdida. La flexilidad de estos modelos hace que sean bastante personalizables, y permite utilizar una gran variedad de funciones de pérdida para mejorar la precisión \citep{natekin2013gradient}.

\subsubsection{XGBoost}\label{sec:xgboost}

Este algoritmo es una de las implementaciones más eficientes de árboles de decisión, el cual se ha potenciado por el gradiente. Se ha diseñado para optimizar el uso de memoria y dar uso máximo a la potencia del hardware. Con esta técnica se disminuye el tiempo de ejecución, incluso su rendimiento es mejor que los modelos de aprendizaje profundo. La idea principal del mismo es construir secuencialmente subárboles que parten de un árbol original, tal que los mismos vayan reduciendo los errores de la función de costo del árbol anterior \citep{8906396}.

\subsubsection{Redes neuronales}\label{sec:redes_neuronales}


Según \textcite{lecun2015}, las redes neuronales son modelos computacionales, cuyo funcionamiento está basado en conjuntos de capas que a su vez, están compuestas por nodos o neuronas que se encargan de procesar la información. Las mismas están diseñadas para detectar patrones, aprender y adaptarse a datos complejos. Su funcionamiento inicia cuando los nodos reciben entradas y las pasan por funciones matemáticas, estos resultados se transmiten a las neuronas de la siguiente capa para continuar con un proceso similar hasta llegar a la capa final. En este proceso se van ajustando los pesos en las conexiones de los nodos con algoritmos de optimización, como el descenso del gradiente, para disminuir el error en las predicciones. Dependiendo de los datos a procesar, hay diferentes tipos de redes y las mismas son usadas en múltiples campos por sus ventajas como su adaptabilidad, puesto que pueden generalizar combinaciones nuevas de datos; su capacidad para captar patrones de alto nivel y su eficiencia en el manejo de datos complejos ya que pueden procesos datos de varias dimensiones.

\subsection{Técnicas utilizadas}\label{sec:tec_us}

Para optimizar la eficiencia de los modelos en la obtención de resultados, fue necesario realizar un preprocesamiento de los datos. Este proceso incluyó la aplicación de diversas técnicas que ayudaron a mejorar tanto el rendimiento, como la precisión de los diferentes modelos.

\subsubsection{Balanceo de clases}\label{sec:balanceo}

Según \textcite{batista2004balancing}, las técnicas de balanceo de clases son utilizadas para mejorar el rendimiento de los sistemas de aprendizaje automático en conjuntos de datos desequilibrados. Esto ya que en datos reales es frecuente la presencia de clases desbalanceadas, lo que puede favorecer a la clase mayoritaria al no incluir los patrones de la clase minoritaria, disminuyendo su precisión.

Para el balanceo de clases se usan los métodos oversampling y undersampling. El oversampling se encarga de generar copias de ejemplos de la clase minoritaria, pero puede incrementar el riesgo de sobreajuste. Mientras que, el undersampling elimina ejemplos de la clase mayoritaria para equilibrar el conjunto, pero puede descartar datos útiles, afectando negativamente la capacidad del modelo para generalizar.

Las principales técnicas usadas para llevar a cabo los distintos tipos de balanceo son SMOTE (Synthetic Minority Over-sampling Technique) y SMOTE + ENN (Edited Nearest Neighbors).

\begin{itemize}
\item SMOTE es un método de oversampling que genera nuevos ejemplos sintéticos de la clase minoritaria interpolando con las instancias existentes, lo que reduce el sobreajuste en modelos de clasificación. Este logra que las fronteras de decisión de la clase minoritaria vayan más adentro del espacio de la clase mayoritaria, lo que mejora el rendimiento del modelo.
\item SMOTE + ENN combina la creación de nuevos ejemplos, mediante SMOTE, con la eliminación de ejemplos mal clasificados de ambas clases usando ENN, lo que elimina ruido y mejora la definición de los clústeres de clases. Este enfoque es útil especialmente en casos en los cuales hay pocos ejemplos de la clase minoritaria.
\end{itemize}

\subsubsection{Normalización}\label{sec:normalizacion}

Según \textcite{Ali2014}, la normalización comprende el proceso de ajustar los datos para que los mismos estén dentro de un rango específico, ya sea, entre 0 y 1 o entre -1 y 1. Este procedimiento es necesario cuando hay grandes diferencias en los rangos de las distintas características presentes en un conjunto de datos. Hay que tener en cuenta que este proceso tiene un mejor funcionamiento cuando no hay valores atípicos en los datos.

\subsubsection{Validación Cruzada}\label{sec:val_cruzada}

La validación cruzada es una técnica que se utiliza para evaluar la capacidad de generalización de un modelo, su capacidad predictiva y precisión, a través de la división de los datos en conjuntos de entrenamiento y prueba. Ayuda a evitar el sobreajuste y mejora la replicabilidad de los resultados \citep{Koul2018}.

Se tienen diversas técnicas de aplicación de la Validación Cruzada entre las cuales, las principales son:
\begin{enumerate}
\item Holdout: Se divide el conjunto una vez, entrenando en una parte y probando en la otra. Es simple, pero puede tener alta varianza.
\item  k-Fold: Se divide el conjunto de datos en k partes, o folds. Se entrena el modelo en k-1 partes y se prueba en la parte restante, repitiendo el proceso k veces y promediando los resultados. Esto reduce la varianza y proporciona una evaluación del modelo confiable.
\item Leave-One-Out Cross-Validation (LOOCV): Es un tipo de la técnica k-Fold donde k es el número total de observaciones. Cada observación se deja fuera una vez, y el modelo es entrenado con el resto de las observaciones. Una desventaja del mismo es que puede ser computacionalmente costoso.
\end{enumerate}

\subsubsection{Método del Codo}\label{sec:codo}

Según \textcite{yuan2019kvalue}, el método del codo es una técnica bastante usada para determinar el número de clústers en los algoritmos de agrupamiento, sobre todo en K-means. Su propósito es encontrar el número de clústers, donde si se aumentan más, ya no hay una mejora significativa en la calidad de las agrupaciones. Este método, en primer lugar, grafica la suma se lo errores cuadrados dentro de los clústers dependiendo el número de clústers elegidos, k. Con el aumento de k, este error disminuye, pero después de cierto punto, la mejora se ve reducida y forma una curva semejante a un codo. Donde, el codo marca el número óptimo de clústers ya que a partir de ellos el resultado no se ve significativamente mejorado.

\subsection{Método de la Silueta}\label{sec:silueta}

El método de la silueta es usado para evaluar la calidad de los clústeres que se forman mediante un algoritmo de agrupamiento, y representa una herramienta visual para su análisis. De esta forma, este método es ampliamente usado para determinar el número óptimo de clústeres que debe tener el agrupamiento de ciertos datos, ya que se evalúa la calidad de las agrupaciones cuando la cantidad de clústeres va cambiando. Su funcionamiento en este contexto es, primero, aplicar un algoritmo de clusterización con las diferentes cantidades de grupos a formar. Posteriormente, para cada número de clústeres se calcula la silueta para cada objeto y se promedian sus valores, encontrando la calidad de los agrupamientos obtenidos por cada modelo. Finalmente, se grafican los resultados y cuando la silueta aumenta representa un agrupamiento de mejor calidad, con esto, se observa con qué cantidad de grupos el promedio de la silueta tiene un valor más alto, tomando en cuenta también que la silueta suele estabilizarse y a pesar de que se añadan más clústeres no mejora la calidad de la agrupación. Entre sus ventajas encontramos su flexibilidad ya que puede ser aplicada a varios algoritmos de agrupamiento, su utilidad debido a que constituye un criterio para decidir el número de clústeres a elegir, y su fácil entendimiento e interpretación debido a que al permitir una representación gráfica facilita la comprensión de los resultados mostrados \citep{ROUSSEEUW198753}.

\subsubsection{Análisis de componentes principales (PCA)}\label{sec:acp}

El Análisis de Componentes Principales (PCA) es una técnica usada para la reducción de la dimensionalidad de las variables en un conjunto de datos, manteniendo la mayor cantidad de varianza posible. Se proyectan los datos originales en un espacio de menor dimensión tomando en cuenta la minimización de la distancia entre los puntos iniciales y los proyectados. \citep{wiskott2009pca}

El PCA, en primer lugar, determina las direcciones de mayor varianza de los datos a través de una matriz de covarianza. Posteriormente, diagonaliza la matriz, encontrando sus valores propios. De esta forma se proyectan los datos en las nuevas dimensiones, disminuyen la dimensionalidad y preservando la mayor parte de información de los datos originales \citep{wiskott2009pca}.


\subsubsection{Optimización de Hiperparámetros}\label{sec:hiperparametros}

Según \textcite{opthip}, la optimización de hiperparámetros es un proceso fundamental debido a su importancia para mejorar el rendimiento de los algoritmos de aprendizaje automáticos ya que con él se controlan los aspectos importantes de un modelo como la tasa de aprendizaje, parámetros de regularización, entre otros. Las diferentes elecciones de hiperparámetro llevan a rendimientos variables, por lo que encontrar una buena configuración de los mismo es necesario para mejorar la precisión del modelo. Con este propósito, tenemos dos estrategias de búsqueda:

\begin{itemize}

\item Random Search: Este método selecciona aleatoriamente valores de hiperparámetros dentro de un rango específico, sin probar todas las combinaciones ya que realiza selecciones aleatorias independientes de valores de hiperparámetros. Por tanto, este modelo permite evaluar combinaciones potencialmente prometedoras en una cantidad de tiempo menor.

\item Grid Search: Esta estrategia busca optimizar los hiperparámetros en un espacio discreto determinado. Se seleccionan conjuntos de valores para cada parámetro y posteriormente, se prueban todas las combinaciones posibles. Una desventaja de este método es que, al combinar todas las posibilidades, el método se vuelve computacionalmente ineficiente, sobre todo en espacios de alta dimensionalidad.

\end{itemize}


\subsection{Validación de resultados en modelos de clasificación}\label{sec:validacion}

Según \textcite{FAWCETT2006861}, una de las herramientas más comunes para evaluar el rendimiento de un modelo de clasificación, es la matriz de confusión. La misma tiene como objetivo medir el funcionamiento de un algoritmo, comparando los valores reales con los resultados obtenidos, donde los resultados pueden formar parte de cuatro categorías:

\begin{itemize}
\item Verdaderos positivos (VP): Casos donde el modelo predice una clase positiva correctamente.
\item Falsos Positivos (FP): Casos donde el modelo predice incorrectamente una clase positiva.
\item Verdaderos Negativos (VN): Casos en los cuales el modelo predice correctamente una clase negativa.
\item Falsos Negativos (FN): Casos donde se predice una clase negativa de forma incorrecta.
\end{itemize}

A partir de esta estructura, se pueden calcular diversas métricas que describen la capacidad predictiva del modelo y dependiendo de nuestros objetivos se deberá optar por la métrica más adecuada. Entre la mismas tenemos:

\begin{itemize}
\item Sensibilidad: Mide cuan bien un modelo identifica las instancias positivas, es decir, mide cuántas observaciones positivas se identificaron correctamente.

\[ \text{Sensibilidad} = \frac{VP}{VP+FN}\]

\item Especificidad: Mide cuán bien un modelo puede identificar las clases negativas, es decir, se identifican de forma adecuada los casos que son realmente negativos.

\[ \text{Especificidad} = \frac{VN}{VN + FP}\]

\item Precisión: Identifica los casos correctamente clasificados como positivos tomando en cuenta todos los casos que fueron clasificados como positivos.

\[ \text{Precisión} = \frac{VP}{VP+FP}\]

\item Exactitud: Corresponde a todos los casos que se clasificaron de forma correcta, es decir, aquellos casos tanto positivos como negativos que fueron clasificados correctamente.

\[ \text{Exactitud} = \frac{VP+VN}{Total}\]

\end{itemize}

Finalmente, una parte fundamental en la clasificación de instancias es la selección de los umbrales de decisión. Generalmente, en los modelos de clasificación binaria se asigna una probabilidad a cada observación de pertenecer a determinada clase, por lo que, el umbral de decisión representa el valor por encima del cual el modelo asignará una instancia a la clase positiva. El umbral predeterminado suele ser de 0.5, pero puede verse modificado según las necesidades del modelo. Si se reduce, la sensibilidad aumentará, pero disminuirá la especificidad. Mientas que, si se incrementa, la especificidad aumentará, pero disminuirá la sensibilidad.



\section{Trabajos previos}\label{sec:trabajos_previos}
 
La investigación previa relacionada con la detección de fraude en el ámbito de la seguridad social, tanto en Ecuador como en otros países, es limitada. No obstante, el uso del Machine Learning en la detección de fraudes en otros campos es amplio, especialmente en el fraude con tarjetas de crédito, donde numerosos estudios han analizado tanto las consecuencias como las medidas para enfrentar estos delitos.

\textcite{8123782} abordan la  detección de fraudes en transacciones de tarjetas de crédito, cuyo incremento se debe al aumento de las transacciones en línea, complicando la naturaleza de los comportamientos fraudulentos y su detección. En este estudio se comparan los algoritmos KNN, Naive Bayes y Regresión Logística, utilizando datos altamente desequilibrados, donde solo el 0.172\% de las transacciones eran fraudulentas. Para mejorar la detección y reducir los falsos negativos, se aplicaron técnicas de submuestreo, sobremuestreo y balanceo híbrido. El método híbrido fue el más eficaz, logrando un equilibrio entre las clases y conservando los patrones valiosos. El mejor desempeño se obtuvo con el algoritmo KNN bajo una proporción 34:66 entre transacciones fraudulentas y no fraudulentas, alcanzando una precisión de 1, sensibilidad de 0.9375 y una exactitud de 0.9792.

De manera similar, \textcite{8717766} evaluaron modelos como Random Forest y Naive Bayes para la detección de fraudes en las tarjetas de crédito. En este caso, se utilizó el sobremuestreo para mantener la información de la clase mayoritaria, y se hace énfasis en el uso de la técnica SMOTE para crear ejemplos sintéticos de la clase minoritaria.
También, menciona la importancia de la selección de variables y la partición de datos realizada con una distribución 80:20, con la cual el modelo Random Forest se destacó por su rendimiento, obteniendo una precisión de 79.21\%, recall de 81.63\% y exactitud de 99.93\%.

Por otro lado, \textcite{perols2011} se centra en la detección de fraude en estados financieros. Al igual que en los estudios previos, se compararon distintos algoritmos, destacando que las redes neuronales artificiales funcionan bien en condiciones de equilibrio, pero su rendimiento disminuye ante un desequilibrio. Las máquinas de soporte vectorial (SVM) han demostrado tener un buen rendimiento con datos desequilibrados, aunque no son las más eficientes cuando existe comparación directa. Finalmente, los métodos de ensamble sobresalen frente a los otros métodos al mejorar su precisión cuando los datos están equilibrados. Además, se hace énfasis en que el rendimiento de los algoritmos puede varias dependiendo de los datos.

Finalmente, \textcite{9103880} buscan identificar transacciones electrónicas fraudulentas mediante el uso de Xgboost, el cual superó a otros modelos como SVM, regresión logística y Random Forest, alcanzando un AUC-ROC de 0.952 y una precisión del 98.1\%. En este estudio, SMOTE se utilizó nuevamente para balancear los datos, se dio gran importancia a la ingeniería de características y de igual forma se realizó una optimización de hiperparámetros. Xgboost demostró ser particularmente efectivo en este contexto debido a su capacidad para manejar grandes volúmenes de características, mejorando tanto la velocidad de detección como la predicción.

Además de los trabajos enfocados en la detección de fraude, es crucial revisar estudios previos sobre las técnicas de identificación de valores atípicos por su importancia en el desarrollo de este trabajo  final de máster. \textcite{flores2018revision} estudian la efectividad de distintos algoritmos para este propósito, donde se concluye que los enfoques híbridos y basados en densidad son más eficaces cuando los datos no están distribuidos de manera uniforme. En contraste, los enfoques basados en distancia resultan útiles en situaciones más simples, pues son especialmente eficientes para conjuntos de datos pequeños y con una distribución uniforme. Mientras que los algoritmos de clustering, como el K-means, presentan inconvenientes debido a su sensibilidad al ruido. De igual forma, \textcite{orellana2020deteccion} complementan estos hallazgos al mencionar que la combinación de los métodos chi-cuadrado, KNN y K-Means, en su conjunto de datos, logró una efectividad de 100\% en la detección de valores atípicos. Su efectividad se vio mejorada, puesto que el KNN identificó los puntos alejados de sus vecinos cercanos, mientras que K-Means ayudó a reducir los falsos negativos generados por el KNN, confirmando si los valores detectados eran realmente atípicos. Lo que lleva a consideración de la gran efectividad de los modelos híbridos.

De forma adicional, \textcite{wei2019msd} menciona que la técnica de agrupamiento, K-means,  es utilizada para la detección de atípicos debido a su facilidad de implementación. Entre sus desventajas encontramos la sensibilidad a datos ruidosos, ya que disminuye su precisión de detectar valores atípicos. Sin embargo, a pesar de las mismas, este algoritmo tiene la ventaja de su forma de operar al asignar datos a diferentes clústeres basándose en la proximidad a los centroides de los mismos, esto, ayuda a identificar qué datos se desvían significativamente de los promedios, tomándolos como posibles valores atípicos. Además, en varios estudios se ha tomado como un proceso fundamental para combinarlo con otros algoritmos, venciendo sus desventajas y mejorando la detección. Por otro lado, este método puede ser computacionalmente costoso en la presencia de una gran cantidad de datos, debido a su sensibilidad y a la necesidad de detectar el número de clústeres. Sin embargo, la paralelización del mismo aumenta la velocidad y aumenta su eficiencia.

Por otra parte, \textcite{boucher2020outlier} analiza la efectividad de varios algoritmos de detección de datos atípicos en el estudio del fraude financiero, evaluando técnicas como DBSCAN, HDBSCAN, métodos estadísticos, Support Vector Machine, y Regresión Logística. Aquí se destaca la capacidad de detección de datos atípicos de DBSCAN y HDBSCAN debido a su capacidad de identificar clústeres con formas arbitrarias y densidades variables, lo que los hace ideales para la detección de fraudes en conjuntos de datos no homogéneos. No obstante, su rendimiento disminuye cuando los datos no se agrupan claramente. En este análisis, los métodos Isolation Forest y LOF fueron los más efectivos para la detección de valores atípicos.

Asimismo, \textcite{smiti2020outlier} mencionan que, a pesar de que DBSCAN es sensible a la configuración de parámetros, es adecuado para clústeres con formas arbitrarias. El mismo es bastante conocido por su capacidad para manejar el ruido de los datos y su eficacia para agrupar datos dispersos, lo que le permite identificar datos atípicos en áreas de menor densidad.

En cuanto al desempeño del algoritmo de Clustering Jerárquico para la detección de datos atípicos, \textcite{Hodge2004} destacan las ventajas del mismo. En primer lugar, toma en cuenta que este algoritmo permite descubrir la estructura subyacente de los datos, facilitando la detección de puntos que no pertenecen a ninguno de los clústeres identificados. También, a diferencia de otros métodos, el mismo presenta la ventaja de no necesitar un previo establecimiento del número de clústeres, lo cual es útil para la detección de datos atípicos ya que los mismos pueden formar clústeres pequeños o aislados. Finalmente, también se menciona su gran funcionalidad al permitir crear dendogramas, donde se visualizan las relaciones de jerarquía de los datos. Esto permite identificar visualmente los datos atípicos y analizar cómo difieren de los otros datos.

Finalmente, \textcite{Pamula2011} da una conclusión general acerca de los diversos algoritmos para la detección de atípicos, donde afirma que los métodos basados en distancias presentan dificultades cuando se enfrentan a una alta dimensionalidad, mientras que los enfoques basados en densidad son más precisos al identificar atípicos locales significativos, aunque a un mayor costo computacional.

\section{Herramientas tecnológicas}\label{herramientas_tecnnologicas}

Para la implementación de las diversas técnicas y modelos utilizados en este trabajo de fin de máster, se emplearon diversas herramientas tecnológicas que facilitaron tanto la etiquetación de datos como la realización de predicciones. Estos procesos se llevaron a cabo principalmente utilizando el lenguaje de programación Python, junto con sus bibliotecas especializadas.

Asimismo, R fue utilizado para la edición y compilación del documento final. La combinación de estas herramientas tecnológicas optimizó el flujo de trabajo, y a su vez mejoró la precisión y la interpretación de los modelos desarrollados.

Toda esta estructura se respaldó utilizando el sistema de control de versiones Git, creando un repositorio en GitHub. Allí, el lector podrá examinar con mayor detalle la implementación del proceso KDD, así como la aplicación de los diversos algoritmos y técnicas del aprendizaje supervisado y no supervisado que permitieron alcanzar el objetivo planteado en este trabajo de titulación.

\subsection{Python}

Python es un lenguaje de programación de alto nivel, es utilizado en diversas áreas del desarrollo de software. Fue creado por Guido van Rossum a finales de la década de 1980 y lanzado en 1991. Python se destaca por su simplicidad y legibilidad. Su versatilidad, junto con una extensa biblioteca de módulos, lo convierte en una herramienta poderosa para una variedad de aplicaciones. Además, su compatibilidad con múltiples plataformas, su amplia comunidad de soporte y su facilidad de integración son características que han contribuido a que sea uno de los lenguajes más destacados.

Las librerías utilizadas para cumplir con los propósitos del trabajo fueron las siguientes:

\begin{itemize}

\item	Pandas: Esta librería es ampliamente reconocida por su eficiencia para la manipulación y análisis de datos. Entre sus características encontramos la facilidad de lectura y escritura de datos que la misma provee. Además, es compatible con diversas bibliotecas, lo que vuelve a esta librería en una herramienta versátil. 

\item	NumPy: Esta librería es fundamental para el cálculo numérico y el análisis de datos. Por sus funcionalidades es esencial para la ciencia de datos y la computación científica, permitiendo crear estructuras de datos eficientes, realización de operaciones matemáticas, manipulación de datos, y más.

\item Scikit-learn: Esta librería es clave para la construcción de modelos de aprendizaje automático y minería de datos. Ofrece herramientas para la preparación y limpieza de datos, así como métricas para evaluar modelos y validarlos. Scikit-learn es ampliamente usada para la construcción de modelos predictivos, gracias a su gran simplicidad y eficiencia. Entre sus principales usos encontramos clasificación, regresión, clustering, reducción de dimensionalidad, preprocesamiento de datos, evaluación de modelos, creación de pipelines, entre otros.

\end{itemize}

\subsection{R}

R es un lenguaje de programación y un entorno de software diseñado para el análisis estadístico y la visualización de datos, desarrollado en la década de 1990.  Este lenguaje permite una amplia gama de funciones, entre las cuales se destaca su capacidad de integración con otros lenguajes, como \LaTeX. Esto representa una gran ventaja, especialmente para la creación de documentos técnicos y académicos. 

Entre las ventajas de esta integración encontramos la reproducibilidad de análisis y resultados, facilidad para crear gráficos de calidad y de tener una tipografía adecuada, así como la integración de diversos contenidos. Además, R incorpora potentes herramientas para la gestión de referencias, permite la automatización de informes, simplifica manejo de datos complejos, y fundamentalmente, ofrece la posibilidad del versionamiento y el control de cambios.

\subsection{Git}

El sistema de control de las versiones Git es un software distribuido y de código abierto diseñado para gestionar proyectos de desarrollo de software o cualquier tipo de archivo que requiera un seguimiento de los cambios efectuados en el mismo \citep{git_ver}. El servicio en la nube del sistema de control de versionamiento a utilizar será  Github. En esta plataforma se puede tanto almacenar  el código desarrollado, así como trabajar, colaborar y compartir un proyecto con múltiples colaboradores \citep{github2024}. El repositorio\footnote{Es el espacio de almacenamiento en la nube donde se guarda y gestiona el código del proyecto junto con su historial de versiones.} creado en github tiene por nombre \textit{VIU\_TFM} y en el mismo se encuentra todo el detalle realizado en este trabajo de fin de máster. 

