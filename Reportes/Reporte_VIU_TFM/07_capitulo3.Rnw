\chapter{Estado del arte y marco teórico}\label{cap:estado_arte}

\section{Sistema de Seguridad Social en Ecuador}\label{sistema_seg_soc}

La normativa de la Seguridad Social en Ecuador está principalmente regulada por \estcite{ConsEcu:2008}, específicamente en el artículo 34, donde se establece lo siguiente:

\begin{quote}
\setlength{\parindent}{1.27cm}
El derecho a la seguridad social es un derecho irrenunciable de todas las personas, y será deber y responsabilidad primordial del Estado. La seguridad social se regirá por los principios de solidaridad, obligatoriedad, universalidad, equidad, eficiencia, subsidiaridad, suficiencia, transparencia y participación, para la atención de las necesidades individuales y colectivas. El Estado garantizará y hará efectivo el ejercicio pleno del derecho a la seguridad social, que incluye a las personas que realizan trabajo no remunerado en los hogares, actividades para el auto sustento en el campo, toda forma de trabajo autónomo y a quienes se encuentran en situación de desempleo.
\end{quote}

El IESS es la entidad responsable de brindar servicios y prestaciones de Seguridad Social, garantizando la protección de sus asegurados a nivel nacional. Esta institución está encargada de la protección tanto de la población urbana como rural, con o sin relación de dependencia laboral, ante contingencias como enfermedad, maternidad, riesgos laborales, discapacidad, cesantía, desempleo, invalidez, vejez y muerte. Los seguros cubiertos por el IESS incluyen: el Seguro del Sistema de Pensiones, el Seguro General de Salud Individual y Familiar, el Seguro General de Riesgos del Trabajo, el Seguro Social Campesino, así como los Fondos de Terceros y el Seguro de Desempleo \citep{iess_boletin_2022}. 

El financiamiento de estos seguros proviene principalmente por las contribuciones individuales de los afiliados, el aporte patronal obligatorio de los empleadores públicos y privados, la contribución financiera obligatoria del Estado, reservas técnicas, rentas, y otro tipo de ingresos. Es necesario destacar que el funcionamiento de la Seguridad Social en el Ecuador se basa en el principio de solidaridad intergeneracional \citep{iess_boletin_2022}.

Uno de los principales seguros es el Seguro del Sistema de Pensiones, también conocido como Seguro de Invalidez, Vejez y Muerte. Este seguro cubre prestaciones como la jubilación ordinaria por vejez, la jubilación por discapacidad, la jubilación por invalidez (incluyendo el subsidio transitorio por incapacidad), así como las pensiones de montepío y el auxilio de funerales [13].

Para acceder a estas prestaciones, los individuos deben cumplir determinadas condiciones establecidas en la \legalcite{LeySS}. Una vez que se cumplan estos requisitos, la cuantía de las prestaciones se determina en la \legalcite{ResIESS_CD100}, específicamente en el artículo 13. Según esta resolución, la pensión mensual por invalidez o vejez y el subsidio transitorio por incapacidad se calcularán multiplicando la base de cálculo, obtenida de acuerdo al procedimiento descrito en el artículo 2 de dicha resolución, por el coeficiente anual de años cumplisdos de imposiciones \citep{iessivm2020}. Donde, la base de cálculo se determina como el salario promedio de los cinco mejores años de la vida laboral del individuo.

\section{Desafíos de Integridad en la Seguridad Social}\label{estafas_ss}

\subsection{Definición legal de estafa en Ecuador}

Según el artículo 186 del Código Orgánico Integral Penal del Ecuador se define a la estafa como: 

\begin{quote}
\setlength{\parindent}{1.27cm}
La persona que, para obtener un beneficio patrimonial para sí misma o para una tercera persona, mediante la simulación de hechos falsos o la deformación u ocultamiento de hechos verdaderos, induzca a error a otra, con el fin de que realice un acto que perjudique su patrimonio o el de una tercera, será sancionada con pena privativa de libertad de cinco a siete años.
\end{quote}

\subsection{Impacto y medidas necesarias frente a las estafas en la Seguridad Social}

La presencia de estafas en la Seguridad Social, ya sea al momento de obtener indebidamente prestaciones o al ejecutar acciones fraudulentas, supone un fenómeno social con graves consecuencias para toda la sociedad, afectando especialmente a ciertos grupos. Estas prácticas no solo conllevan a un deterioro social, sino que también provocan un importante desequilibrio en los recursos y gastos del sistema de Seguridad Social, cuyas consecuencias pueden afectar a una amplia parte de la población. Por ello, el estudio, la detección y el control mediante la implementación de medidas contra este tipo de fraudes son fundamentales para garantizar la sostenibilidad y el desarrollo de dicho sistema.

Como se mencionó previamente, el sistema de Seguridad Social se basa en el principio de solidaridad intergeneracional. Por lo que, la ocurrencia de estafas representa una disminución de los recursos disponibles, impactando negativamente a toda la sociedad. De ahí surge la necesidad primordial de detectar estas situaciones y combatirlas a través de planes estratégicos, con la participación activa de las diferentes áreas del IESS.

\subsection{Estudios acerca de Fraudes en los Sistemas de Seguridad Social}

El campo de investigación referente a los fraudes en la Seguridad Social en Ecuador, por parte de los afiliados, está poco desarrollado. En contraste, en otros países, este tema ha sido ampliamente estudiado. Existen estudios que analizan los distintos tipos de fraudes, las posibles formas de detección y las medidas a implementar para combatirlos.

\legalcite{aibar2012fraude} en su artículo realiza el estudio de las actividades ilegales presentadas en la Seguridad Social relacionadas a la omisión del pago de cuotas, la manipulación de las prestaciones indebidas y la creación de empresas ficticias en España. Enfatiza las repercusiones de las mismas, como la pérdida de ingresos en la caja única de la Seguridad Social, las inscripciones fraudulentas en el sistema, complicaciones para los trabajadores en el acceso a prestaciones, competencia desleal. Se han adoptado ciertas medidas para combatir el fraude como el fortalecimiento del intercambio y control de la información, análisis del riesgo e identificación de patrones de fraude, embargos preventivos para asegurar pago de deudad, control de bonificaicones y reducciones indebidas, creaciones de planes integrales contra el fraude. Finalmente, se reconoce la necesidad de innovar en medidas y herramienta tecnológicas para enfrentar el fraude.

De manera análoga, \legalcite{Rodriguez1999} aborda la problemática de uno de los tipos de fraude relacionado con los incrementos injustificados de las bases de cotización a la seguridad social. Este fenómeno cuando los trabajadores incrementan artificialmente sus bases de cotización en los últimos años laborales para recibir una pensión de jubilación más alta. Según este estudio, se atribuye que los casos de fraudes más comunes son cometidos por personas que tienen influencia sobre la empresa, como directivos o dueños, ya que tienen mayor facilidad para aumentar su salario.
A pesar de que la detección del fraude es compleja debido a las diferentes estrategias para eludir normativas antifraude, se han tomado medidas como la limitación de incrementos salariales en los dos años previos a la jubilación que exceden cierto valor, permitir incrementos solo en ascensos o nuevas responsabilidades, y aplicación de códigos civiles y normativas específicas.
La detección de fraudes implementada se da mediante informes de análisis de casos donde el trabajdor ha aumentado su base de cotización sin justificación. Se presta especial atención a trabajdores con posiciones de poder, se toma en cuena aumentos salarios desproporcionados, y se analizan patrones y presunciones. 


 \section{Machine Learning}\label{sec:machine_learning}

El Machine Learning o Aprendizaje Automático comprende un conjunto de algoritmos y modelos computacionales evolutivos, los cuales se han diseñado con el fin de reproducir la inteligencia humana aprendiendo y mejorando su rendimiento a partir de los datos del entorno sin la intervención humana o tareas explícitamente programadas. Esto se logra analizando diferentes patrones y relaciones de los datos, a partir de los cuales se generan resultados \citep{ElNaqa2015}.

Este tipo de modelos se emplea principalmente en problemas que requieren ajustes complejos o un gran número de reglas, ya que estas técnicas simplifican el código y mejoran el rendimiento. También son eficaces para problemas complejos donde las técnicas tradicionales no obtienen buenas soluciones, así como en entornos dinámicos, en los cuales su capacidad para reentrenarse fácilmente con nuevos datos permite mantener el modelo actualizado. Además, son útiles para la obtención de información de problemas complejos y grandes volúmenes de datos \citep{geron2023hands}. 

\subsection{Algoritmos no supervisados}\label{sec:no_supervisados}

Los algoritmos no supervisados son un enfoque del Aprendizaje Automático, cuyo objetivo es detectar patrones en puntos que no tienen estructura o etiquetas. Por lo que, el modelo es provisto únicamente de los datos de entrada pero no de los resultados asociados de salida. Por la naturaleza del mismo, no se necesita de supervisión humana, ya que el mismo modelo identifica y analiza los datos. \citep{alloghani2020systematic}.

Varios problemas del entorno cuentan con datos que no están etiquetados, y etiquetarlos resulta en un proceso que consume una gran cantidad de tiempo y requiere intervención humana. Por lo tanto, la importancia de estos algoritmos radica en que los mismos permiten automatizar el proceso de codificación, y de esta forma, ayudan a la investigación de datos no procesados o desconocidos. También, por su naturaleza, estos modelos son útiles para trabajar con una gran cantidad de datos y detectar patrones. \citep{naeem2023unsupervised}.

\subsubsection{K Means}\label{sec:k_means}
 
Según \legalcite{kanagala2016comparative}, el algoritmo K-Means es un algoritmo de agrupamiento no supervisado, que agrupa o clasifica datos en grupos tomando en cuenta su similitud. Su objetivo es dividir los datos en k conjuntos definidos, donde k es un parámetro de entrada. La agrupación se dará de forma que la distancia de los puntos a los centroides de cada grupo sean minimizadas, asegurando la similaridad de los datos pertenecientes a cada grupo.

El funcionamiento de este algoritmo sigue los siguientes pasos: en primer lugar, se seleccionan arbitrariamente k elementos de la base de datos como las medias o centroides iniciales de los clústeres, se calcula la distancia de cada elemento hacia los centros de los clústeres y se asignan al clúster más cercano. Posteriormente, se vuelve a calcular el centroide de cada clúster como la media de los elementos que pertenecen al mismo. Finalmente, se repiten el paso 1 y 2 hasta que no haya cambio en los elementos que forman parte de cada conjunto \citep{kanagala2016comparative}.

\subsubsection{DBScan}\label{sec:dbscan}

Según \legalcite{kanagala2016comparative}, DBScan es un algoritmo de agrupamiento basado en la densidad, ya identifica las regiones de alta densidad que están separadas entre sí por regiones de baja densidad. En este algoritmo los puntos se clasifican en tres tipos: punto central, aquellos que tienen más del mínimo de puntos especificado por el usuario (MinPts) dentro de un radio específico que igual lo determina el usuario (Eps); punto de frontera, los puntos que tienen menos punto que el número MinPts dentro de un radio Eps, pero un punto central se encuentra en su vecindad; y puntos de ruido, que son aquellos que no son centrales ni de bordes.

El algoritmo, en primer lugar, identifica de qué tipo es cada puntos y se eliminan los puntos de ruido, o de otra forma, se identifican los puntos atípicos y se eliminan. Luego, se establece un borde entre los puntos cercanos unos a otros y se convierte cada grupo de puntos centrales que están conectados en grupos separados. Finalmente, los puntos frontera son asignados a los grupos de los puntos centrales que tengan asociados \citep{kanagala2016comparative}. 

\subsubsection{Clúster jerárquico (de división)}\label{sec:cluster_jerarquico}

El algoritmo de Clúster Jerárquico se basa en la división de los elementos de la base de datos, tomando en cuenta el orden en el que los mismos se dividen. Para la representación de este proceso se hace uso de árboles o dendogramas. El algoritmo inicia con un único clúster que contiene todos los elementos de la base de datos. Luego, los separa iterativamente, removiendo de los clústers aquellos elementos que no tienen similitudes con los demás. Finalmente, se continúa el proceso alcanzar el criterio de parada p que todos los elementos formen grupos de un solo elemento \citep{shetty2021hierarchical}. 

\subsection{Algoritmos supervisados}\label{sec:supervisados}

Es una técnica de Aprendizaje Automático usada cuando el conjunto de datos se encuentra etiquetado, es decir, existe una preclasificación. Los datos son procesados y el algoritmo corrige sus errores iterativamente hasta que alcanza el nivel de precición requerido \citep{unnoba2020tratamiento}.

Es decir, el modelo va a ser entrenado con un conjunto de datos con resultados conocidos, donde la respuesta correcta en su etiqueta asociada, de los cuales aprende realizando ajustes en sus parámetros. Esto con el fin de, posteriormente, realizar predicciones de las etiquetas de nuevas observaciones. Estos algoritmos se dividen en dos tipos: de regresión y de clasificación (falta cita).

\subsubsection{Algoritmos supervisados de clasificación}\label{sec:supervisado_clasificacion}
 
Este tipo de algoritmos son aquellos que parten con datos etiquetados, donde la variable dependiente es conocida y su tipo es categórica. Una vez entrenado el modelo, se aplica a datos nuevos y permite realizar una predicción de a qué categoría pertenece la información nueva falta citar.

\subsubsection{Support Vector Machine}\label{sec:svm}

Es un conjunto de métodos de aprendizaje supervisado usados para tareas de clasificación, regresión y detección de datos atípicos. Este algoritmo busca el hiperplano óptimo que separe los datos en dos clases, esto lo realiza maximizando el margen entre los puntos más cercanos de cada clase. Los puntos que se encuentran en los límites se conocen como vectores de soporte, y el centro del margen representa el hiperplano de separación óptimo. falta citar documento 1
 
\subsubsection{KNN}\label{sec:knn}
 
El algoritmo KNN se basa en la clasificación de un punto de la base de datos considerando la categoría más frecuente entre sus k vecinos más cercanos, donde k es un parámetro establecido por el usuario. De esta manera, se etiqueta a cada punto con la clase predominante de la mayoría de vecinos. Se puede dar el mismo valor a todos los vecinos del punto. Sin embargo, también hay la posibilidad de dar el peso a cada vecino dependiendo de su distancia al punto \citep{9314060}.

\subsubsection{Árboles de decisión}\label{sec:arboles}

Este algoritmo clasifica los elementos de estudio al plantear una serie de preguntas sobre las características que están asociadas a los elementos. En cada nodo se encuentra una pregunta, y a cada nodo hijo le corresponde una respuesta de la misma, siguiendo este proceso se forma una jerarquía. Así, los árboles se construyen aumentando nodos pregunta de manera incremental, utilizando los elementos de entramiento para determinar la elección de preguntas, donde una buena pregunta dividirá un conjunto de elementos de clases heterogéneas en subconjuntos con etiquetas más homogéneas, formando estratos tales que haya poca variación en cada uno de ellos. Existen varias medidas que evalúan el grado de impureza en los conjuntos de elementos, entre las principales encontramos la entropía y el índice de Gini. Este algoritmo tiene algunas ventajas, entre las cuales destacan su fácil interpretabilidad y su flexibilidad para el manejo de características numéricas y categóricas \citep{kingsford2008decisiontrees}. 

\subsubsection{Bagging}\label{sec:bagging}

El Bagging es una técnica usada para mejorar la predicción de varios modelos La misma extrae muestras Bootstrap de los datos y, sobre cada una de ellas, se aplica un método de predicción. Finalmente, se combinan los resultados obtenidos de las diferentes muestras. En el caso de clasificación, para obtener el resultado final, se vota de manera simple y así se encuentra la predicción general \citep{SUTTON2005303}.


\subsubsection{Random Forest}\label{sec:random_forest}

Este algoritmo usa el método de Bagging para obtener múltiples conjuntos de entrenamiento diferentes entre ellos. Cada uno de estos conjuntos genera un árbol de decisión  que utiliza atributos aleatorios. Debido a la cantidad de árboles que este algoritmo genera, se concluye que el mismo es bueno para resistir el ruido y tiene una buena capacidad de clasificación. Dependiendo de la información dada, cada árbol generará un resultado, y finalmente, con la integración de los resultados de todos los árboles se obtendrá el resultado final \citep{moreno2023manual}.

Este algoritmo, en primer lugar, se encarga de seleccionar aleatoriamente un conjunto de datos con reemplazo del conjunto de entrenamiento usando el método de Bagging, formando así conjuntos de entrenamiento diversos. Luego, se escoge un grupo de variables de forma aleatoria y las escogidas se toman en cuenta para la división de nodos al construir los árboles. Finalmente, se construyen los árboles y el resultado final se obtendrá por métodos como votación por mayoría \citep{moreno2023manual}.

\subsubsection{Boosting}\label{sec:boosting}

El Boosting es una técnica usada para mejorar la precisión de los modelos. Es similar al Bagging pero con la diferencia de que en esta técnica las muestras utilizadas en cada iteración no se extraen de la población de la misma forma. Si no, los casos que han sido clasificados incorrectamente en pasos anteriores, reciben un peso mayor en el siguiente paso. Así, este es un proceso interativo que incorpora pesos, y no toma un promedio simple para las predicciones \citep{SUTTON2005303}.


\section{Trabajos previos}\label{sec:trabajos_previos}
 
La investigación previa relacionada con la detección de fraude en el ámbito de la seguridad social, tanto en Ecuador como en otros países, es limitada. No obstante, el uso del Machine Learning en la detección de fraudes en otros campos es amplio, especialmente en el fraude con tarjetas de crédito, donde numerosos estudios han analizado tanto las consecuencias como las medidas para enfrentar estos delitos.

En \legalcite{8123782}, se aborda la  detección de fraudes en transacciones de tarjetas de crédito, cuyo incremento se debe la aumento de las transacciones en línea, complicando la naturaleza de los comportamientos fraudulentos y su detección. En este estudio se comparan los algoritmos KNN, Naive Bayes y Regresión Logística,, utilizando datos altamente desequilibrados, donde solo el 0.172\% de las transacciones eran fraudulentas. Para mejorar la detección y reducir los falsos negativos, se aplicaron técnicas de submuestreo, sobremuestreo y balanceo híbrido. El método híbrido fue el más eficaz para, logrando un equilibrio entre las clases y conservando los patrones valiosos, y el mejor desempeño se obtuvo con el algoritmo KNN bajo una proporción 34:66 entre transacciones fraudulentas y no fraudulentas, alcanzando una precisión de 1, sensibilidad de 0.9375 y una exactitud de 0.9792.

De manera similar, el estudio \legalcite{8717766} evaluó modelos como Random Forest y Naive Bayes para la detección de fraudes en las tarjetas de crédito. En este caso, se utilizó el sobremuestreo para mantener la información de la clase mayoritaria, y se hace enfásis en el uso de la técnica SMOTE para crear ejemplos sintéticos de la clase minoritaria.
También, menciona la importancia de la selección de variables y la partición de datos realizada con una distribución 80:20, con la cual el modelo Random Forest se destacó por su rendimiento, obteniendo una precisión de 79.21\%, recall de 81.63\% y exactitud de 99.93\%.

Por otro lado, el paper \legalcite{10.2308/ajpt-50009} se centra en la detección de fraude en estados financieros. Al igual que en los estudios previos, se compararon distintos algoritmos, destacando que las redes neuronales artificiales funcionan bien en condiciones de equilibrio, pero su rendimiento disminuye ante un desequilibrio. Las máquinas de soporte vectorial (SVM) han demostrado tener un buen rendimiento con datos desequilibrados, aunque no son las más eficientes cuando existe comparación directa. Finalmente, los métodos de ensamble sobresalen frente a los otros métodos al mejorar su precisión cuando los datos están equilibrados. Además, se hace énfasis en que el rendimiento de los algoritmos puede varias dependiendo de los datos.

Finalmente, en \legalcite{9103880}, se busca identificar transacciones electrónicas fraudulentas mediante el uso de Xgboost, el cual superó a otros modelos como SVM, regresión logística y Random Forest, alcanzando un AUC-ROC de 0.952 y una precisión del 98.1\%. En este estudio, SMOTE se utilizó nuevamente para balancear los datos, se dio gran importancia a la ingeniería de características y de igual forma se realizó una optimización de hiperparámetros. Xgboost demostró ser particularmente efectivo en este contexto debido a su capacidad para manejar grandes volúmenes de características, mejorando tanto la velocidad de detección como la predicción.

Además de los trabajos enfocados en la detección de fraude, es crucial revisar estudios previos sobre las técnicas de detección de valores atípicos. El artículo \estcite{flores2018revision} destaca la efectividad de distintos algoritmos para este propósito, donde se concluye que los enfoques híbridos y basados en densidad son más eficaces cuando los datos no están distribuidos de manera uniforme. En contraste, los enfoques basados en distancia resultan útiles en situaciones más simples, mientras que los algoritmos de clustering, como el k-means, son menos recomendables debido a su sensibilidad al ruido. Los enfoques basados en distancia son especialmente eficientes para conjuntos de datos pequeños y con una distribución uniforme. De igual forma, el paper 1390-6542-enfoqueute-11-01-00056.pdf complementa estos hallazgos al mencionar que la combinación de los métodos chi-cuadrado, KNN y K-Means, en su conjunto de datos, logró una efectividad de 100\% en la detección de valores atípicos. Su efectividad se vio emjorada, puesto que el KNN identificó los puntos alejados de sus vecinos cercanos, mientras que K-Means ayudo a reducir los falsos negativos generado por el KNN, confirmando si los valores detectados eran realmente atípicos.

Por otra parte, el artículo \estcite{boucher2020outlier} analiza la efectividad de varios algoritmos de detección de datos atípicos en el estudio del fraude financiero, evaluando técnicas como DBSCAN, HDBSCAN, métodos estadísticos, Support Vector Machine, y Regresión Logística. Aquí se destaca la capacidad de detección de datos atípicos de DBSCAN y HDBSCAN debido a su capacidad de identificar clústeres con formas arbitrarias y densidades variables, lo que los hace ideales para la detección de fraudes en conjuntos de datos no homogéneos. No obstante, su rendimiento disminuye cuando los datos no se agrupan claramente. En este análisis, los métodos Isolation Forest y LOF fueron los más efectivos para la detección de valores atípicos.

Asimismo, los artículos \estcite{smiti2020outlier} y \estcite{Pamula2011} mencionan que, aunque DBSCAN es adecuado para clústeres con formas arbitrarias, es sensible a la configuración de parámetros. Los métodos basados en distancias presentan dificultades cuando se enfrentan a alta dimensionalidad, mientras que los enfoques basados en densidad son más precisos al identificar outliers locales significativos, aunque a un mayor costo computacional. Los métodos basados en clustering, por su parte, manejan bien el ruido y los valores atípicos, pero dependen considerablemente de una adecuada configuración de parámetros. En este sentido, K-Means se considera una opción eficiente para agrupar datos no atípicos y reducir el costo computacional.


\section{Herramientas tecnológicas}\label{herramientas_tecnnologicas}

Para la implementación de las diversas técnicas y modelos utilizados en este trabajo de fin de máster, se emplearon diversas herramientas tecnológicas que facilitaron tanto la etiquetación de datos como la realización de predicciones. Estos procesos se llevaron a cabo principalmente utilizando el lenguaje de programación Python, junto con sus bibliotecas especializadas.

Asimismo, R fue utilizado para la edición y compilación del documento final. La combinación de estas herramientas tecnológicas optimizó el flujo de trabajo, y a su vez mejoró la precisión y la interpretación de los modelos desarrollados.

\subsection{Python}

Python es un lenguaje de programación de alto nivel, es utilizado en diversas áreas del desarrollo de software. Fue creado por Guido van Rossum a finales de la década de 1980 y lanzado en 1991. Python se destaca por su simplicidad y legibilidad. Su versatilidad, junto con una extensa biblioteca de módulos, lo convierte en una herramienta poderosa para una variedad de aplicaciones. Además, su compatibilidad con múltiples plataformas, su amplia comunidad de soporte y su facilidad de integración son características que han contribuido a que sea uno de los lenguajes más destacados.

Las librerías utilizadas para cumplir con los propósitos del trabajo fueron las siguientes:

\begin{itemize}

\item	Pandas: Esta librería es ampliamente reconocida por su eficiencia para la manipulación y análisis de datos. Entre sus características encontramos la facilidad de lectura y escritura de datos que la misma provee. Además, es compatible con diversas bibliotecas, lo que vuelve a esta librería en una herramienta versátil. 

\item	NumPy: Esta librería es fundamental para el cálculo numérico y el análisis de datos. Por sus funcionalidades es esencial para la ciencia de datos y la computación científica, permitiendo crear estructuras de datos eficientes, realización de operaciones matemáticas, manipulación de datos, y más.

\item Scikit-learn: Esta librería es clave para la construcción de modelos de aprendizaje automático y minería de datos. Ofrece herramientas para la preparación y limpieza de datos, así como métricas para evaluar modelos y validarlos. Scikit-learn es ampliamente usada para la construcción de modelos predictivos, gracias a su gran simplicidad y eficiencia. Entre sus principales usos encontramos clasificación, regresión, clustering, reducción de dimensionalidad, preprocesamiento de datos, evaluación de modelos, creación de pipelines, entre otros.

\end{itemize}

\subsection{R}

R es un lenguaje de programación y un entorno de software diseñado para el análisis estadístico y la visualización de datos, desarrollado en la década de 1990.  Este lenguaje permite una amplia gama de funciones, entre las cuales se destaca su capacidad de integración con otros lenguajes, como Latex. Esto representa una gran ventaja, especialmente para la creación de documentos técnicos y académicos. 

Entre las ventajas de esta integración encontramos la reproducibilidad de análisis y resultados, facilidad para crear gráficos de calidad y de tener una tipografía adecuada, así como la integración de diversos contenidos. Además, R incorpora potentes herramientas para la gestión de referencias, permite la automatización de informes, simplifica manejo de datos complejos, y fundamentalmente, ofrece la posibilidad del versionamiento y el control de cambios.






