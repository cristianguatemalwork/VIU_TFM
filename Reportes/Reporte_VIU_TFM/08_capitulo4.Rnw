\chapter{Resultados}\label{cap:resul}

\section{Contexto y problema planteado}

Conforme lo expuesto en el capítulo introductorio y en consonancia con los objetivos planteados, el propósito de este trabajo de fin de máster es que, através de las aportaciones históricas realizadas por los pensionistas del Seguro IVM, detectar mediante técnicas de aprendizaje no supervizado aquellas que son fraudulentas y luego generar modelos de aprendizaje supervizado que permitan clasificar si dentro de las aportaciones que realizan los actuales (hasta diciembre del 2023) afiliados existen casos que podrían realizar fraude, en función del comportamiento de su materia gravada\footnote{Es todo ingreso susceptible de apreciación pecuniaria, percibido por la persona afiliada, o en caso del trabajo no remunerado del hogar, por su unidad económica familiar.}, con el fin de generar un mayor beneficio al momento de solicitar la jubilación.\\

El primer paso es la generación de un adecuado proceso KKD (de acuerdo a lo descrito en la sección \ref{sec:proceso_kdd}), para que a partir de los datos primarios que dispone el IESS, generar el conocimiento adecuado que permita dar solución al problema planteado. 

Un aspecto importante a considerar es que la información inicial (llamada \textit{data\_afi}) con la cual se generó la primera parte del proceso KDD es resultado de un preprocesamiento\footnote{La fecha de extracción de la información fue a mayo 2024 y el análisis se centran en todos los registros anteriores al 31 de diciembre de 2023.} a las bases transaccionales del IESS. La manipulación a estas bases fue realizada mediante lenguaje pl/SQL en la base de datos oracle. El esquema de las tablas transaccionales y el código utilizado para la generación de la información inicial se detalla en el anexo \ref{cap:anexo1}.  

Por otra parte, es preciso indicar que todo el proceso de limpieza, preprocesamiento, transformación de datos e implementación de los diferentes algoritmos y técnicas de aprendisaje supervisado y no supervisado, se lo realizó usando el lenguaje de programación Python. El alojamiento del código  se lo realiza mediante versionamiento a través de la herramienta \textit{git}, creando un repositorio (de nombre \textit{VIU\_TFM}) en Github (Véase \legalcite{CristianVIUTFM}). En \textit{VIU\_TFM} se encuentran los scripts de Python que fueron utilizados para alcanzar el objetivo planteado y los scripts de R utilizados en la edición del presente proyecto en formato \texttt{\LaTeX}.


\section{Fuente de datos}

Dentro de todas las fuentes de información que dispone el IESS, se consideran las bases de datos transaccionales que guardan la información de Historia Laboral\footnote{Historia Laboral contiene la información de la vida laboral del afiliado.} (HL) y Prestacional\footnote{La base Prestacional contiene la información del pensionista.} (Pres). Adicionalmente, la información se complementa con los datos proporcionados por el Registro Civil\footnote{Entidad pública que guarda los datos personales de la población ecuatoriana}.

\section{Selección de datos} \label{sec:seleccion_datos}

Para tener información consolidada se hicieron joins entre las tablas de la sección \ref{sec_anex:fuentes_datos} de la parte de anexos. Las tablas resultantes con la información de HL (\textit{data\_afi})  y Pres (\textit{pensi}) tenían un sinnúmero de variables que no estaban involucradas con el problema a abordar, por lo que, solo se seleccionaron aquellos atributos relacionados con el objetivo planteado.

Los atributos que componen a \textit{data\_afi} son los siguientes:

\begin{multicols}{2}  
    \begin{itemize}
        \item CEDULA\_COD: Secuencial que identifica al afiliado,\\[-0.75cm]
        \item ANIO: Corresponde al año del pago de la planilla del afiliado,\\[-0.75cm]
        \item MES: Corresponde al mes del pago de la planilla del afilaido,
    \end{itemize}
    \begin{itemize}  
        \item CODSEC: Corresponde a la abreviación del sector en el cual trabajó el afiliado,\\[-0.75cm]
        \item SECTOR: Corresponde al sector en el cual trabajó el afiliado,\\[-0.75cm]
        \item SALARIO: Corresponde al salario sobre el cual aportó el afiliado,
    \end{itemize}
\end{multicols}

La información de \textit{pensi} tiene las siguientes variables:

\begin{multicols}{2}  
    \begin{itemize}
        \item CEDULA\_COD: Secuencial que identifica al pensionista,\\[-0.75cm]
        \item SEXO: Es el sexo del pensionista,\\[-0.75cm]
        \item FECHA\_NACIMIENTO: Es la fecha de nacimiento del pensionista,\\[-0.75cm]
        \item FECHA\_MUERTE: Es la fecha de muerte del pensionista,\\[-0.75cm]
        \item FECHA\_DERECHO: Es la fecha de derecho\footnote{Es la fecha en la cual una persona tiene derecho a solicitar su jubilación} del pensionista,\\[-0.75cm]
        \item N\_MESES: Es el número de meses trabajados,\\[-0.75cm]
        \item NUMERO\_IMPOSICIONES: Es el número de imposiciones\footnote{Una imposición corresponde a treinta (30) días laborados},\\[-0.75cm]
         \item COEFICIENTE\_REAL: Valor del coeficiente\footnote{Es el valor del coeficiente anual de años cumplidos de imposiciones acorde a la \legalcite{ResIESS_CD100}} en el sistema Prestacional,\\[-0.75cm]
          \item COEFICIENTE\_CALCULADO: Valor del coeficiente calculado en base a HL,\\[-0.75cm]
          \item ID\_PRESTACION: Corresponde al identificador único de la prestación,\\[-0.75cm]
    \end{itemize}
    \begin{itemize}  
       
        \item PROMEDIO\_SUELDO\_REAL: Es la base de cálculo\footnote{De acuerdo con la \legalcite{ResIESS_CD100}, para el cómputo de la base de cálculo de la pensión, se procederá a la suma de doce (12) meses de imposiciones consecutivas y ese resultado se dividirá para doce (12). Obteniendo así el promedio mensual de los sueldos o salarios de cada año de imposiciones del afiliado, se seleccionarán los cinco (5) promedio mensuales de mayor cuantía y el resultado de la suma se dividirá para cinco (5)} en el sistema Prestacional,\\[-0.75cm]
          \item PROMEDIO\_CAL: Estimación de la base de cálculo en base a HL,\\[-0.75cm]
        \item VALOR\_PENSION: Corresponde a la pensión calculada en el sistema Prestacional,\\[-0.75cm]
        \item PENSION\_CAL: Esla pensión calculada en función del artículo 13\footnote{La pensión mensual por invalidez o vejez y el subsidio transitorio por incapacidad será igual al resultado de la multiplicación de la Base de Cálculo de la \legalcite{ResIESS_CD100}, por el coeficiente anual de años cumplidos de imposiciones. De los 40 años en adelante se incrementará el 0,0125 por cada año de imposiciones adicionales.} de la \legalcite{ResIESS_CD100},\\[-0.75cm]
        \item RANGO\_INI\_5MEJ: Corresponde a la fecha máxima de los cinco (5) mejores años de sueldo,\\[-0.75cm]
        \item RANGO\_FIN\_5MEJ: Corresponde a la fecha mínima de los cinco (5) mejores años de sueldo

    \end{itemize}
\end{multicols}

La información de \textit{pensi} es dividida en tres grupos, para las jubilaciones de vejez, para las de invalidez y para las de discapacidad. El nombre de estos tres conjuntos de datos serán \textit{data\_vej}, \textit{data\_inv} y \textit{data\_dis} respectivamente.


En lo que sigue, el proceso KDD se divide en dos partes. La primera parte se centra en el proceso de minería de datos para la información utilizada antes de la ejecución de los algoritmos de aprendizaje no supervizado, misma que tendrá con fin etiquetar los salarios y a las personas que generaron alguna aportación fraudelenta. La seguna parte corresponde al preprocesamiento de la información resultante de los algoritmos no supervizados, para su posterior utilización dentro de los modelos de aprendisaje supervisado.

%Tratamiento de valores perdidos--------------------------------------------------------------------
\section{Tratamiento de valores perdidos} \label{chp:resultados:sec_valores_perdidos}

Los atributos de \textit{data\_afi} no contienen valores perdidos. Por el contrario, \textit{data\_vej}, \textit{data\_inv} y \textit{data\_dis} si tienen variables con valores perdidos. Estas variables son la FECHA\_MUERTE, NUMERO\_IMPOSICIONES y el COEFICIENTE\_CAL. La corrección de sus valores, a excepción de FECHA\_MUERTE\footnote{No se hace un tratamiento de datos faltante para esta variable debido a que no se la considera en el análisis}, se explica en la sección \ref{subsec:trans_datos_super}.

El principal problema con los datos faltantes  se debe a que la información de historia laboral está registrada en dos sistemas de información, HL y Host\footnote{Sistema de información de los aportes anteriores a la aparación del sistema HL}. Esta estructura afecta directamente al esquema Prestacional y por ende a los pensionistas.

%Tratamiento de outliers----------------------------------------------------------------------------
\section{Tratamiento de outliers}

La principal variable de interés en \textit{data\_afi} es el \textit{SALARIO}, misma que permitirá clasificar  si las personas realizaron algún tipo de fraude, con el fin de obtener un mejor beneficio pensional a largo plazo. En este sentido, no se realiza ninguna corrección de sus valores pues es justamente lo que queremos detectar con los algoritmos de aprendizaje no supervisado.

%Limpieza de datos antes del aprendizaje no supervisado---------------------------------------------
\section{Limpieza de datos antes del aprendizaje no supervisado}\label{cha_resul:limpieza_no_super}

Como ya se mencionó en la sección \ref{chp:resultados:sec_valores_perdidos}, \textit{data\_afi} no va a presentar problemas con missing values ni tampoco con datos inconsistentes en ningún atributo, debido a que estas novedades ya se resolvieron durante su fase de construcción, cuando se utilizó pl/SQL.

Por otro parte, para corregir la información de \textit{data\_vej}, \textit{data\_inv} y \textit{data\_dis} se considera la información del Registro Civil. Esto debido a que se evidenció que los registros administrativos del IESS están contaminados y son inconsistentes desde su origen\footnote{Inicialmente la generación de planillas (documento que contiene la información del aporte del afiliado) era de forma manual y en documentos físicos, por lo que, una vez que pasaron a digitalizarse arrastraron un centenar de novedades, pues era un proceso manual. Actualmente con HL ya se corrigen gran parte de las novedades.}. Los novedades encontradas eran mal ingreso del sexo (por ejemplo existían categorías con valores de H, M, 1, 2, null, hombre, mujer), fechas de nacimiento y muerte erróneas (existían personas que murieron antes de nacer o nacían por el año 2030) con distintos formatos (por ejemplo dd/mm/yyy, yyyy/dd/mm, mm/dd/yyyy ); y, en muchos casos una alta presencia de missing values.\\

Dar solución a cada problema detectado requiere de un amplio tiempo de análisis y ejecución, además de que se debe realizar una limpieza de datos desde el origen del registro administrativo (información generada desde el 13 de marzo de 1928. Para mayor detalle \href{https://www.iess.gob.ec/en/web/afiliado/noticias?p_p_id=101_INSTANCE_3dH2&p_p_lifecycle=0&p_p_col_id=column-2&p_p_col_count=4&_101_INSTANCE_3dH2_struts_action=\%2Fasset_publisher\%2Fview_content&_101_INSTANCE_3dH2_assetEntryId=2246192&_101_INSTANCE_3dH2_type=content&_101_INSTANCE_3dH2_groupId=10174&_101_INSTANCE_3dH2_urlTitle=iess-celebra-hoy-86-anos-de-servicio-al-pais&redirect=\%2Fen\%2Fweb\%2Fafiliado\%2Fnoticias?mostrarNoticia=1#:~:text=Posteriormente\%2C\%20el\%2025\%20de\%20julio,Ecuatoriano\%20de\%20Seguridad\%20Social\%2DIESS.}{click aquí}). Por ello, con el fin de reducir los tiempos de análisis se hizo merge por DNI (foreing key) entre las tablas del IESS y el Registro Civil para tener información personal estándar y consistente.


Así, el dataset con la información laboral contiene 68.062.270 filas y 12 columnas, \textit{data\_vej} tiene 681.799 filas con 17 columnas, \textit{data\_inv} tiene 48.146 filas y 17 columnas y \textit{data\_dis} tiene 14.662 filas con 17 columnas.

%Transformación de datos antes del aprendizaje no supervisado---------------------------------------
\section{Transformación de datos antes del aprendizaje no supervisado}\label{subsec:trans_datos_no_super}

Con la finalidad de aliviar el costo computacional, se transforman algunas variables de \textit{data\_afi} en atributos más sencillos y se crean variables que permitirán alcanzar el objetivo planteado. Todo el proceso de transformación de los datos se describe en la sección \ref{sec:limpieza_datos} de la parte de anexos. Si el lector desea tener una experiencia más didáctica con todo el proceso, puede hacerlo visualizando el archivo  \textit{VIU\_clean\_data.ipynb} del \legalcite{CristianVIUTFM}.

De manera general, en lo que sigue se muestran las variables que fueron creadas:

\begin{itemize}
\item SUELDO: Es la suma de la materia gravada histórica del afiliado hasta el momento de su jubilación, \\[-0.75cm]
\item APORTE: Es la suma del aporte\footnote{Es la multiplicación entre la materia gravada del afiliado, mes a mes, con el valor de la tasa de aportación. Estos valores se presentan en la tabla \ref{tab:info_tasa_aport_actuarial} de la sección \ref{sec:limpieza_datos} de la parte de anexos.} del afiliado hasta el momento de su jubilación,\\[-0.75cm]
\item INTERES\_APORTE: Corresponde a la suma por concepto del interés\footnote{Corresponde a la multiplicación entre la variable APORTE, mes a mes, con la tasa de interés actuarial. Estos valores se presentan en la tabla \ref{tab:info_tasa_aport_actuarial} de la sección \ref{sec:limpieza_datos} de la parte de anexos.} que generó el aporte del afiliado durante su vida laboral.\\[-0.75cm]
\item FIN\_HL: Corresponde a la fecha de la última planilla generada por concepto de aportes durante a vida laboral del afiliado\\[-0.75cm]
\item INI\_HL: Corresponde a la fecha de la primera planilla generada por concepto de aportes durante a vida laboral del afiliado\\[-0.75cm]
\item MES\_AS: Corresponde al número de meses aportados\footnote{Es el conteo del número de veces no únicos, es decir, si una persona aporta dos o más veces en el mismo mes, se le cuenta dos o más veces} durante la vida laboral del afiliado\\[-0.75cm]
\item MES\_TU: Corresponde al número de meses distintos\footnote{Es el conteo del número de meses únicos, es decir, si una persona aportó dos o más veces en el mismo mes, solo se le contabiliza como una vez} aportados durante la vida laboral del afiliado,\\[-0.75cm]
\item  N\_PRI, N\_PUB, N\_IND, N\_VOL\_EX y N\_VOL\_EC: Guardan la información sobre las veces que el afiliado aportó en los sectores Privado, Público, Independiente, Voluntario del Exterior y Voluntario residente del Ecuador, respectivamente.

\end{itemize}

En base a los artículos 2 y 13 de la \legalcite{ ResIESS_CD100}, se procede a estimar la base de cálculo. Para esto, se crean las variables GRUPO\_SEL\footnote{Es una variable indicatriz, que toma los valores de 1, si para un mes x del año Y, el registro se cuenta para la base de cálculo; 0 caso contrario} y BASE\_CAL\footnote{Es el valor estimado de la Base de Cálculo en concordancia con el artículo 2 de la \legalcite{ResIESS_CD100}.}. Adicionalmente, se crean las variables SBU\footnote{Contiene la información del Salario Básico Unificado (SBU) de los trabajadores del Ecuador.}, cuyos valores son los registrados por el \citet{bce_salarios}, INI\_CAL y FIN\_CAL que corresponden a la fecha máxima y mínima de la base de cálculo respectivamente; y, M\_PRI, M\_PUB, M\_IND, M\_VOL\_EX y M\_VOL\_EC que guardarán la información sobre las veces que el afiliado aportó dentro de sus mejores años de aportación en los sectores Privado, Público, Independiente, Voluntario del Exterior y Voluntario residente del Ecuador respectivamente.

El dataset resultante de este proceso de datos se llamará \textit{data\_l}, mismo que será utilizado para la generación de la etiqueta de fraude o no fraude en las aportaciones. El tabulado contiene 62.130.167 filas y 16 atributos, que corresponden a los registros salariales de  442.570 personas. También es necesario crear las variables LS1, SAL\_PROM1, LS2 y SAL\_PROM2, que corresponden al límite superior\footnote{Se define como la suma entre el tercer cuartil + 1,5 veces el rango inter cuartilico.} del bigote del diagrama de caja y sueldo promedio para los salarios históricos y a partir del año 2000\footnote{Debido a la crisis ecónomica por la cual atravesaba el Ecuador, a partir del año 2000 el país cambia los súcres (como moneda nacional)  a dólares estadounidenses como moneda oficial \citet{Ecua_sucre_dolar}} en adelante respectivamemte, debido a que Ecuador cambió su moneda, por lo que, las contribuciones realizadas en sucres y su posterior transformación a dólares con la tasa de cambios a esa fecha, eran significativamente menores al valor del SBU del año 2000 y así en adelante. En este sentido, se dará mayor importancia a los valores de las variables LS2 y SAL\_PROM2.

%Aprendizaje no supervisado-------------------------------------------------------------------------
\section{Aprendizaje no supervisado}

Para la implementación del aprendizaje no supervisado se utiliza la base de datos \textit{data\_l}. Los algoritmos a utilizar son Clúster jerárquico, K-Means y DBSCAN, puesto que como se menciona en la sección \ref{sec:trabajos_previos} del capítulo \ref{cap:estado_arte}, los mismos permitarán realizar la búsqueda de los salarios que son atípicos dentro del comportamiento salarial del afiliado.

\subsection{Clúster jerárquico} \label{sec:apr_no_sup_cj}

La selección de este algoritmo en la detección de atípicos se basa en sus ventajas para identificar la estructura inherente de los datos, lo que permite detectar aquellos que no pertenecen a ningún clúster. Además, proporciona una visión clara de la relación entre los datos atípicos y el resto de los datos mediante el uso de dendrogramas, lo que facilita visualizar las diferencias y relaciones entre ellos, enfocando el análisis hacia la dirección adecuada en este proceso \citep{Hodge2004}. Así, para  el análisis, solo se considerarán los valores  iguales a uno (1) de la variable GRUPO\_SEL, puesto que, estos registros son los meses y años de los cinco (5) mejores años de sueldo. 

Como la ejecución del algoritmo de Clúster Jerárquico (CJ) requiere de gran capacidad computacional, se procedió a dividir el dataset \textit{data\_l} en dos grupos. El primer grupo (de nombre \textit{ul} tiene la información de 373.069 personas) solo considera a las personas que durante toda su vida laboral y en un mismo mes realizaron una sola aportación.  El segundo grupo es el complemento del primero (se llamará \textit{ml}) y contiene información de 69.501 personas

Tanto para \textit{ul} como \textit{ml}, se calcula el bigote superior (LS\_MS) de los sueldos de los cinco mejores años a nivel de persona.

Por otra parte, en referencia a la literatura, el utilizar el método single\footnote{Se utiliza el método single pues permite detectar outliers \citep{Hodge2004}} en el algoritmo de Clúster Jerárquico, permite detectar registros atípicos, por lo cual, está métrica será la que se va a utilizar en el análisis. El código utilizado es la función \textit{cluster\_jerarquico} del código de la sección \ref{sec:alg_clus_jear} de anexos. La idea general del código es considerar como máximo dos clústeres y dentro de cada uno de ellos realizar el siguiente análisis:

\begin{itemize}
\item Si el número de clústeres que se forman es mayor a 1, se calcula el centroide de cada clúster y si estos valores son mayores a LS\_MS, entonces todas las observaciones del clúster respectivo serán atípicas, por lo que, se etiquetarán con el valor de 1. Caso contrario, su etiqueta será 0. \\[-0.75cm]
\item Si el  número de clústeres formados es igual a 1, entonces la etiqueta de todos los registros será -1, valor que se corregirá después.
\end{itemize}

Los resultados de la función \textit{cluster\_jerarquico} indican que hay 123.850 personas con al menos un sueldo atípico y 52.121 personas cuyos sueldos formaron un solo clúster. A continuación, se presenta un ejemplo de cada caso.
% \begin{figure}[H]
% \centering
% \includegraphics[scale=0.30]{graficos/algoritmo_cj_271_1.png}
% \caption{\headlinecolor{\underline{Cluster jerárquico: Caso 1}}}
% \label{fig:algoritmo_cj_271_1}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[scale=0.30]{graficos/algoritmo_cj_20889867_1.png}
% \caption{\headlinecolor{\underline{Cluster jerárquico: Caso -1}}}
% \label{fig:algoritmo_cj_271_1}
% \end{figure}
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[scale=0.25]{graficos/algoritmo_cj_271_1.png}
        \caption{\headlinecolor{\underline{Cluster jerárquico: Caso 1}}}
        \label{fig:algoritmo_cj_271_1}
    \end{subfigure}
    \hspace{0.08\textwidth} % Espacio entre las imágenes
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[scale=0.25]{graficos/algoritmo_cj_20889867_1.png}
        \caption{\headlinecolor{\underline{Cluster jerárquico: Caso -1}}}
        \label{fig:algoritmo_cj_20889867_1} % Corrijo el label
    \end{subfigure}
\end{figure}


La clasificación anterior debe ser corregida, debido a que hay sueldos que, el algoritmo los clasifica como atípicos, pero que por su naturaleza legal no deberían serlo. Estos valores recaen sobre los sueldos que son iguales al SBU\footnote{En Ecuador, según la \legalcite{LeySS}, la remuneración mínima sobre la que un afiliado aporta es el Salario Básico Unificado (SBU). }.

Por otro lado, una segunda correción se realiza a los sueldos etiquetados con -1, pues el algoritmo no determinó si en efecto eran o no valores atípicos, por lo que, si el sueldo clasificado como -1  es mayor al mínimo entre LS2 y LS\_M entonces será atípico. Una última corrección se hace a los sueldos de \textit{ml}, pues aquí la variable sueldo para un mes es la suma de todas las aportaciones que se realizaron en ese mes, por lo que, existen registros que son la suma de dos, tres, cuatro o más aportaciones. Por lo tanto, si una de las partes que componen la suma es mayor  al mínimo entre LS2 y LS\_MS, entonces ese sueldo sí es atípico.

En este sentido, en la primera fila de la figura \ref{fig:algoritmo_cj_depu} se muestran algunos ejemplos de la clasificación dada por el algoritmo de clusterización,  y en la segunda, las correcciones realizadas a la etiqueta del sueldo en base a lo expuesto con anterioridad. Adicionalmene, en la figura \ref{fig:algoritmo_cj_depu_anex} de la parte de anexos se muestran más ejemplos.

\begin{landscape}
\begin{figure}[H]  
    \centering
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_cj_8804_2.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_cj_3071_2.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_cj_20857054_2.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_cj_10133_2.png} 
    \end{minipage}
   
   \vfill 
  \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_cj_8804_3.png} 
    \end{minipage}
     \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_cj_3071_3.png}  
        
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_cj_20857054_3.png}  
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_cj_10133_3.png}  
    \end{minipage}

\caption{\headlinecolor{\underline{Ejemplos de la aplicación del CJ y su correción }}}
\label{fig:algoritmo_cj_depu}  
    
\end{figure}

Como se observa en la figura anterior, la corrección es adecuada puesto que, si no se la realiza se estarían clasificando como atípicos valores que no lo son.

Para que el lector pueda tener una visión más detallada y explícita del proceso de ejecución de este algorimto, puede revisar el script \textit{VIU\_cluster\_jerarquico.ipynb} de \legalcite{CristianVIUTFM}.

\end{landscape}

\subsection{K-Means} \label{sec:apr_no_sup_km}

Otro algoritmo que tiene gran potencial para la estimación de valores atípicos es  K-Means, destacando por su facilidad de implementación y forma de operar. Este método permite identificar observaciones atípicas de forma más óptima al detectar aquellos puntos que se desvían significativamente de los centroides de los clústeres que se han formado. Así, se localizan las anomalías que se alejan de los patrones de los distintos grupos, siendo una herramienta bastante útil para la detección de outliers \citep{wei2019msd}. 

En este sentido, se sigue un proceso similar a lo expuesto en la subsección \ref{sec:apr_no_sup_cj} con respecto a las consideraciones para GRUPO\_SEL, data\_l, ul, ml y LS\_MS. La ejecución de este algoritmo requiere de un tiempo muy extenso de procesamiento, debido a la gran cantidad de datos.

Por otra parte, siguiendo lo expresado en la literatura, este algoritmo se ejecutará en cuatro casos, dos casos para los datos con y sin normalización y otros dos casos para los datos con y sin registros duplicados. Esto tiene la finalidad de mejorar la velocidad del procesamiento del algoritmo de K-Means \citep{scikit_learn_kmeans} y \citep{AdventuresinMachineLearning}.

Con la finalidad de realizar un ejercicio didácico que estima el número de clústeres que agruparán los salarios de cada persona, se utiliza la función \textit{num\_cluster} del código de la sección \ref{sec:alg_kmean}. Los casos serán abreviados por \textit{ATI\_KM\_M1} para datos con duplicados y sin normalización,  \textit{ATI\_KM\_M2}  para datos sin duplicados y sin normalización,  \textit{ATI\_KM\_M3} para datos con duplicados y  con normalizados; y, \textit{ATI\_KM\_43} para datos sin duplicados y con normalizados

Los métodos para estimar el número de clústeres, aplicados a los datos de \textit{ul} y \textit{ml}, son del codo y  silueta. Los resultados promedios son:

Para \textit{ul}
\begin{itemize}
\item Caso \textit{ATI\_KM\_M1}.- Método del codo da 2,053 cluster y la silueta 3,665 cluster,\\[-0.75cm]
\item Caso \textit{ATI\_KM\_M2}.- Método del codo da 1,677 cluster y la silueta 2,066 cluster,\\[-0.75cm]
\item Caso \textit{ATI\_KM\_M3}.- Método del codo da 2,134 cluster y la silueta 3,558 cluster,\\[-0.75cm]
\item Caso \textit{ATI\_KM\_M4}.- Método del codo da 1,677 cluster y la silueta 2,065 cluster,
\end{itemize}

Para \textit{ml}
\begin{itemize}
\item Caso \textit{ATI\_KM\_M1}.- Método del codo da 2,223 cluster y la silueta 3,626 cluster,\\[-0.75cm]
\item Caso \textit{ATI\_KM\_M2}.- Método del codo da 2,067 cluster y la silueta 2,532 cluster,\\[-0.75cm]
\item Caso \textit{ATI\_KM\_M3}.- Método del codo da 2,233 cluster y la silueta 3,622 cluster,\\[-0.75cm]
\item Caso \textit{ATI\_KM\_M4}.- Método del codo da 2,066 cluster y la silueta 2,529 cluster,
\end{itemize}

Se observa que el número de clústeres promedio oscila entre 2 y 3 aproximadamente, lo que refuerza la decisión de que la clasificación solo se tendrán en cuenta dos (2) clúster, puesto que, se hace el supuesto que los salarios se agrupen en dos clúster, con y sin atípicos. La función que se utilizará es \textit{clasificacion\_kmean} del código de la sección \ref{sec:alg_kmean}. Es importante notar que, con la finalidad de mejorar la elección inicial de los centroides se considera el método \textit{K-Means++}.

Tras aplicar \textit{clasificacion\_kmean}, tenemos que si el número de clústeres\footnote{Independientemente de elegir que formen dos clústeres, el algoritmo generaba uno o dos clúster, lo cual dependía del comportamiento de los datos} generados es mayor a 1, se calcula el centroide de cada clúster y si estos valores son mayores a LS\_MS entonces todas las observaciones del clúster respetivo serán atípicas (su etiqueta será 1). Por otra parte, si el número de clústeres es igual a 1, entonces la etiqueta será igual a -2, puesto que solo se formó un único clúster. Ahora bien, al momento  de considerar o no los valores repetidos, la mayoría de los casos se comportaban como lo que ya se expuso anteriormente. Sin embargo, existían casos en donde al eliminar los duplicados, el promedio de los cinco mejores años era un único valor, por lo cual, cuando se daban estos casos la etiqueta era -1. Los resultados se presentan en la tabla \ref{tab:resul_kmean_pre}, cuyos valores hacen mención a las personas que al menos tienen un sueldo que corresponde a una categoría  de los cuatro casos anteriores.

\begingroup\scriptsize
\setlength\extrarowheight{1pt}
\setlength\aboverulesep{-0.5pt}
\setlength\belowrulesep{0pt}
\fontsize{7}{8}\selectfont
\begin{longtable}[H]{r|r|r|r|r|r|r|r|r|r|r|r|r } 
\caption{\headlinecolor{\underline{Resultados previos de aplicar K-Means}}}
\label{tab:resul_kmean_pre}\\[-0.2cm]

\toprule
\rowcolor{naranja}
& \multicolumn{12}{c}{\textbf{Clasificación}} \\ \hline

data
& \multicolumn{3}{c|}{\textit{ATI\_KM\_M1}}
& \multicolumn{3}{c|}{\textit{ATI\_KM\_M2}}
& \multicolumn{3}{c|}{\textit{ATI\_KM\_M3}}
& \multicolumn{3}{c}{\textit{ATI\_KM\_M4}}\\ \cmidrule{2-13}

& -2 & -1 & 1
& -2 & -1 & 1
& -2 & -1 & 1
& -2 & -1 & 1 \\\hline


\midrule
\endfirsthead

\toprule
\rowcolor{naranja}
& \multicolumn{12}{c}{\textbf{Clasificación}} \\ \hline

data
& \multicolumn{3}{c|}{\textit{ATI\_KM\_M1}}
& \multicolumn{3}{c|}{\textit{ATI\_KM\_M2}}
& \multicolumn{3}{c|}{\textit{ATI\_KM\_M3}}
& \multicolumn{3}{c}{\textit{ATI\_KM\_M4}}\\ \cmidrule{2-13}

& -2 & -1 & 1
& -2 & -1 & 1
& -2 & -1 & 1
& -2 & -1 & 1 \\\hline


\midrule
\endhead

  \hline \multicolumn{13}{r}{continúa...} \\
  \endfoot

  \bottomrule
  %\caption*{\scriptsize \textbf{Fuente}: Datos administrativos del IESS.\\\textbf{Elaborado}: DAIE.}
  \endlastfoot
  
  ul 
  & 39.768 & 0 & 64.785 
  & 0      & 39.768 & 37.479
  & 39.768 & 0 & 64.795
  & 39.768 & 0 & 37.388 \\\hline
  
  ml 
  & 244 & 0 & 21.635
  & 0      & 244 & 12.984
  & 244 & 0 & 21.639
  &244 & 0 & 12.962 \\


\end{longtable}
\endgroup

De la tabla anterior, prácticamente en los cuatro casos se tiene la misma clasificación en cantidad de personas, la diferencia solo radica en el tiempo de procesamiento de los resultados, puesto que, cuando se trabajaba con los datos sin normalizar y con duplicados, el tiempo de ejecución era muy elevado. Asi mismo, como bien se mencionó en la subsección anterior, tras la aplicación de K-Means es necesario hacer las mismas correcciones. 

Una vez realizadas las correcciones respectivas, se procede a crear una variable que considere a los cuatro casos y determine la etiqueta. La regla  considerara el valor  máximo de los cuatro casos. Así, si tres de los casos para un sueldo determinado tienen el valor 0  y en el otro tiene el valor de 1, entonces la etiqueta será 1 (atípico). El atributo que contiene esta información es ATI\_KM.

\begin{landscape}
Algunos ejemplos con los resultados de la implementación de K-Means se muestran en la figura \ref{fig:algoritmo_km_depu}.
\begin{figure}[H]  
    \centering
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_km_8804_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_km_3071_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_km_20857054_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_km_10133_1.png} 
    \end{minipage}
   
   \vfill 
  \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_km_20013447_1.png} 
    \end{minipage}
     \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_km_3608_1.png}  
        
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_km_135752_1.png}  
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_km_154409_1.png}  
    \end{minipage}

\caption{\headlinecolor{\underline{Resultados de la aplicación de K-Means }}}
\label{fig:algoritmo_km_depu}  
    
\end{figure}

Si el lector desea tener una una visión más detallada del proceso que se siguió, puede revisar el script \textit{VIU\_k\_means.ipynb} de \legalcite{CristianVIUTFM}.

\end{landscape}


\subsection{DBSCAN} \label{sec:apr_no_sup_db}

Debido a las desventajas que tiene K-Means al momento de agrupar datos que tienen diferentes tamaños, densidades y forma (forma no globular) se opta por implementar el algoritmo DBSCAN, pues el mismo tiene la capacidad de identificar clúster con formas arbitrarias y densidades variables, lo que ayuda a la detección de valores atípicos en datos no homogéneos \citep{boucher2020outlier}. De igual forma, este método al basarse en la densidad de los datos es más preciso en la identificación de atípicos locales significativos, debido a su flexibilidad al identificarlos en áreas de menor densidad y su capacidad de manejar el ruido \citep{smiti2020outlier}.

Se adopta nuevamente el proceso descrito en Clúster Jerárquico con respecto a las consideraciones para GRUPO\_SEL, data\_l, ul, ml y LS\_MS y se procede a determinar los parámetros (MinPts y Eps) que el algoritmo necesita. La función \textit{valor\_epsilon} de la sección \ref{sec:alg_dbscan} de la parte de anexos toma los K-ésimos vecinos más cercanos en un rango de 1 a 12\footnote{Se considera este valor  debido a que se esperaría que los 12 meses de aporte de un grupo de los mejores años tengan el mismo comportamiento} (MinPts) y determina el valor de \textit{épsilon} a través del método del codo. Si todas las distancias dadas por los k-ésimos vecinos tienen el mismo valor, entonces el valor de \textit{épsilon} (Eps) es cero, puesto que,el valor del sueldo es el mismo para los mejores años de sueldo. Con la función \textit{esti\_eps\_out} para un rango de valores de Eps, dados por MinPts, se calcula el número de clústeres y de atípicos generados por DBSCAN.

El proceso anterior nos da resultados para varias combinaciones de valores, por lo que, la función \textit{cal\_eps\_out} de la sección \ref{sec:alg_dbscan}, determiná los valores de Eps y MinPts  con los cuales se va a aplicar el algoritmo de DBSCAN, haciendo la consideración de que la selección abarque la mayor cantidad de valores atípicos con la menor cantidad de clústeres. Bajo este criterio tenemos que en \textit{ul} existen 316.552 personas con algún registro de sueldos atípico y en \textit{ml} existen 68.596 personas que tienen al menos una aportación atípica.

Nuevamente, es necesario hacer las correcciones indicadas en los dos métodos anteriores, por lo que, una vez realizadas las correcciones, en la figura \ref{fig:algoritmo_db_depu} se muestran los resultados.

Para que el lector pueda tener una visión más detallada del proceso, puede revisar el script \textit{VIU\_dbscan.ipynb} de \legalcite{CristianVIUTFM}.

\begin{landscape}

\begin{figure}[H]  
    \centering
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_8804_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_3071_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_20857054_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_10133_1.png} 
    \end{minipage}
   
   \vfill 
  \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_20013447_1.png} 
    \end{minipage}
     \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_db_3608_1.png}  
        
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_135752_1.png}  
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_154409_1.png}  
    \end{minipage}

\caption{\headlinecolor{\underline{Clasificación de DBSCAN }}}
\label{fig:algoritmo_db_clasi}  
 \vfill
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_ati_8804_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_ati_3071_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_ati_20857054_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_ati_10133_1.png} 
    \end{minipage}
   
   \vfill 
  \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_ati_20013447_1.png} 
    \end{minipage}
     \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_ati_3608_1.png}  
        
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_ati_135752_1.png}  
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_db_ati_154409_1.png}  
    \end{minipage}

\caption{\headlinecolor{\underline{Resultados de la aplicación de DBSCAN}}}
\label{fig:algoritmo_db_depu}  
    
\end{figure}


\end{landscape}

\section{Limpieza de datos antes del aprendizaje supervisado}\label{cha_resul:limpieza_super}

Una vez obtenida la etiqueta de atípico (valor 1) o no atípico (valor de 0) para los sueldos dentro de los cinco mejores años de los afiliados al IESS tras la implementación de los algoritmos de las secciones \ref{sec:apr_no_sup_cj}, \ref{sec:apr_no_sup_km} y \ref{sec:apr_no_sup_db} se procede a unir en un único conjunto de datos las variables que serán consideradas en el aprendizaje supervisado. Los atributos son básicamente los mismos que \textit{data\_l}, pero se incluyen las variables \textit{ATI\_CJ}\footnote{Para los resultados de Clúster Jerárquico}, \textit{ATI\_KM}\footnote{Para los resultados de K-means} y \textit{ATI\_DB}\footnote{Para los resultados de DBSCAN}, cuyos valores son 0 o 1 para identificar si los sueldos son atípicos o no, respectivamente. La clasificación y el número de personas que tienen al menos un sueldo atípico con los tres algoritmos es muy similar (su valor es aproximadamente 171.158 personas).

Como es importante tener conocimiento de cuanta es la afectación en el cálculo de la pensión de las personas que hicieron fraude, se crean las variables \textit{BCS\_CJ}, \textit{BCS\_KM} y \textit{BCS\_DB} que corresponden al cálculo de la variable \textit{BASE\_CAL} de la subsección \ref{subsec:trans_datos_no_super}, pero sin considerar las aportaciones que fueron clasificadas como atípicas. En los casos cuando el valor de esta variable es nulo, debido a que todos los sueldos de esa persona fueron clasificados como atípicos, se le asigna a las variables de la base de cálculo sin atípicos, el valor mínimo entre \textit{LS2} y \textit{LS\_MS}. El dataset que contiene toda esta información se llama \textit{data\_ati}.

Ejemplos de lo casos en los que todos los suelos de los mejores años fueron clasificados como atípicos se presenta en la figura \ref{fig:sueldos_unicos_atipicos}; y, claramente se observa que no necesariamente las aportaciones fraudulentas se dan en los últimos cincos años de la vida laboral del afiliado.

Por otra parte, en la figura \ref{fig:base_calculo_sin_atipicos} se muestran ejemplos de cómo es la afectación en la base de cálculo si no se consideran estos valores atípicos. La primera fila de la figura muestra los casos en los que la razón entre la base de cálculo original (\textit{BASE\_CAL}) sobre la corrección de la misma sin considerar sueldos atípicos  es menor a diez (10) veces, la segunda final corresponde a si la razón está entre 10 y 20 veces, la tercera fila si la razón está entre 20 y 30 veces y la última si la razón es mayor o 30 veces. Gráficamente se puede observar la forma en que distan estas dos bases de cálculo, valores que en algunos casos son afectados por un único valor atípico o grupos de atípicos que van desde 5 o hasta su totalidad de registros.

Adicionalmente, en \textit{data\_ati} se crean las variables \textit{CJ}, \textit{KM} y \textit{DB}, cuyos valores corresponden al valor máximo de las etiquetas de las clasificaciones dadas por los algoritmos no supervisados, es decir, si un persona tiene al menos un sueldo con etiqueta de atípico, entonces esa persona será etiquetada como atípica.


\begin{landscape}

\begin{figure}[H]  
    \centering
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_CJ_29231_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_CJ_9886_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_CJ_18457_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_CJ_18587_1.png} 
    \end{minipage}
   
   \vfill 
  \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_CJ_28952_1.png} 
    \end{minipage}
     \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/algoritmo_ATI_KM_32065_1.png}  
        
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_KM_77614_1.png}  
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_KM_82208_1.png}  
    \end{minipage}

 \vfill
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_KM_91427_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_DB_6037_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_DB_8851_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_DB_9315_1.png} 
    \end{minipage}
   
   \vfill 
  \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_DB_20856876_1.png} 
    \end{minipage}
     \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_DB_20878541_1.png}  
        
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_DB_20889867_1.png}  
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/algoritmo_ATI_DB_177653_1.png}  
    \end{minipage}
\caption{\headlinecolor{\underline{Ejemplos de sueldos cuyos valores son todos atípicos}}}
\label{fig:sueldos_unicos_atipicos}  

    
\end{figure}


\end{landscape}


\begin{landscape}

\begin{figure}[H]  
    \centering
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_4382_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_26246_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_2141271_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_10351659_1.png} 
    \end{minipage}
   
   \vfill 
  \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_1844848_1.png} 
    \end{minipage}
     \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.19]{graficos/base_sin_ati_CJ_5287079_1.png}  
        
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_5414462_1.png}  
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_13290239_1.png}  
    \end{minipage}

 \vfill
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_5579649_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_16132025_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_16445975_1.png} 
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_16508727_1.png} 
    \end{minipage}
   
   \vfill 
  \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_110456_1.png} 
    \end{minipage}
     \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_172889_1.png}  
        
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_16135176_1.png}  
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[scale=0.17]{graficos/base_sin_ati_CJ_13161696_1.png}  
    \end{minipage}

\caption{\headlinecolor{\underline{Ejemplos de la correción de la base de calculo  sin considerar sueldos atípicos}}}
\label{fig:base_calculo_sin_atipicos}  
    
\end{figure}


\end{landscape}

Ahora se procede a analizar la base de datos de los pensionistas. Como la estructura de los datos para los pensionistas de vejez, invalidez y discapacidad es la misma y se rige a lo mostrado en la sección \ref{sec:seleccion_datos}, se indica solo el proceso de limpieza para los pensionistas de vejez (el dataset se llamará \textit{vej}) y para el resto, la idea es la misma.

En primera instancia, en \textit{vej} se crea una variable adicional que es \textit{IMPO\_2023}, misma que contiene la información del número de imposiciones calculadas de los pensionistas. Una vez analizada la información que se encuentra en el sistema de información \textit{PRESTACIONAL} (para más detalle véase la sección \ref{sec_anex:fuentes_datos} de anexos), se observan varios registros duplicados para los pensionistas, lo cual es erróneo porque solo debería existir un único registro prestacional para cada pensionista. Es por ello que se crea la función \textit{pens\_correc} de la sección \ref{sec:limpieza_datos_super} de anexos para corregir esta situación. La idea es escoger el primer registro duplicado e ir eliminando los posibles casos, en función de la completitud de la información.

\section{Transformación de datos antes del aprendizaje supervisado}\label{subsec:trans_datos_super}

Los resultados de la sección anterior para el caso de los pensionistas deben ser corregidos, porque, se evidencia que la información de \textit{NUMERO\_IMPOSICIONES} y \textit{COEFICIENTE\_CAL} está con valores nulos o tiene un valor de cero, lo cual es imposible, puesto que tanto el número de imposiciones como el valor del coeficiente son fundamentales en el cálculo de la pensión \legalcite{ResIESS_CD100}. Las afectaciones que tiene la variable  \textit{COEFICIENTE\_CAL} están directamente correlacionadas con el valor de \textit{NUMERO\_IMPOSICIONES}. En este sentido, se realizaron la siguientes correcciones:

\begin{itemize}
\item Cuando el número de imposiciones es cero, se lo completa con los valores de \textit{IMPO\_2023},
\item Cuando el número de imposiciones es cero y el valor de \textit{IMPO\_2023} también es cero\footnote{Debido a las limitaciones del acceso a al información, solo se consideró la información del HL y no la del Host, en este sentido, si el valor es cero es porque todas las aportaciones de esas personas están en el sistema Host}, se completa la información del número de imposiciones con los valores de \textit{N\_MESES}\footnote{Esta aproximación tiene sentido, debido a que el número de meses trabajado por una persona es muy similar al cálculo de su número de imposiciones.},
\item Cuando el valor del coeficiente calculado es menor a 60, se le agrega 0,4375,
\item Cuando el valor del coeficiente calculado tiene un valor nulo y los valores de \textit{COEFICIENTE\_REAL} no son nulos, entonces al coeficiente calculado se le agrega el valor de \textit{COEFICIENTE\_REAL}
\end{itemize}

De \textit{data\_ati} se escogen las variables \textit{CEDULA\_COD}, \textit{INI\_CAL}, \textit{FIN\_CAL}, \textit{BASE\_CAL}, \textit{LS1}, \textit{SAL\_PROM1}, \textit{LS2}, \textit{SAL\_PROM2}, \textit{LS\_MS}, \textit{BCS\_CJ}, \textit{BCS\_KM}, \textit{BCS\_DB}, \textit{CJ}, \textit{KM} y \textit{DB} para luego hacer un left join con\footnote{Se da prioridad a la información de \textit{vej} y se busca en la tabla \textit{data\_ati}} los resultados de los datos de los pensionistas de vejez.

Realizando el mismo proceso para los pensionistas de invalidez y discapacidad se crea una única tabla (de nombre \textit{pen}), haciendo una concatenación por filas, con la información  de los pensionistas. Esta tabla contiene 686.479 filas con 33 columnas. Sin embargo, dados los problemas mostrados en \textit{NUMERO\_IMPOSICIONES} y \textit{COEFICIENTE\_CAL}, en \textit{pen} existen registros de pensionistas cuya información no se encuentra en \textit{data\_ati}, por lo cual, solo se trabajará con los registros que son diferentes de nulo en la variable \textit{BASE\_CAL}. Esto da como consecuencia que se trabaje con 442.979 filas y 33 columnas. En un paso adicional, a este conjunto de datos se le agregan las variables \textit{MES\_AS}, \textit{MES\_TU}, \textit{SUELDO}, \textit{APORTE}, \textit{INTERES\_APORTE}, \textit{N\_PRI}, \textit{N\_PUB}, \textit{N\_IND}, \textit{N\_VOL\_EX}, \textit{N\_VOL\_EC}, \textit{M\_PRI}, \textit{M\_PUB}, \textit{M\_IND}, \textit{M\_VOL\_EX} y \textit{M\_VOL\_EC} descritas en la sección \ref{subsec:trans_datos_no_super}.

Por otra parte, se crean las variables \textit{PEN\_CJ}, \textit{PEN\_KM} y \textit{PEN\_DB} que corresponden al cálculo de las pensiones corregidas en función del artículo 13 de la \legalcite{ResIESS_CD100}, es decir, se multiplica la base de cálculo corregida (es decir, sin considerar los sueldos atípicos) por la variable \textit{COEFICIENTE\_CAL}. 

El proceso a manera más detallada se encuentra en el script \textit{VIU\_analisis\_atipicos.ipynb} de \legalcite{CristianVIUTFM}.

Adicionalmente, si \textit{LS2} o \textit{SAL\_PROM2} son nulos se los completa con los valores de \textit{LS1} y \textit{SAL\_PROM1}, respectivamente. Por otra parte, también es necesaria la creación de las variables \textit{EDAD\_J}\footnote{Corresponde a la diferencia entre la fecha del derecho para poder solicitar la jubilación con la fecha de nacimiento del afiliado}, \textit{TIEM\_T}\footnote{Corresponde a la diferencia entre la fecha actual con la fecha de nacimiento de la persona} y \textit{TIEM\_MA}\footnote{Corresponde a la diferencia entre las variables \textit{RANGO\_INI\_5MEJ} y \textit{RANGO\_FIN\_5MEJ}, dividido para 30}. Como parte complementaria, a las variables del número de veces que el afiliado aporta históricamente en los diferentes sectores se la divide para la variable \textit{MES\_AS}; y se reescala a las variables del número de veces que el afiliado aportó, en los cinco mejores años, sobre el número de aportaciones totales en los cinco mejores años de sueldo. Estás variables tendrán el mismo nombre de las variables del número de aportaciones en los sectores, en sus respectivos casos, concatenado con el término \textit{\_P}.

El aspecto más importante a tener en cuenta es la creación de la variable objetivo (de nombre \textit{TARGET}), misma que contiene la etiqueta de si el pensionista realizó o no una aportación fraudulenta. La construcción de esta variable considera a todos los valores de \textit{CJ}, \textit{KM} y \textit{DB}. Si todos los valores  son 1 a la vez será etiquetado como pensionistas que hicieron fraude, pero si uno de los valores no es 1 entonces no será etiquetado como fraude. Así, los atributos con los cuales se va a implementar la parte del aprendizaje supervisado son \textit{SEXO}, \textit{NUMERO\_IMPOSICIONES}, \textit{BASE\_CAL}, \textit{N\_MESES}, \textit{PRES}, \textit{LS2}, \textit{SAL\_PROM2}, \textit{LS\_MS}, \textit{TIEM\_T}, \textit{TIEM\_MA}, \textit{MES\_AS}, \textit{SUELDO}, \textit{N\_PRI\_P}, \textit{N\_PUB\_P}, \textit{N\_IND\_P}, \textit{N\_VOL\_EC\_P}, \textit{N\_VOL\_EX\_P}, \textit{M\_PRI\_P}, \textit{M\_PUB\_P}, \textit{M\_IND\_P}, \textit{M\_VOL\_EC\_P}, \textit{M\_VOL\_EX\_P}, \textit{EDAD\_J}, \textit{TARGET}.  Todo esto se guardará en el dataset de nombre \textit{data}.


Un mayor detalle del proceso de los dos últimos párrafos de esta sección se presenta en \textit{VIU\_ml\_preparacion\_data.ipynb} de \legalcite{CristianVIUTFM}.

%Aprendizaje supervisado----------------------------------------------------------------------------
\section{Aprendizaje supervisado}

Previo a la aplicación de los algoritmos de aprendizaje supervisado es fundamental la determinación de los atributos que son los más significativos para el análisis y así evitar la maldición de la dimensionalidad. Esto tiene como fin evitar considerar una variable que no aporte nada a la predicción de los modelos y genere un tiempo de ejecución de los algoritmos muy elevado. En este sentido, se plantean dos enfoques, uno considerando nuevas características dadas por el análisis de componentes principales (ACP) y el otro con la utilización de algoritmos de aprendizaje no supervizado.

\subsection{Selección de características dadas por el ACP}\label{sub_apre_cara_acp}

Como se menciona en \cite{tesis_guatemal_epn} y \cite{wiskott2009pca}, el objetivo del ACP es, dadas $X_1, X_2, \ldots, X_n$ variables, hallar $n$ nuevas variables que son combinaciones lineales de las variables iniciales, de tal manera que $r$ ($r<n$) recojan la mayor cantidad de información posible sobre los atributos $X_1, X_2, \ldots, X_n$. Antes de la utilización del ACP es necesario, como se menciona en \cite{tesis_guatemal_epn}, ``verificar si la correlación entre las variables analizadas es lo suficientemente grande como para poder justificar la factorización de la matriz de coeficientes de correlación'' (p. 90). Esto se logra mediante el Test de Esfericidad de Bartlett y el Índice Kaiser-Meyer-Olkin (KMO). 

El resultado de aplicar el test de esfericidad nos da un p-valor menor a 0, lo cual nos permite decir que estadísticamente las variables están correlacionadas. No obstante, el valor de KMO es 0,4720, lo cual da señales que no es factible\footnote{En \cite{tesis_guatemal_epn} se manifiesta que si los valores del KMO están entre 0,5 y 1 es apropiado aplicar un ACP} realizar un ACP. En este sentido, tenemos los suficientes argumentos para no realizar el ACP. Sin embargo, con la finalidad de comparar los resultados que nos darán los métodos supervizados y el ACP, se procede a implementarlo. 

La primera parte es dividir a los datos de \textit{data} en \textit{train} y \textit{test}. En este punto y en lo que sigue se considera que el 20\% de los datos serán para el conjunto de test. Luego se normalizan los atributos de train con excepción de las variables \textit{SEXO}\footnote{Sus valores son 0 para hombres y 1 para mujer}, \textit{PRES}\footnote{Sus valores son 0 para pensionistas de discapacidad, 1 para pensionistas de invalidez y 2 para pensionistas de vejez}, \textit{N\_PRI\_P}, \textit{N\_PUB\_P}, \textit{N\_IND\_P}, \textit{N\_VOL\_EC\_P}, \textit{N\_VOL\_EX\_P}, \textit{M\_PRI\_P}, \textit{M\_PUB\_P}, \textit{M\_IND\_P}, \textit{M\_VOL\_EC\_P}, \textit{M\_VOL\_EX\_P} y \textit{TARGET}. No se consideró a las variables con terminación en \textit{\_P} pues ya estan en escala de 0 a 1. Los parámetros de la normalización en el \textit{train} se utilizan para normalizar los datos de \textit{test}.

Tras la aplicación del ACP en el conjunto de datos de train, en la octava componente se recoje el 92,60\% de la información de las variables originales. Es así que, se utiliza este número de componentes y se proyectan los datos normalizados del train y del test en las mismas. Estas variables tomarán los nombres \textit{PC1}, \textit{PC2}, \textit{PC3}, \textit{PC4}, \textit{PC5}, \textit{PC6}, \textit{PC7} y \textit{PC8}.

Ahora bien, se repite el mismo proceso descrito anteriormente, pero con una balanceo de clases. Lo que se hace es que posterior al proceso de normalización de train y test, generar un balanceo de clases con el método \textit{SMOTEENN}. Se aplica el ACP y se tiene que en la octava componente se recoje el 93,12\% de la información de las variables originales. Después se proyectan los datos del train y test normalizados y balanceados en estás nuevas componentes principales y el nombre de las variables será \textit{PC1\_bal}, \textit{PC2\_bal}, \textit{PC3\_bal}, \textit{PC4\_bal}, \textit{PC5\_bal}, \textit{PC6\_bal}, \textit{PC7\_bal} y \textit{PC8\_bal}.

Como se expuso anteriormente, esta parte es solo didáctica, por lo que estás variables solo serán consideradas en la implementación del algoritmo \textit{KNN}.

\subsection{Selección de características dadas por métodos no supervisados}\label{sub_apre_cara_no_super}

Con la ayuda de técnicas del aprendizaje no supervisado se van a seleccionar las variables más significativas para utilizarlas en la implementación  de los algoritmos de aprendizaje supervisado. Lo primero a realizar es normalizar a \textit{data} sin los atributos \textit{SEXO}, \textit{PRES}, \textit{N\_PRI\_P}, \textit{N\_PUB\_P}, \textit{N\_IND\_P}, \textit{N\_VOL\_EC\_P}, \textit{N\_VOL\_EX\_P}, \textit{M\_PRI\_P}, \textit{M\_PUB\_P}, \textit{M\_IND\_P}, \textit{M\_VOL\_EC\_P}, \textit{M\_VOL\_EX\_P} y \textit{TARGET}, en base a la justificación del párrafo tres de la subsección \ref{sub_apre_cara_acp}. Se aplica el proceso MinMaxScaler() a la transpuesta de la data resultante (el resultado se llamará \textit{features\_norm}).

El primer enfoque es aplicar a \textit{features\_norm} el cluster jerárquico con la métrica \textit{single}. El resultado se presenta en la figura \ref{fig:dendograma_sel_atri_cj}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{graficos/dendograma_sel_carac_cj.png}
\caption{\headlinecolor{\underline{Dendograma para la selección de atributos}}}
\label{fig:dendograma_sel_atri_cj}
\end{figure}

Visualmente, se realiza el corte en 100 y se generan 12 agrupaciones. La asignación de las variables en sus respectivos cluster es la siguiente:

\begin{itemize}
\item Cluster 1: \textit{SEXO} y \textit{BASE\_CAL},
\item Cluster 2: \textit{SAL\_PROM2},
\item Cluster 3: \textit{MES\_AS},
\item Cluster 4: \textit{N\_IND\_P} y \textit{M\_IND\_P},
\item Cluster 5: \textit{NUMERO\_IMPOSICIONES}, \textit{N\_MESES}, \textit{PRES}, \textit{LS2} y \textit{TIEM\_MA},
\item Cluster 6: \textit{N\_PUB\_P} y \textit{M\_PUB\_P},
\item Cluster 7: \textit{N\_VOL\_EC\_P}, \textit{N\_VOL\_EX\_P}, \textit{M\_PRI\_P}, \textit{M\_VOL\_EC\_P}, \textit{M\_VOL\_EX\_P} y \textit{EDAD\_J},
\item Cluster 8: \textit{TARGET},
\item Cluster 9: \textit{SUELDO},
\item Cluster 10: \textit{LS\_MS},
\item Cluster 11: \textit{TIEM\_T},
\item Cluster 12: \textit{N\_PRI\_P},
\end{itemize}

El segundo enfoque es utilizar DBSCAN para identificar ouliters y con ello detectar a las características que son diferentes a las demás y que nos ayudarán en el problema de la maldición de la dimensionalidad. En este sentido, para \textit{features\_norm} y tomando $minPts = 3$ debido a su simplicidad de implementación y ya que ha demostrado obtener buenos resultados, se va a determinar el valor de epsilon. En la figura \ref{fig:sele_dbscan_atributos} se ven los resultados. El valor de epsilon a utilizar es de 198.1671.


\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{graficos/valor_epsilon_feacture_norm.png}
\caption{\headlinecolor{\underline{Estimación del valor de epsilon para la selección de atributos}}}
\label{fig:sele_dbscan_atributos}
\end{figure}

Se amplía el rango de búsqueda para el valor de epsilon y se escoje el valor de epsilon igual a 64, que genera 2 clústeres con 17 atributos atípicos. El detalle de esta clasificación se presenta a continuación:

\begin{itemize}
\item Cluster 1: \textit{NUMERO\_IMPOSICIONES}, \textit{PRES} y \textit{TIEM\_MA},
\item Cluster 2: \textit{N\_VOL\_EX\_P}, \textit{M\_PRI\_P}, \textit{M\_VOL\_EX\_P} y \textit{EDAD\_J},
\item Atípicos: \textit{SEXO}, \textit{BASE\_CAL}, \textit{N\_MESES}, \textit{LS2}, \textit{SAL\_PROM2}, \textit{LS\_MS}, \textit{TIEM\_T}, \textit{MES\_AS}, \textit{SUELDO}, \textit{N\_PRI\_P}, \textit{N\_PUB\_P}, \textit{N\_IND\_P}, \textit{N\_VOL\_EC\_P}, \textit{M\_PUB\_P}, \textit{M\_IND\_P}, \textit{M\_VOL\_EC\_P} y \textit{TARGET}
\end{itemize}

En base a los resultados de la selección de características dadas por el clúster jerárquico y DBSCAN, las variables a considerar en lo que sigue son \textit{BASE\_CAL}, \textit{LS2}, \textit{SAL\_PROM2}, \textit{LS\_MS}, \textit{SUELDO}, \textit{SEXO}, \textit{PRES}, \textit{N\_PRI\_P}, \textit{N\_PUB\_P}, \textit{N\_IND\_P}, \textit{N\_VOL\_EC\_P}, \textit{N\_VOL\_EX\_P}, \textit{M\_PRI\_P}, \textit{M\_PUB\_P}, \textit{M\_IND\_P}, \textit{M\_VOL\_EC\_P}, \textit{M\_VOL\_EX\_P}, \textit{NUMERO\_IMPOSICIONES}, \textit{TIEM\_T} y \textit{TARGET}. Por facilidad, al grupo de estas variables se llamará \textit{variables\_ml} y las mismas serán utilizadas en la implementación de todos los algortimos de aprendizaje supervisado a utilizar.

\subsection{KNN}

El KNN es un método no paramétrico que clasifica los puntos según su distancia a los datos de entrenamiento. De donde, a los nuevos puntos se les asigna la etiqueta más común entre sus vecinos. Entre sus ventajas encontramos su facilidad de implementación, su flexibilidad cuando no se conoce la estructura subyacente del modelo, eficacia especialmente cuando hay una buena distribución de datos, y su robustez al ruido ya que permite modificaciones con el fin de disminuir la influencia de valores atípicos. Es necesario tener en cuenta que su rendimiento depende de la elección tanto del número de vecinos como del peso usado \citep{syriopoulos2023knn}.

Utilizando los códigos provistos en la materia de Machine Learning y las variables \textit{PC1}, \textit{PC2}, \textit{PC3}, \textit{PC4}, \textit{PC5}, \textit{PC6}, \textit{PC7}, \textit{PC8}, se procede a parametrizar la elección KNeighbors en el rango de 1 a 30 y los pesos (ponderaciones) serán dados por \textit{uniform} y \textit{distance}. Se ejecutará validación cruzada y como medida de éxito vamos a utilizar el Accuracy. El código se presenta en la sección \ref{sec_anexos:knn} de anexos.
 
Los resultados se muestran en la figura \ref{fig:algoritmo_knn_acp}, por lo que, la elección de los parámetros es con 29 vecinos y la ponderación es dada por \textit{distance}. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{graficos/algortimo_knn_acp.png}
\caption{\headlinecolor{\underline{Estimación del número de vecinos y peso para el KNN con ACP}}}
\label{fig:algoritmo_knn_acp}
\end{figure}

En este estudio, las medidas clave para la evaluación del modelo serán tanto la sensibilidad como el accuracy. Esto ya que el accuracy proporciona una visión general de qué tan bien el modelo clasifica los datos globalmente. No obstante, es necesario tener en cuenta los casos en los que los valores atípicos se han clasificado incorrectamente como no atípicos, lo que se obtiene restando a 1 la sensibilidad. Dado el contexto del estudio, la detección de valores atípicos es crucial, ya que la falta de identificación de uno de ellos puede generar más problemas que la mala clasificación de un valor no atípico como atípico. En este estudio, no detectar un atípico representa no identificar a una persona que ha cometido fraude, lo cual puede resultar en grandes pérdidas para la Seguridad Social. Por el contrario, si una persona que no ha cometido fraude es identificada como un valor atípico, únicamente se realizará una investigación más profunda, sin que esto resulte en mayores pérdidas económicas.

Tomando esto en cuenta, una vez entrenado el modelo con los parámetros previamente adquiridos, se ha obtenido que el acurracy del mismo al ser aplicado en los datos del test es de 0,7483230571176909, mientras que al aplicarse en los datos de train el mismo es de 1. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión de las predicciones realizadas de la base de datos de testeo, que se encuentra en la tabla \ref{tab:mc_knn_1} y en la figura \ref{fig:matriz_conf_knn_acp} en anexos.

\begin{table}[h!]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} & & \textbf{No atípico} & \textbf{Atípico} \\
\cline{2-4}
\cellcolor[HTML]{FFA074} \textbf{Real}
 & \textbf{No atípico}  & 50557 & 7576 \\ \cline{2-4}
\cellcolor[HTML]{FFA074} & \textbf{Atípico} & 14711 & 15710 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el KNN con ACP}}}
\label{tab:mc_knn_1}
\end{table}

A partir de estos resultados, podemos extraer ciertas métricas, a parte del accuracy que nos ayudarán a establecer la capacidad de predicción del modelo, es decir, si su funcionamiento es óptimo o no dependiendo de nuestros objetivos. Para ello, tenemos que en este modelo la sensibilidad es de \Sexpr{15710/(15710+14711)}, la especifidad es de \Sexpr{50557/(50557+7576)} y y finalmente, la precisión es de \Sexpr{15710/(15710+7576)}.


Repitiendo el proceso anterior  para las variables \textit{PC1\_bal}, \textit{PC2\_bal}, \textit{PC3\_bal}, \textit{PC4\_bal}, \textit{PC5\_bal}, \textit{PC6\_bal}, \textit{PC7\_bal} y \textit{PC8\_bal}, los resultados se muestran en la figura \ref{fig:algoritmo_knn_bal_acp}, por lo que, la elección de los parámetros es con 2 vecinos y la ponderación es dada por \textit{distance}. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{graficos/algortimo_knn_bal_acp.png}
\caption{\headlinecolor{\underline{Estimación del número de vecinos y peso para el KNN con balanceo y ACP}}}
\label{fig:algoritmo_knn_bal_acp}
\end{figure}

Una vez entrenado el modelo con estos parámetros, se ha obtenido que el acurracy del mismo al ser aplicado en los datos del test es de 0.7128644668789665, mientras que al aplicarse en los datos de train es 1. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:mc_knn_2} y e n la figura \ref{fig:matriz_conf_knn_bal_acp} en anexos.

\begin{table}[h!]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} & & \textbf{No atípico} & \textbf{Atípico} \\
\cline{2-4}
\cellcolor[HTML]{FFA074} \textbf{Real}
 & \textbf{No atípico}  & 41160 & 16973 \\ \cline{2-4}
\cellcolor[HTML]{FFA074} & \textbf{Atípico} & 8454 & 21967 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el KNN con balanceo y ACP}}}
\label{tab:mc_knn_2}
\end{table}


A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{21967/(21967+8454)}, especifidad es de \Sexpr{41160/(41160+16973)} y finalmente, precisión  de \Sexpr{21967/(21967+16973)}.


Al considerar las variables de descritas en el último párrafo de \ref{sub_apre_cara_no_super}, realizando lo descrito en el párrafo tres de la subsección \ref{sub_apre_cara_acp} para estas variables, parametrizando la elección KNeighbors en el rango de 1 a 30 y los pesos  por \textit{uniform} y \textit{distance}, se ejecuta validación cruzada que tiene como medida de éxito el Accuracy, tenemos que:

% \begin{figure}[H]
% \centering
% \includegraphics[scale=0.35]{graficos/algortimo_knn_ml.png}
% \caption{\headlinecolor{\underline{Estimación del número de vecinos y peso para el KNN y \textit{variables\_ml}}}}
% \label{fig:matriz_conf_knn_ml}
% \end{figure}

Una vez entrenado el modelo con estos parámetros, se ha obtenido que el acurracy del mismo al ser aplicado en los datos del test es de 0.7128644668789665, mientras que al aplicarse en los datos de train es 1. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:mc_knn_3} y en la figura \ref{fig:matriz_conf_knn_ml} en anexos.

\begin{table}[h!]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} & & \textbf{No atípico} & \textbf{Atípico} \\
\cline{2-4}
\cellcolor[HTML]{FFA074} \textbf{Real}
 & \textbf{No atípico}  & 51203 & 6930 \\ \cline{2-4}
\cellcolor[HTML]{FFA074} & \textbf{Atípico} & 13376 & 17045 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el KNN y \textit{variables\_ml} }}}
\label{tab:mc_knn_3}
\end{table}


A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{17045/(17045+13376)}, especifidad es de \Sexpr{51203/(51203+6930)} y finalmente, precisión  de \Sexpr{17045/(17045+6930)}.


Se realiza un proceso de balanceo de clases para \textit{variables\_ml}, siguiendo el enfoque mostrado en el último párrafo de la subsección \ref{sub_apre_cara_acp}. A este conjunto de datos resultante, se aplica la validación cruzada con parametrización de KNeighbors en el rango de 1 a 30 y pesos en \textit{uniform} y \textit{distance}. Los resultados se muestran en la figura \ref{fig:algoritmo_knn_bal_ml}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{graficos/algortimo_knn_bal_ml.jpeg}
\caption{\headlinecolor{\underline{Estimación del número de vecinos y peso para el KNN con balanceo y \textit{variables\_ml}}}}
\label{fig:algoritmo_knn_bal_ml}
\end{figure}

Una vez entrenado el modelo con estos parámetros, se ha obtenido que el acurracy del mismo al ser aplicado en los datos del test es de 0.7272624613230345, mientras que al aplicarse en los datos de train es 1. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:mc_knn_4} y e n la figura \ref{fig:matriz_conf_knn_bal_acp} en anexos.

\begin{table}[h!]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} & & \textbf{No atípico} & \textbf{Atípico} \\
\cline{2-4}
\cellcolor[HTML]{FFA074} \textbf{Real}
 & \textbf{No atípico}  & 40929 & 17841 \\ \cline{2-4}
\cellcolor[HTML]{FFA074} & \textbf{Atípico} & 6311 & 24110 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el KNN y \textit{variables\_ml} }}}
\label{tab:mc_knn_4}
\end{table}


A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{24110/(24110+6311)}, especifidad es de \Sexpr{40929/(40929+17841)} y finalmente, precisión  de \Sexpr{24110/(24110+17841)}.


Para ver con mayor detalle  todo el proceso realizado para este algorimo vaya a \textit{VIU\_ml\_knn.ipynb} de \legalcite{CristianVIUTFM}.

\subsection{Árboles y conjuntos de modelos}\label{sub_seccion:arboles}

Considerando a \textit{variables\_ml}, se implementa un único árbol de decisión realizando la validación cruzada. Los parámetros generales que se consideran son que el número de fold es 10, \textit{gini} como  criterio de evaluación de las divisiones, la profundidad del árbol estará en el rango de 2 a 100 y la medida de éxito será el accuracy.

Los resultados se presentan en la figura \ref{fig:algortimo_arbol_simple}, por lo que la profundidad del árbol será de 14.

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{graficos/algortimo_arbol_simple.png}
\caption{\headlinecolor{\underline{Estimación de la profundidad  del árbol}}}
\label{fig:algortimo_arbol_simple}
\end{figure}

Una vez que el modelo ha sido entrenado con estos parámetros, se ha obtenido que el acurracy del mismo al ser aplicado en los datos del train y test son de 0.8432114867764302 y 0.7994218217133049, respectivamente. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:arbol_simple} y en la figura \ref{fig:confusion_matrix_algoritmo_arbol_simple} en anexos.

\begin{table}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} &  & \textbf{No atípico} & \textbf{Atípico} \\ \cline{2-4} 
\cellcolor[HTML]{FFA074} \textbf{Real}   & \textbf{No atípico}  & 50158 & 7975 \\ \cline{2-4} 
\cellcolor[HTML]{FFA074}                  & \textbf{Atípico}     & 9787 & 20634 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para árboles simples}}}
\label{tab:arbol_simple}
\end{table}

A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{20634/(20634+9787)}, especifidad de \Sexpr{50158/(50158+7975)} y finalmente, precisión  de \Sexpr{20634/(20634+7975)}.


La cuatro variables más importantes son \textit{BASE\_CAL} (28,07\%), \textit{LS2} (19,46\%), \textit{LS\_MS} (16,21\%) y \textit{M\_PRI\_P} (14,52\%)

Se repite el mismo proceso, pero realizando un balanceo de clases, con lo cual tenemos que en la figura \ref{fig:algortimo_arbol_simple_bal}, que la profundidad del árbol es de 15.

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{graficos/algortimo_arbol_simple_bal.png}
\caption{\headlinecolor{\underline{Estimación de la profundidad del árbol con datos balanceados}}}
\label{fig:algortimo_arbol_simple_bal}
\end{figure}

Una vez que el modelo ha sido entrenado con estos parámetros, se ha obtenido que el acurracy del mismo al ser aplicado en los datos del train y test son de 0.964122359796067 y 0.7735167242586445, respectivamente. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:arbol_simple_bal} y en la figura \ref{fig:confusion_matrix_algoritmo_arbol_simple_bal} en anexos.

\begin{table}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} &  & \textbf{No atípico} & \textbf{Atípico} \\ \cline{2-4} 
\cellcolor[HTML]{FFA074} \textbf{Real}   & \textbf{No atípico}  & 43944 & 14189 \\ \cline{2-4} 
\cellcolor[HTML]{FFA074}                  & \textbf{Atípico}     & 5867 & 24554 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para árboles simples balanceados}}}
\label{tab:arbol_simple_bal}
\end{table}

A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{24554/(24554 + 5867)}, especifidad de \Sexpr{43944/(43944+14189)} y finalmente, precisión  de \Sexpr{24554/(24554+14189)}.

La cuatro variables más importantes son \textit{BASE\_CAL} (33,23\%), \textit{LS2} (18,98\%), \textit{N\_PRI\_P	} (18,21\%) y \textit{SAL\_PROM2	} (6,98\%).

Generalmente, los árboles individuales son propensos a generar overfitting, lo que provoca que su eficacia sea muy poca al momento de predecir el comportamiento de los datos que no fueron considerados en la etapa de entrenamiento. Es así que, con el fin de evitar el sobreajuste se plantea un conjunto de modelos y así tener un mejor rendimiento predictivo.  Los métodos de muestreo que se van a considerar son \textit{RandomForestClassifier} (es el algoritmo de Random Forest utilizado para clasificación), \textit{AdaBoostClassifier} (Boosting) y \textit{GradientBoostingClassifier} (Boosting). Para la selección de los parámetros se utilizará la optimización de hiperparámetros con las herramientas de \textit{RandomizedSearch} y \textit{GridSearch}.


\subsubsection{RandomForestClassifier}

La primera parte es una selección  aleatoria de los  valores de hiperparámetros requeridos por \textit{RandomForestClassifier} mediante Random Search. El rango de valores se presenta en la subseción \ref{sec_anexos:conjunto_modelos_RandomForestClassifier} de anexos. Los parámetros son 

\begin{lstlisting}[style=pythonstyle]
RandomForestClassifier(class_weight='balanced_subsample', criterion='log_loss',
                       max_features=None, min_samples_leaf=8,
                       min_samples_split=24, n_estimators=32, n_jobs=-1,
                       random_state=8)
\end{lstlisting}

Con esto se entrena el modelo y tenemos que el accuracy del train y test es  0.9013257447433205 y 0.825089775730063, respectivamente. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:random_forest_randoms} y en la figura \ref{fig:confusion_matrix_algoritmo_RandomForestClassifier_random_search} en anexos.

\begin{table}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} &  & \textbf{No atípico} & \textbf{Atípico} \\ \cline{2-4} 
\cellcolor[HTML]{FFA074} \textbf{Real}   & \textbf{No atípico}  & 48142 & 9991 \\ \cline{2-4} 
\cellcolor[HTML]{FFA074}                  & \textbf{Atípico}     & 5498 & 24923 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el algoritmo con RandomizedSearch}}}
\label{tab:random_forest_randoms}
\end{table}

A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{24923/(24923 + 5498)}, especifidad de \Sexpr{48142/(48142+9991)} y finalmente, precisión  de \Sexpr{24923/(24923+9991)}.

La cuatro variablaes más importantes son \textit{BASE\_CAL} (26,53\%), \textit{LS2} (17,07\%), \textit{LS\_MS} (16,30\%) y \textit{SAL\_PROM2} (8,91\%).

Con la ayuda de los parámetros anteriores se realiza la búsqueda en cuadrícula mediante Grid Search. El rango de valores se presenta en la subseción \ref{sec_anexos:conjunto_modelos_RandomForestClassifier} de anexos. Los resultados de esto nos da

\begin{lstlisting}[style=pythonstyle]
RandomForestClassifier(class_weight='balanced_subsample', criterion='log_loss',
                       max_features=None, min_samples_leaf=4,
                       min_samples_split=20, n_estimators=30, n_jobs=-1,
                       random_state=8)
\end{lstlisting}

Con esto se entrena el modelo y tenemos que el accuracy del train y test es 0.9275357409038553 y 0.8278903268062425, respectivamente. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:random_forest_randomg} y en la figura \ref{fig:confusion_matrix_algoritmo_RandomForestClassifier_grid_search} en anexos.

\begin{table}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} &  & \textbf{No atípico} & \textbf{Atípico} \\ \cline{2-4} 
\cellcolor[HTML]{FFA074} \textbf{Real}   & \textbf{No atípico}  & 48828 & 9305 \\ \cline{2-4} 
\cellcolor[HTML]{FFA074}                  & \textbf{Atípico}     & 5936 & 24485 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el algoritmo con GridSearchCV}}}
\label{tab:random_forest_randomg}
\end{table}

A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{24485/(24485 + 5936)}, especifidad de \Sexpr{48828/(48828+9305)} y finalmente, precisión  de \Sexpr{24485/(24485+9305)}.

La cuatro variablaes más importantes son \textit{BASE\_CAL} (25,50\%), \textit{LS2} (16,66\%), \textit{LS\_MS} (16,12\%) y \textit{SAL\_PROM2} (9,22\%).

\subsubsection{AdaBoostClassifier}

 El rango de valores se presenta en la subseción \ref{sec_anexos:conjunto_modelos_AdaBoostClassifier} de anexos. Los parámetros son

\begin{lstlisting}[style=pythonstyle]
AdaBoostClassifier(algorithm='SAMME',
                   estimator=DecisionTreeClassifier(random_state=8),
                   learning_rate=0.01, n_estimators=4, random_state=8)
\end{lstlisting}

Con esto se entrena el modelo y tenemos que el accuracy del train y test es 1 y 0.7768480249339386, respectivamente. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:AdaBoost_random_search} y en la figura \ref{fig:confusion_matrix_algoritmo_AdaBoostClassifier_random_search} en anexos.

\begin{table}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} &  & \textbf{No atípico} & \textbf{Atípico} \\ \cline{2-4} 
\cellcolor[HTML]{FFA074} \textbf{Real}   & \textbf{No atípico}  & 48268 & 9865 \\ \cline{2-4} 
\cellcolor[HTML]{FFA074}                  & \textbf{Atípico}     & 9896 & 20525 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el algoritmo con RandomizedSearch}}}
\label{tab:AdaBoost_random_search}
\end{table}

A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{20525/(20525 + 9896)}, especifidad de \Sexpr{48268/(48268+9865)} y finalmente, precisión  de \Sexpr{20525/(20525+9865)}.

La cuatro variablaes más importantes son \textit{BASE\_CAL} (21,89\%), \textit{LS2} (16,11\%), \textit{LS\_MS} (15,40\%) y \textit{SAL\_PROM2} (10,03\%).

La búsqueda en cuadrícula mediante Grid Search  se presenta en la subseción \ref{sec_anexos:conjunto_modelos_RandomForestClassifier} de anexos. Los resultados de esto nos da

\begin{lstlisting}[style=pythonstyle]
AdaBoostClassifier(algorithm='SAMME',
                   estimator=DecisionTreeClassifier(random_state=8),
                   learning_rate=0.005, n_estimators=1, random_state=8)
\end{lstlisting}

Con esto se entrena el modelo y tenemos que el accuracy del train y test es 1 y 0.7768480249339386, respectivamente. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:AdaBoost_grid_search} y en la figura \ref{fig:confusion_matrix_algoritmo_AdaBoostClassifier_grid_search} en anexos.

\begin{table}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} &  & \textbf{No atípico} & \textbf{Atípico} \\ \cline{2-4} 
\cellcolor[HTML]{FFA074} \textbf{Real}   & \textbf{No atípico}  & 48268 & 9865 \\ \cline{2-4} 
\cellcolor[HTML]{FFA074}                  & \textbf{Atípico}     & 9896 & 20525 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el algoritmo con RandomizedSearch}}}
\label{tab:AdaBoost_grid_search}
\end{table}

A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{20525/(20525 + 9896)}, especifidad de \Sexpr{48268/(48268+9865)} y finalmente, precisión  de \Sexpr{20525/(20525+9865)}.

La cuatro variablaes más importantes son \textit{BASE\_CAL} (21,89\%), \textit{LS2} (16,11\%), \textit{LS\_MS} (15,40\%) y \textit{SAL\_PROM2} (10,03\%).


\subsubsection{GradientBoostingClassifier}

 El rango de valores se presenta en la subseción \ref{sec_anexos:conjunto_modelos_GradientBoostingClassifier} de anexos. Los parámetros son

\begin{lstlisting}[style=pythonstyle]
AdaBoostClassifier(algorithm='SAMME',
                   estimator=DecisionTreeClassifier(random_state=8),
                   learning_rate=0.01, n_estimators=4, random_state=8)
\end{lstlisting}

\subsection{XGBClassifier}\label{sub_seccion:XGBClassifier}

\begin{lstlisting}[style=pythonstyle]
XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=1.0, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric='mlogloss',
              feature_types=None, gamma=0.1, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=0.1, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=11,
              max_leaves=None, min_child_weight=1, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=450,
              n_jobs=None, num_parallel_tree=None, random_state=8, ...)
\end{lstlisting}

Con esto se entrena el modelo con los parámetros obtenidos con el algoritmo Random Search y tenemos que el accuracy del train y test es 0.9264488334801364 y 0.8252817489893173, respectivamente. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:confusion_matrix_algoritmo_XGBClassifier_random_search} y en la figura \ref{fig:confusion_matrix_algoritmo_XGBClassifier_random_search} en anexos.

\begin{table}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} &  & \textbf{No atípico} & \textbf{Atípico} \\ \cline{2-4} 
\cellcolor[HTML]{FFA074} \textbf{Real}   & \textbf{No atípico}  & 47989 & 10144 \\ \cline{2-4} 
\cellcolor[HTML]{FFA074}                  & \textbf{Atípico}     & 5328 & 25093 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el algoritmo con RandomSearch}}}
\label{tab:confusion_matrix_algoritmo_XGBClassifier_random_search}
\end{table}

A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{25093/(25093 + 5328)}, especifidad de \Sexpr{47989/(47989+10144)} y finalmente, precisión  de \Sexpr{25093/(25093+10144)}.


Por otro lado, al entrenar el modelo con los parámetros obtenidos con el algoritmo Grid Search, se tiene que el accuracy del train y test es 0.94972841430087854 y 0.8275967206450302, respectivamente. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:confusion_matrix_algoritmo_XGBClassifier_grid_search} y en la figura \ref{fig:confusion_matrix_algoritmo_XGBClassifier_grid_search} en anexos.

\begin{table}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} &  & \textbf{No atípico} & \textbf{Atípico} \\ \cline{2-4} 
\cellcolor[HTML]{FFA074} \textbf{Real}   & \textbf{No atípico}  & 48460 & 9673 \\ \cline{2-4} 
\cellcolor[HTML]{FFA074}                  & \textbf{Atípico}     & 5594 & 24827 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el algoritmo con GridSearch}}}
\label{tab:confusion_matrix_algoritmo_XGBClassifier_grid_search}
\end{table}

A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{24827/(24827 + 5594)}, especifidad de \Sexpr{48460/(48460+9673)} y finalmente, precisión  de \Sexpr{24827/(24827+9673)}.

La cuatro variablaes más importantes son \textit{BASE\_CAL} (11,37\%), \textit{M\_PRI\_P} (10,42\%), \textit{N\_PRI\_P} (9,99\%) y \textit{M\_PUB\_P} (8,04\%).


\subsection{Redes Neuronales}\label{sub_seccion:redes_neuronales}

La búsqueda en cuadrícula mediante Grid Search  se presenta en la subseción \ref{sec_anexos:conjunto_modelos_redes} de anexos. Los resultados de esto nos da

\begin{lstlisting}[style=pythonstyle]
MLPClassifier(hidden_layer_sizes=(200,), max_iter=100, random_state=8)
\end{lstlisting}

Con esto se entrena el modelo y tenemos que el accuracy del train y test es 0.8101412697337218 y 0.8048083655170856, respectivamente. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:confusion_matrix_algoritmo_redes_grid_search} y en la figura \ref{fig:confusion_matrix_algoritmo_redes_grid_search} en anexos.

\begin{table}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} &  & \textbf{No atípico} & \textbf{Atípico} \\ \cline{2-4} 
\cellcolor[HTML]{FFA074} \textbf{Real}   & \textbf{No atípico}  & 51482 & 6651 \\ \cline{2-4} 
\cellcolor[HTML]{FFA074}                  & \textbf{Atípico}     & 10634 & 19787 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el algoritmo con GridSearch}}}
\label{tab:confusion_matrix_algoritmo_redes_grid_search}
\end{table}

A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{19787/(19787 + 10634)}, especifidad de \Sexpr{51482/(51482+6651)} y finalmente, precisión  de \Sexpr{19787/(19787+6651)}.

Con un balanceo de clases y usando lo anterior tenemos que

\begin{lstlisting}[style=pythonstyle]
MLPClassifier(hidden_layer_sizes=(250,), max_iter=100, random_state=8)
\end{lstlisting}

Una vez balanceada la base de datos, se entrena el modelo y tenemos que el accuracy del train y test es 0.7625855410258148 y 0.7582604964202634, respectivamente. Para contrastar este resultado y evaluar el modelo se obtiene la matriz de confusión que se encuentra en la tabla \ref{tab:confusion_matrix_algoritmo_redes_bal_grid_search.png} y en la figura \ref{fig:confusion_matrix_algoritmo_redes_bal_grid_search.png} en anexos.

\begin{table}[H]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{E65013} \textbf{Matriz de confusión} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFA074} \textbf{Predicción}} \\ \hline
\multirow{2}{*}{\cellcolor[HTML]{FFA074}} &  & \textbf{No atípico} & \textbf{Atípico} \\ \cline{2-4} 
\cellcolor[HTML]{FFA074} \textbf{Real}   & \textbf{No atípico}  & 41361 & 16772 \\ \cline{2-4} 
\cellcolor[HTML]{FFA074}                  & \textbf{Atípico}     & 4635 & 25786 \\ \hline
\end{tabular}
}
\caption{\headlinecolor{\underline{Matriz de confusión para el algoritmo balanceado con GridSearch}}}
\label{tab:confusion_matrix_algoritmo_redes_bal_grid_search.png}
\end{table}

A partir de estos resultados obtenemos que este modelo tiene sensibilidad de \Sexpr{25786/(25786 + 4635)}, especifidad de \Sexpr{41361/(41361+16772)} y finalmente, precisión  de \Sexpr{25786/(25786+16772)}.


\section{Comparación de Resultados}\label{seccion:comparacion_resultados}

Para defininir cuáles modelos de aprendizaje supervisado son más efectivos y apropiados para el problema de estudio de este Trabajo de Fin de Máster es necesario realizar una comparación de las métricas de cada uno de ellos. Con ello, los mismos pueden ser evaluados y podemos obtener tanto sus ventajas como desventajas. A continuación encontramos una tabla comparativa de los resultados obtenidos:

% \begin{table}[H]
% \centering
% \resizebox{0.75\textwidth}{!}{
% \begin{tabular}{|c|c|c|c|c|c|}
% \hline
% \cellcolor[HTML]{E65013} \textbf{Modelo} & \cellcolor[HTML]{E65013} \textbf{Exactitud}& 
% \cellcolor[HTML]{E65013} \textbf{Sensibilidad}& \cellcolor[HTML]{E65013} \textbf{Especifidad}&
% \cellcolor[HTML]{E65013} \textbf{Precisión} & \cellcolor[HTML]{E65013} \textbf{Diferencia Exactitud}\\ 
% \hline
% \textbf{KNN con ACP} & 0.74832& 0.51642& 0.86968 & 0.67465& 0.25168 \\ 
% \hline
% \textbf{KNN con balanceo y ACP} & 0.71286& 0.7221& 0.70803& 0.56412& 0.28714 \\ 
% \hline
% \textbf{KNN con variables_ml} & & & & &  \\ 
% \hline
% \textbf{Matriz de confusión} & & & & &  \\ 
% \hline
% \textbf{Matriz de confusión} & & & & &  \\ 
% \hline
% \end{tabular}
% }
% \caption{\headlinecolor{\underline{Tabla de comparación de resultados}}}
% \label{tab:comparacion}
% \end{table}























